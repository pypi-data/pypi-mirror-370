# Quick Start Guide

Welcome to **Shekar**, a Python library for Persian Natural Language Processing. This guide will walk you through the most essential components so you can get started quickly with preprocessing, tokenization, pipelines, normalization, and embeddings.

---

## 1. Normalize Your Text

Use the `Normalizer` to standardize noisy or non-standard Persian text by applying a built-in pipeline of transformations.

```python
from shekar import Normalizer

normalizer = Normalizer()
text = "Û¿Ø¯Ù Ù…Ø§ Ø»Ù…Ú« Ø¨Û€ ÛÚªÚ‰ÙŠÚ±Ú• Ø£ÚšÙ¼"
print(normalizer.normalize(text))  # Output: "Ù‡Ø¯Ù Ù…Ø§ Ú©Ù…Ú© Ø¨Ù‡ ÛŒÚ©Ø¯ÛŒÚ¯Ø± Ø§Ø³Øª"
```

---

## 2. Use Preprocessing Components

Import and use individual text cleaners like `EmojiRemover`, `DiacriticsRemover`, or `URLMasker`.

```python
from shekar.preprocessing import EmojiRemover

text = "Ø³Ù„Ø§Ù… ğŸŒ¹ğŸ˜Š"
print(EmojiRemover()(text))  # Output: "Ø³Ù„Ø§Ù…"
```

See the full list of components in `shekar.preprocessing`.

---

## 3. Build Custom Pipelines

Create your own pipeline by chaining any number of preprocessing steps:

```python
from shekar import Pipeline
from shekar.preprocessing import EmojiRemover, PunctuationRemover

pipeline = Pipeline([
    ("emoji", EmojiRemover()),
    ("punct", PunctuationRemover())
])

text = "Ù¾Ø±Ù†Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ğŸ” Ù‚ÙØ³ÛŒØŒ Ø¹Ø§Ø¯Øª Ø¯Ø§Ø±Ù† Ø¨Ù‡ Ø¨ÛŒâ€ŒÚ©Ø³ÛŒ!"
print(pipeline(text))  # Output: "Ù¾Ø±Ù†Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ  Ù‚ÙØ³ÛŒ Ø¹Ø§Ø¯Øª Ø¯Ø§Ø±Ù† Ø¨Ù‡ Ø¨ÛŒâ€ŒÚ©Ø³ÛŒ"
```

Supports:
- Single strings or batches
- Function decorators for auto-cleaning input arguments

---

## 4. Tokenize Text into Sentences

Use `SentenceTokenizer` to split text into sentences:

```python
from shekar.tokenizers import SentenceTokenizer

text = "Ù‡Ø¯Ù Ù…Ø§ Ú©Ù…Ú© Ø¨Ù‡ ÛŒÚ©Ø¯ÛŒÚ¯Ø± Ø§Ø³Øª! Ù…Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒÙ… Ø¨Ø§ Ù‡Ù… Ú©Ø§Ø± Ú©Ù†ÛŒÙ…."
sentences = SentenceTokenizer().tokenize(text)

for s in sentences:
    print(s)
```

---
 
## Summary

| Task           | Tool / Class         |
|----------------|----------------------|
| Normalize text | `Normalizer`         |
| Clean text     | `shekar.preprocessing` |
| Create pipeline| `Pipeline`           |
| Tokenize       | `SentenceTokenizer`  |
| Word Cloud   | `WordCloud`           |

All components work with both single strings and lists. Pipelines are composable and can be reused, tested, or applied dynamically via decorators.
