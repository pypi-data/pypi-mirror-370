# llm_tokenizers

Language: [English](https://gitee.com/sky_flash/llm_tokenizers/blob/master/README_en.md) [ä¸­æ–‡](README.md)

#### ä»‹ç»
æ”¶é›†llmçš„å„ç§ tokenizer

#### è½¯ä»¶æ¶æ„
è½¯ä»¶æ¶æ„è¯´æ˜


#### é¡¹ç›®å®‰è£…æ•™ç¨‹

1.  å…‹éš†é¡¹ç›®åˆ°æœ¬åœ°ï¼š
```bash
git clone https://gitee.com/sky_flash/llm_tokenizers.git
``` 
2. è¿›å…¥é¡¹ç›®ç›®å½•ï¼š
```bash
cd llm_tokenizers
```
3. ä½¿ç”¨ pip å®‰è£…ä¾èµ–ï¼š
```bash
pip install -r requirements.txt
```

#### è½¯ä»¶åŒ…å®‰è£…ç¨‹

#### ä½¿ç”¨è¯´æ˜

1.  ä½¿ç”¨ pip å®‰è£…
```bash
pip install llm_tokenizers 
```

#### é¡¹ç›®æ‰“åŒ…

1.  ç¡®ä¿å·²å®‰è£…æ„å»ºå·¥å…·ï¼š
```bash
pip install build
```
 
2. åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹æ‰§è¡Œæ‰“åŒ…å‘½ä»¤ï¼š
```bash
python -m build
```
æ‰“åŒ…å®Œæˆåï¼Œç”Ÿæˆçš„ `.whl` å’Œ `.tar.gz` æ–‡ä»¶ä¼šä¿å­˜åœ¨ `dist/` ç›®å½•ä¸‹ã€‚

3. å®‰è£…æ‰“åŒ…å¥½çš„ `.whl` æ–‡ä»¶ï¼ˆä»¥ç”Ÿæˆçš„æ–‡ä»¶åä¸ºä¾‹ï¼‰ï¼š
```bash
pip install dist/llm_tokenizers-0.1.0-py3-none-any.whl
```

#### API è°ƒç”¨è¯´æ˜

ä½ å¯ä»¥é€šè¿‡å¯¼å…¥ `DeepSeekTokenizer` ç±»æ¥ç›´æ¥ä½¿ç”¨å®ƒæä¾›çš„åŠŸèƒ½ã€‚ä»¥ä¸‹æ˜¯å®Œæ•´çš„ä½¿ç”¨æ•™ç¨‹ã€‚

```python
from llm_tokenizers.deepseek_tokenizer import DeepSeekTokenizer
```

##### 1. è·å– Tokenizer æ ‡è¯†

```python
print(DeepSeekTokenizer.id())  # è¾“å‡º: deepseek
```


> `id()` æ–¹æ³•è¿”å›è¯¥ Tokenizer çš„å”¯ä¸€æ ‡è¯†ç¬¦ï¼Œå¯ç”¨äºç¨‹åºä¸­è¯†åˆ«å½“å‰ä½¿ç”¨çš„æ˜¯å“ªä¸ª Tokenizerã€‚

---

##### 2. ç¼–ç æ–‡æœ¬ä¸º token ID åˆ—è¡¨

```python
text = "Hello, world!"
token_ids = DeepSeekTokenizer.encode(text)
print(token_ids)  # è¾“å‡º: [åˆ—è¡¨å½¢å¼çš„ token IDs]
```


> `encode(text: str) -> List[int]`  
> å°†è¾“å…¥çš„å­—ç¬¦ä¸²æ–‡æœ¬ç¼–ç ä¸ºå¯¹åº”çš„ token ç¼–ç åˆ—è¡¨ã€‚

---

##### 3. è§£ç  token ID ä¸ºåŸå§‹æ–‡æœ¬

```python
decoded_text = DeepSeekTokenizer.decode(token_ids)
print(decoded_text)  # è¾“å‡º: Hello, world!
```


> `decode(data: Union[int, List[int], np.ndarray, torch.Tensor, tf.Tensor]) -> str`  
> æ”¯æŒå¤šç§æ•°æ®ç±»å‹è¾“å…¥ï¼Œè¿”å›è§£ç åçš„å­—ç¬¦ä¸²ã€‚

---

##### 4. ç»Ÿè®¡ token æ•°é‡

```python
token_count = DeepSeekTokenizer.tokens_len(text)
print(f"Token count: {token_count}")  # ç¤ºä¾‹è¾“å‡º: Token count: 5
```


> `tokens_len(text: str)`  
> è¿”å›è¾“å…¥æ–‡æœ¬è¢«ç¼–ç åçš„ token æ•°é‡ã€‚

---

##### âœ… ä½¿ç”¨ç¤ºä¾‹æ±‡æ€»

```python
from llm_tokenizers.deepseek_tokenizer import DeepSeekTokenizer

# è·å–æ ‡è¯†
print("Tokenizer ID:", DeepSeekTokenizer.id())

# ç¼–ç æ–‡æœ¬
text = "Hello, deepseek tokenizer!"
token_ids = DeepSeekTokenizer.encode(text)
print("Encoded tokens:", token_ids)

# è§£ç  token
decoded = DeepSeekTokenizer.decode(token_ids)
print("Decoded text:", decoded)

# ç»Ÿè®¡ token æ•°é‡
print("Token count:", DeepSeekTokenizer.tokens_len(text))
```
---

##### ğŸ“Œ æ³¨æ„äº‹é¡¹ï¼š

- `DeepSeekTokenizer` æ˜¯ä¸€ä¸ª **ç±»æ–¹æ³•é©±åŠ¨** çš„å·¥å…·ç±»ï¼Œæ‰€æœ‰æ–¹æ³•å‡ä¸º `@classmethod`ï¼Œæ— éœ€å®ä¾‹åŒ–å³å¯è°ƒç”¨ã€‚
- ä¾èµ–çš„ `transformers` æ¨¡å‹æ–‡ä»¶åº”æ”¾åœ¨ `resources/deepseek_tokenizer/` ç›®å½•ä¸‹ã€‚
- è‹¥ä½¿ç”¨ `np.ndarray`, `torch.Tensor`, `tf.Tensor` ç±»å‹çš„æ•°æ®ï¼Œéœ€ç¡®ä¿å·²å®‰è£…å¯¹åº”åº“ï¼ˆå¦‚ `numpy`, `torch`, `tensorflow`ï¼‰ã€‚


#### å‘½ä»¤è¡Œè°ƒç”¨

åœ¨å®Œæˆé¡¹ç›®å®‰è£…åï¼Œå¦‚ `whl`å®‰è£…åï¼Œæ‰§è¡Œ `llm-token` å‘½ä»¤

```bash
llm-token [é€‰é¡¹]
```
##### å‘½ä»¤è¡Œå‚æ•°è¯´æ˜

| å‚æ•° | å…¨ç§° | è¯´æ˜ | ç¤ºä¾‹ |
|------|------|------|------|
| `-t` | `--tokenizer` | æŒ‡å®šè¦ä½¿ç”¨çš„ tokenizer ç±»å‹ | `llm-token -t deepseek -i "Hello"` |
| `-f` | `--file` | æŒ‡å®šè¾“å…¥æ–‡ä»¶è·¯å¾„ | `llm-token -f ./input.txt` |
| `-u` | `--url` | æŒ‡å®šè¾“å…¥ URL è·¯å¾„ | `llm-token -u https://example.com/text` |
| `-o` | `--output` | æŒ‡å®šè¾“å‡ºæ–‡ä»¶è·¯å¾„ | `llm-token -i "Hello" -o ./output.txt` |
| `-c` | `--count` | ç»Ÿè®¡ tokens é•¿åº¦ | `llm-token -c -i "Hello world"` |
| `-i` | `--input` | ç›´æ¥è¾“å…¥æ–‡æœ¬å†…å®¹ | `llm-token -i "Hello world"` |
| `--read-charset` |  | æŒ‡å®šè¯»å–æ–‡ä»¶çš„å­—ç¬¦é›† | `llm-token -f ./input.txt --read-charset gbk` |

##### ä½¿ç”¨ç¤ºä¾‹

1. **ç›´æ¥è¾“å…¥æ–‡æœ¬è¿›è¡Œç¼–ç **ï¼š
   ```bash
   llm-token -i "Hello, world!"
   ```
2. **ç»Ÿè®¡æ–‡æœ¬çš„ token æ•°é‡**ï¼š
   ```bash
   llm-token -c -i "Hello, world!"
   # è¾“å‡ºç¤ºä¾‹: tokens count: 5
   ```
3. **ä»æ–‡ä»¶è¯»å–å†…å®¹è¿›è¡Œç¼–ç **ï¼š
   ```bash
   llm-token -f ./input.txt
   ```
4. **ä» URL è·å–å†…å®¹è¿›è¡Œç¼–ç **ï¼š
   ```bash
   llm-token -u https://example.com/sample.txt
   ```
5. **æŒ‡å®š tokenizer ç±»å‹**ï¼š
   ```bash
   llm-token -t deepseek -i "Hello, world!"
   ```
6. **å°†ç»“æœè¾“å‡ºåˆ°æ–‡ä»¶**ï¼š
   ```bash
   llm-token -i "Hello, world!" -o ./encoded_output.txt
   ```
7. **æŒ‡å®šæ–‡ä»¶è¯»å–å­—ç¬¦é›†**ï¼š
   ```bash
   llm-token -f ./chinese_text.txt --read-charset gbk
   ```
##### ä¼˜å…ˆçº§è¯´æ˜

å½“åŒæ—¶æŒ‡å®šå¤šç§è¾“å…¥æ–¹å¼æ—¶ï¼Œç¨‹åºæŒ‰ç…§ä»¥ä¸‹ä¼˜å…ˆçº§å¤„ç†ï¼š
1. `-i` / `--input` (ç›´æ¥è¾“å…¥æ–‡æœ¬)
2. `-f` / `--file` (æ–‡ä»¶è¾“å…¥)
3. `-u` / `--url` (URLè¾“å…¥)

##### æ³¨æ„äº‹é¡¹

- è‡³å°‘éœ€è¦æŒ‡å®šä¸€ç§è¾“å…¥æ–¹å¼ï¼ˆ`-i`ã€`-f` æˆ– `-u`ï¼‰
- ä½¿ç”¨ `-c` å‚æ•°æ—¶ï¼Œåªä¼šè¾“å‡º token æ•°é‡ï¼Œä¸ä¼šè¾“å‡ºç¼–ç ç»“æœ
- è¾“å‡ºé»˜è®¤æ‰“å°åˆ°æ§åˆ¶å°ï¼Œä½¿ç”¨ `-o` å‚æ•°å¯æŒ‡å®šè¾“å‡ºæ–‡ä»¶

#### å‚ä¸è´¡çŒ®

1.  Fork æœ¬ä»“åº“
2.  æ–°å»º Feat_xxx åˆ†æ”¯
3.  æäº¤ä»£ç 
4.  æ–°å»º Pull Request