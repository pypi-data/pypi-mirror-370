# Mixed Provider Workflow Example
# Demonstrates using both LLM and Python execution

name: "Mixed Provider Workflow"
description: "Combines LLM tasks with Python code execution"
version: "1.0"

# Workflow configuration
timeout: 180
wait_for_completion: true

# Tasks using different providers
tasks:
  - id: "generate_numbers"
    method: "llm/chat"
    priority: 3
    parameters:
      model: "llama3.2"
      messages:
        - role: "user"
          content: "Generate 5 random numbers between 1 and 100. Reply with just the numbers separated by commas, nothing else."

  - id: "calculate_stats"
    method: "python/execute"
    priority: 2
    dependencies: ["generate_numbers"]
    parameters:
      file: "examples/scripts/calculate_stats.py"
      context:
        numbers_str: "${generate_numbers.response}"
      timeout: 30

  - id: "summarize_results"
    method: "llm/chat"
    priority: 1
    dependencies: ["calculate_stats"]
    parameters:
      model: "llama3.2"
      messages:
        - role: "user"
          content: "Summarize these statistical results in a friendly, easy-to-understand way: ${calculate_stats.result}"