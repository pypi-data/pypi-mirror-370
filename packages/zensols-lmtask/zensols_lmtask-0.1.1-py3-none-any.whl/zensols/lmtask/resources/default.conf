#@meta {author: "landes"}
#@meta {desc: "application/API defaults", date: "2024-12-12"}


## Generation
#
[lmtask_default]
# whether to cache LLM responses
cache = False
# the default model (configured in the generator instance)
base_generator = lmtask_llama_base_generator
base_resource = lmtask_llama_base_resource
instruct_generator = lmtask_llama_instruct_generator
instruct_resource = lmtask_llama_instruct_resource

# model hyperparameters
[lmtask_model_generate_args]
# whether or not to use sampling ; use greedy decoding otherwise
#
#do_sample = True

# number of most likely tokens the model considers when generating text
#
#top_k = 1

# the number of independently computed returned sequences for each element in
# the batch
#
#num_return_sequences = 1

# maximum numbers of tokens to generate, ignoring the number of tokens in the prompt
#
max_new_tokens = 256

# Maximum length the generated tokens can have. Corresponds to the length of
# the input prompt + max_new_tokens. Its effect is overridden by
# max_new_tokens, if also set.
#
#max_length = 256

# the probability of how to distribute over the vocab; the lower it is the more
# the output follows the training data
#
temperature = 0.1


## Train
#
[lmtask_trainer_default]
# trainer truncates by default the sequences to the max_seq_length
max_seq_length = 2048
# directory where the model checkpoints are written
checkpoint_dir = ${default:data_dir}/model/base
# the directory to save the Peft model
peft_output_dir = ${default:data_dir}/model/peft
# the directory to save the base + peft in one model (if set)
merged_output_dir = None
## the check point to train from
## models:
# - unsloth/llama-3-70b-bnb-4bit
# - unsloth/llama-3-8b-Instruct-bnb-4bit
# - meta-llama/Llama-3.1-8B
# - meta-llama/Llama-3.1-8B-Instruct
source_model = meta-llama/Llama-3.1-8B
