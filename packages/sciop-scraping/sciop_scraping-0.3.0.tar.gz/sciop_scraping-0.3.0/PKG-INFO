Metadata-Version: 2.1
Name: sciop-scraping
Version: 0.3.0
Summary: scraping code to use with sciop-coordinated scrapes
Author-Email: sneakers-the-rat <sneakers-the-rat@protonmail.com>
License: EUPL-1.2
Project-URL: repository, https://codeberg.org/safeguarding/sciop-scraping
Project-URL: homepage, https://codeberg.org/safeguarding/sciop-scraping
Requires-Python: >=3.11
Requires-Dist: sciop-cli>=0.2.0
Requires-Dist: click>=8.2.1
Requires-Dist: scrapy>=2.13.0
Requires-Dist: tqdm>=4.67.1
Provides-Extra: test
Requires-Dist: pytest>=8.3.5; extra == "test"
Requires-Dist: bagit>=1.9.0; extra == "test"
Provides-Extra: dev
Requires-Dist: black>=25.1.0; extra == "dev"
Requires-Dist: ruff>=0.12.4; extra == "dev"
Description-Content-Type: text/markdown

# sciop-scraping

A (yet-to-be-named) tool to enable scraping of very large datasets to be distributed across multiple volunteers and then reassembled as dataset parts on [sciop](https://sciop.net).

**NB. this is currently a work in progress,** and it depends on planned features in [sciop](https://codeberg.org/safeguarding/sciop) that are not yet stabilised. If you're interested in contributing, experience of or interest in web scraping, Python CLI tools and/or REST APIs would be very helpful.

We absolutely want this to be as easy to use as possible, so as soon as we can we'll be adding detailed documentation and putting out a call for wider testing. Watch this space!

In the meantime, please [subscribe to the Safeguarding Research & Data forum](https://forum.safeguar.de) for pointers to datasets that need saving along with help & advice with collecting them, preparing for upload and creating torrents.
