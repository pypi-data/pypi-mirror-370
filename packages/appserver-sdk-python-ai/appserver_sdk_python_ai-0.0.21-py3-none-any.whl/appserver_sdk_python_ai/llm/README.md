# M√≥dulo LLM - AppServer SDK Python AI

Um m√≥dulo profissional para integra√ß√£o com modelos de linguagem (LLM) e APIs de intelig√™ncia artificial.

## üöÄ Caracter√≠sticas

- **üîÑ Cliente Ass√≠ncrono e S√≠ncrono**: Suporte completo para opera√ß√µes s√≠ncronas e ass√≠ncronas
- **üõ°Ô∏è Retry Autom√°tico**: Sistema robusto de retry com backoff exponencial
- **üìä Modelos Pydantic**: Valida√ß√£o de dados com type hints completos
- **üîå M√∫ltiplos Provedores**: Suporte para diferentes APIs de IA (OpenAI, Anthropic, etc.)
- **üìù Logging Estruturado**: Sistema de logs detalhado para debugging e monitoramento
- **‚ö° Performance Otimizada**: Conex√µes reutiliz√°veis e pool de conex√µes
- **üîê Seguran√ßa**: Gerenciamento seguro de API keys e autentica√ß√£o
- **üéØ Interface Simples**: API intuitiva para uso b√°sico e avan√ßado

## üìÅ Estrutura do M√≥dulo

```
llm/
‚îú‚îÄ‚îÄ __init__.py                 # Inicializa√ß√£o e exports principais
‚îú‚îÄ‚îÄ README.md                   # Esta documenta√ß√£o
‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ model_manager.py       # Gerenciamento de modelos
‚îÇ   ‚îî‚îÄ‚îÄ token_counter.py       # Contagem de tokens
‚îú‚îÄ‚îÄ service/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ ai_service.py          # Servi√ßo principal de IA
‚îÇ   ‚îî‚îÄ‚îÄ providers/             # Provedores espec√≠ficos
‚îî‚îÄ‚îÄ exceptions/
    ‚îî‚îÄ‚îÄ llm_exceptions.py      # Exce√ß√µes customizadas
```

## üì¶ Instala√ß√£o e Depend√™ncias

### Depend√™ncias Principais
- **Python**: >= 3.8
- **pydantic**: >= 2.0.0 - Valida√ß√£o de dados e modelos
- **httpx**: >= 0.24.0 - Cliente HTTP ass√≠ncrono
- **typing-extensions**: >= 4.0.0 - Extens√µes de tipagem
- **structlog**: >= 23.0.0 - Logging estruturado
- **psutil**: >= 5.9.0 - M√©tricas do sistema

### Instala√ß√£o B√°sica
```bash
# Apenas funcionalidades core
pip install appserver-sdk-python-ai[llm]
```

### Instala√ß√£o com Extras
```bash
# Com suporte a modelos locais
pip install appserver-sdk-python-ai[llm,local-models]

# Com ferramentas de an√°lise
pip install appserver-sdk-python-ai[llm,analysis]

# Instala√ß√£o completa para desenvolvimento
pip install appserver-sdk-python-ai[llm,dev,all]
```

### Depend√™ncias Opcionais

#### Para Modelos Locais
```bash
# Para modelos HuggingFace
pip install transformers>=4.20.0 torch>=1.12.0

# Para modelos Llama
pip install llama-cpp-python>=0.2.0

# Para modelos ONNX
pip install onnxruntime>=1.12.0
```

#### Para An√°lise Avan√ßada
```bash
# Para an√°lise de texto
pip install nltk>=3.8 spacy>=3.4.0

# Para processamento de linguagem natural
pip install textblob>=0.17.0
```

### Verifica√ß√£o de Depend√™ncias
```python
from appserver_sdk_python_ai.llm.utils.dependency_checker import check_dependencies

# Verificar depend√™ncias principais
result = check_dependencies()
if result['missing']:
    print(f"Depend√™ncias faltando: {result['missing']}")
else:
    print("‚úÖ Todas as depend√™ncias principais est√£o instaladas")
```

## üî• Uso R√°pido

### Cliente B√°sico
```python
from appserver_sdk_python_ai.llm import AIService

# Configurar cliente
ai_service = AIService(
    api_key="sua-api-key",
    base_url="https://api.appserver.com.br/ai/v1"
)

# Fazer requisi√ß√£o simples
response = ai_service.chat(
    prompt="Explique machine learning em termos simples",
    model="gpt-4",
    max_tokens=500
)

print(response.content)
print(f"Tokens utilizados: {response.usage.total_tokens}")
```

### Chat com Contexto
```python
from appserver_sdk_python_ai.llm import AIService, Message

ai_service = AIService(api_key="sua-api-key")

# Conversa com contexto
messages = [
    Message(role="system", content="Voc√™ √© um assistente especializado em Python."),
    Message(role="user", content="Como criar uma fun√ß√£o recursiva?"),
    Message(role="assistant", content="Uma fun√ß√£o recursiva √© uma fun√ß√£o que chama a si mesma..."),
    Message(role="user", content="Pode dar um exemplo pr√°tico?")
]

response = ai_service.chat_with_messages(
    messages=messages,
    model="gpt-4",
    temperature=0.7
)

print(response.content)
```

### Cliente Ass√≠ncrono
```python
import asyncio
from appserver_sdk_python_ai.llm import AsyncAIService

async def main():
    ai_service = AsyncAIService(api_key="sua-api-key")
    
    response = await ai_service.chat(
        prompt="O que √© intelig√™ncia artificial?",
        model="gpt-3.5-turbo"
    )
    
    print(response.content)
    await ai_service.close()

asyncio.run(main())
```

## ‚öôÔ∏è Configura√ß√£o Avan√ßada

### AIConfig - Todas as Op√ß√µes
```python
from appserver_sdk_python_ai.llm import AIService, AIConfig

config = AIConfig(
    # Configura√ß√µes de rede
    base_url="https://api.appserver.com.br/ai/v1",
    api_key="sua-api-key",
    timeout=30,                    # Timeout em segundos
    verify_ssl=True,               # Verificar certificados SSL
    
    # Configura√ß√µes de retry
    max_retries=3,                 # M√°ximo de tentativas
    retry_delay=1.0,               # Delay inicial entre tentativas
    backoff_factor=2.0,            # Fator de backoff exponencial
    
    # Configura√ß√µes de logging
    debug=False,                   # Modo debug
    log_requests=True,             # Log de requisi√ß√µes
    log_responses=False,           # Log de respostas (cuidado com dados sens√≠veis)
    
    # Headers customizados
    headers={
        "User-Agent": "AppServer-SDK/1.0",
        "Accept": "application/json"
    }
)

ai_service = AIService(config=config)
```

### Modelos de Dados

#### ChatRequest
```python
from appserver_sdk_python_ai.llm import ChatRequest

request = ChatRequest(
    prompt="Sua pergunta aqui",
    model="gpt-4",                 # Modelo a ser usado
    max_tokens=1000,               # M√°ximo de tokens na resposta
    temperature=0.7,               # Criatividade (0.0 a 1.0)
    top_p=1.0,                     # Nucleus sampling
    frequency_penalty=0.0,         # Penalidade de frequ√™ncia
    presence_penalty=0.0,          # Penalidade de presen√ßa
    stop=["\n\n"],                # Tokens de parada
    stream=False                   # Streaming de resposta
)
```

#### ChatResponse
```python
# A resposta cont√©m:
response.content          # Conte√∫do da resposta
response.model           # Modelo usado
response.usage           # Informa√ß√µes de uso (tokens)
response.finish_reason   # Raz√£o do fim da gera√ß√£o
response.created_at      # Timestamp da cria√ß√£o
response.id              # ID √∫nico da resposta
```

## üéØ Casos de Uso Pr√°ticos

### 1. An√°lise de Sentimento
```python
from appserver_sdk_python_ai.llm import AIService

def analisar_sentimento(texto):
    ai_service = AIService(api_key="sua-api-key")
    
    prompt = f"""
    Analise o sentimento do seguinte texto e retorne apenas uma das op√ß√µes:
    - POSITIVO
    - NEGATIVO
    - NEUTRO
    
    Texto: "{texto}"
    
    Sentimento:
    """
    
    response = ai_service.chat(
        prompt=prompt,
        model="gpt-3.5-turbo",
        max_tokens=10,
        temperature=0.1
    )
    
    return response.content.strip()

# Exemplo de uso
textos = [
    "Adorei o produto, superou minhas expectativas!",
    "O servi√ßo foi terr√≠vel, n√£o recomendo.",
    "O produto chegou no prazo esperado."
]

for texto in textos:
    sentimento = analisar_sentimento(texto)
    print(f"'{texto}' -> {sentimento}")
```

### 2. Gera√ß√£o de Resumos
```python
from appserver_sdk_python_ai.llm import AIService

def gerar_resumo(texto, max_palavras=100):
    ai_service = AIService(api_key="sua-api-key")
    
    prompt = f"""
    Crie um resumo conciso do seguinte texto em no m√°ximo {max_palavras} palavras.
    Mantenha os pontos principais e informa√ß√µes essenciais.
    
    Texto:
    {texto}
    
    Resumo:
    """
    
    response = ai_service.chat(
        prompt=prompt,
        model="gpt-4",
        max_tokens=max_palavras * 2,  # Margem de seguran√ßa
        temperature=0.3
    )
    
    return response.content.strip()

# Exemplo de uso
texto_longo = """
A intelig√™ncia artificial (IA) √© uma √°rea da ci√™ncia da computa√ß√£o que se concentra 
no desenvolvimento de sistemas capazes de realizar tarefas que normalmente requerem 
intelig√™ncia humana. Isso inclui aprendizado, racioc√≠nio, percep√ß√£o, compreens√£o 
de linguagem natural e resolu√ß√£o de problemas. A IA tem aplica√ß√µes em diversos 
setores, desde sa√∫de e educa√ß√£o at√© transporte e entretenimento.
"""

resumo = gerar_resumo(texto_longo, max_palavras=50)
print(f"Resumo: {resumo}")
```

### 3. Chatbot com Mem√≥ria
```python
from appserver_sdk_python_ai.llm import AIService, Message

class ChatBot:
    def __init__(self, api_key, system_prompt="Voc√™ √© um assistente √∫til."):
        self.ai_service = AIService(api_key=api_key)
        self.messages = [Message(role="system", content=system_prompt)]
    
    def chat(self, user_input):
        # Adicionar mensagem do usu√°rio
        self.messages.append(Message(role="user", content=user_input))
        
        # Obter resposta
        response = self.ai_service.chat_with_messages(
            messages=self.messages,
            model="gpt-3.5-turbo",
            max_tokens=500,
            temperature=0.7
        )
        
        # Adicionar resposta do assistente ao hist√≥rico
        self.messages.append(Message(role="assistant", content=response.content))
        
        return response.content
    
    def reset(self):
        # Manter apenas a mensagem do sistema
        self.messages = self.messages[:1]

# Exemplo de uso
bot = ChatBot(
    api_key="sua-api-key",
    system_prompt="Voc√™ √© um especialista em Python. Responda de forma did√°tica."
)

print("ChatBot iniciado! Digite 'sair' para encerrar.")
while True:
    user_input = input("Voc√™: ")
    if user_input.lower() == 'sair':
        break
    
    response = bot.chat(user_input)
    print(f"Bot: {response}")
```

### 4. Processamento em Lote Ass√≠ncrono
```python
import asyncio
from appserver_sdk_python_ai.llm import AsyncAIService

async def processar_textos_em_lote(textos, prompt_template):
    ai_service = AsyncAIService(api_key="sua-api-key")
    
    async def processar_texto(texto):
        prompt = prompt_template.format(texto=texto)
        response = await ai_service.chat(
            prompt=prompt,
            model="gpt-3.5-turbo",
            max_tokens=200
        )
        return {"texto_original": texto, "resultado": response.content}
    
    # Processar todos os textos em paralelo
    tasks = [processar_texto(texto) for texto in textos]
    resultados = await asyncio.gather(*tasks)
    
    await ai_service.close()
    return resultados

# Exemplo de uso
async def main():
    textos = [
        "Python √© uma linguagem de programa√ß√£o.",
        "Machine learning √© um subcampo da IA.",
        "APIs facilitam a integra√ß√£o entre sistemas."
    ]
    
    prompt_template = """
    Traduza o seguinte texto para ingl√™s:
    
    Texto: {texto}
    
    Tradu√ß√£o:
    """
    
    resultados = await processar_textos_em_lote(textos, prompt_template)
    
    for resultado in resultados:
        print(f"Original: {resultado['texto_original']}")
        print(f"Tradu√ß√£o: {resultado['resultado']}")
        print("-" * 50)

# asyncio.run(main())
```

### 5. Extra√ß√£o de Informa√ß√µes Estruturadas
```python
from appserver_sdk_python_ai.llm import AIService
import json

def extrair_informacoes_contato(texto):
    ai_service = AIService(api_key="sua-api-key")
    
    prompt = f"""
    Extraia as informa√ß√µes de contato do seguinte texto e retorne em formato JSON.
    Se alguma informa√ß√£o n√£o estiver dispon√≠vel, use null.
    
    Formato esperado:
    {{
        "nome": "string",
        "email": "string",
        "telefone": "string",
        "empresa": "string",
        "cargo": "string"
    }}
    
    Texto:
    {texto}
    
    JSON:
    """
    
    response = ai_service.chat(
        prompt=prompt,
        model="gpt-4",
        max_tokens=300,
        temperature=0.1
    )
    
    try:
        return json.loads(response.content)
    except json.JSONDecodeError:
        return None

# Exemplo de uso
texto_contato = """
Ol√°, meu nome √© Jo√£o Silva e trabalho como Desenvolvedor Senior na TechCorp.
Voc√™ pode me contatar pelo email joao.silva@techcorp.com ou pelo telefone (11) 99999-9999.
Estou interessado em discutir oportunidades de parceria.
"""

info = extrair_informacoes_contato(texto_contato)
if info:
    print(json.dumps(info, indent=2, ensure_ascii=False))
else:
    print("N√£o foi poss√≠vel extrair as informa√ß√µes.")
```

## üîÑ Streaming de Respostas

```python
from appserver_sdk_python_ai.llm import AsyncAIService

async def chat_com_streaming():
    ai_service = AsyncAIService(api_key="sua-api-key")
    
    prompt = "Conte uma hist√≥ria interessante sobre intelig√™ncia artificial"
    
    print("Resposta: ", end="", flush=True)
    
    async for chunk in ai_service.chat_stream(
        prompt=prompt,
        model="gpt-4",
        max_tokens=500
    ):
        print(chunk.content, end="", flush=True)
    
    print()  # Nova linha no final
    await ai_service.close()

# asyncio.run(chat_com_streaming())
```

## üìä M√©tricas e Monitoramento

O m√≥dulo LLM inclui um sistema abrangente de m√©tricas para monitoramento de performance e uso:

### Coleta Autom√°tica de M√©tricas

```python
from appserver_sdk_python_ai.llm import (
    get_metrics_summary,
    get_metrics_collector,
    export_metrics
)
from appserver_sdk_python_ai.llm.service.client import MockLLMClient

# As m√©tricas s√£o coletadas automaticamente durante opera√ß√µes
client = MockLLMClient()
client.authenticate()

# Opera√ß√µes s√£o automaticamente monitoradas
models = client.list_models()
response = client.generate_text("Hello world", models[0])

# Visualizar resumo das m√©tricas
summary = get_metrics_summary()
print(f"Total de opera√ß√µes: {summary['operation_stats']}")
print(f"M√©tricas do sistema: {summary['system']}")
```

### M√©tricas Customizadas

```python
from appserver_sdk_python_ai.llm import (
    record_operation_metric,
    increment_counter,
    set_gauge,
    OperationStatus
)
import time

# Registrar opera√ß√£o customizada
start_time = time.time()
# ... sua opera√ß√£o aqui ...
duration_ms = (time.time() - start_time) * 1000

record_operation_metric(
    operation_type="custom_processing",
    duration_ms=duration_ms,
    status=OperationStatus.SUCCESS,
    model_name="custom-model",
    token_count=1500
)

# Incrementar contadores
increment_counter("api_requests", 1, {"endpoint": "/chat"})

# Definir gauges
set_gauge("active_connections", 25.0)
```

### Exporta√ß√£o de M√©tricas

```python
# Exportar para diferentes formatos
export_metrics(format_type="json", file_path="metrics.json")
export_metrics(format_type="csv", file_path="metrics.csv")
export_metrics(format_type="prometheus", file_path="metrics.prom")

# Obter m√©tricas em tempo real
collector = get_metrics_collector()
stats = collector.get_histogram_stats("operation_duration_ms")
print(f"Lat√™ncia P95: {stats['p95']:.2f}ms")
```

### Monitoramento de Performance

```python
# Visualizar estat√≠sticas de performance
summary = get_metrics_summary(include_system=True)

for op_type, stats in summary['operation_stats'].items():
    success_rate = (stats['success_count'] / stats['count']) * 100
    print(f"{op_type}:")
    print(f"  Taxa de sucesso: {success_rate:.1f}%")
    print(f"  Dura√ß√£o m√©dia: {stats['avg_duration_ms']:.2f}ms")

# M√©tricas do sistema
sys_metrics = summary['system']
print(f"Uso de mem√≥ria: {sys_metrics['memory']['percent']:.1f}%")
print(f"Uso de CPU: {sys_metrics['cpu']['percent']:.1f}%")
```

## üö® Tratamento de Erros

```python
from appserver_sdk_python_ai.llm import (
    AIService,
    AIException,
    AIConnectionError,
    AIAuthenticationError,
    AIRateLimitError,
    AITimeoutError,
    AIModelNotFoundError
)

def chat_com_tratamento_erro(prompt):
    ai_service = AIService(api_key="sua-api-key")
    
    try:
        response = ai_service.chat(
            prompt=prompt,
            model="gpt-4",
            max_tokens=500
        )
        return response.content
        
    except AIAuthenticationError:
        return "Erro: API key inv√°lida ou expirada"
    except AIRateLimitError as e:
        return f"Erro: Limite de taxa excedido. Tente novamente em {e.retry_after} segundos"
    except AIModelNotFoundError:
        return "Erro: Modelo especificado n√£o encontrado"
    except AITimeoutError:
        return "Erro: Timeout na requisi√ß√£o"
    except AIConnectionError:
        return "Erro: Problema de conex√£o com a API"
    except AIException as e:
        return f"Erro geral da API: {e}"
    except Exception as e:
        return f"Erro inesperado: {e}"

# Exemplo de uso
resultado = chat_com_tratamento_erro("Explique quantum computing")
print(resultado)
```

## üìä Monitoramento e M√©tricas

### Contagem de Tokens
```python
from appserver_sdk_python_ai.llm import TokenCounter

# Contar tokens antes de enviar
counter = TokenCounter()

prompt = "Explique machine learning em detalhes"
tokens_prompt = counter.count_tokens(prompt, model="gpt-4")
print(f"Tokens no prompt: {tokens_prompt}")

# Estimar custo
custo_estimado = counter.estimate_cost(
    prompt_tokens=tokens_prompt,
    completion_tokens=500,  # Estimativa
    model="gpt-4"
)
print(f"Custo estimado: ${custo_estimado:.4f}")
```

### Logging Personalizado
```python
import logging
from appserver_sdk_python_ai.llm import AIService

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

logger = logging.getLogger("meu_app")

ai_service = AIService(
    api_key="sua-api-key",
    debug=True  # Habilita logs detalhados
)

# Os logs ser√£o automaticamente gerados
response = ai_service.chat(
    prompt="Teste de logging",
    model="gpt-3.5-turbo"
)
```

## üöÄ Melhorias e Funcionalidades Avan√ßadas

O m√≥dulo LLM foi aprimorado com diversas funcionalidades avan√ßadas para melhor performance, robustez e usabilidade:

### ‚úÖ Sistema de Cache Inteligente
- **Cache LRU thread-safe** com TTL configur√°vel
- **Decorador @cache_result** para f√°cil aplica√ß√£o
- **Estat√≠sticas de performance** (hits/misses)
- **Monitoramento de uso de mem√≥ria**
- **Limpeza autom√°tica** de entradas expiradas

```python
from appserver_sdk_python_ai.llm import get_cache_stats, clear_cache

# Verificar estat√≠sticas do cache
stats = get_cache_stats()
print(f"Taxa de acerto: {stats['hit_rate']:.2%}")

# Limpar cache se necess√°rio
clear_cache()
```

### ‚úÖ Valida√ß√£o Robusta
- **M√∫ltiplos n√≠veis de valida√ß√£o** (LENIENT, MODERATE, STRICT)
- **Validadores espec√≠ficos** para modelos, tokens, configura√ß√µes e texto
- **Mensagens de erro descritivas** e categorizadas
- **Separa√ß√£o entre warnings e errors**

```python
from appserver_sdk_python_ai.llm import validate_model_name, ValidationLevel

result = validate_model_name("gpt-4", ValidationLevel.MODERATE)
if result.is_valid:
    print("Modelo v√°lido!")
else:
    print(f"Erro: {result.error_message}")
```

### ‚úÖ Logging Estruturado
- **Logs em formato JSON** para an√°lise automatizada
- **Contexto de opera√ß√£o** com metadata
- **M√©tricas de performance** autom√°ticas
- **Rota√ß√£o autom√°tica** de arquivos de log
- **Decorador @log_function_call** para logging autom√°tico

```python
from appserver_sdk_python_ai.llm import get_logger, OperationType

logger = get_logger()
logger.info("Opera√ß√£o executada", operation_type=OperationType.TOKEN_COUNT)
```

### ‚úÖ Sistema de M√©tricas Completo
- **Coleta autom√°tica** de m√©tricas de performance
- **Tipos de m√©tricas**: contadores, gauges, histogramas, timers
- **Exporta√ß√£o** para JSON, CSV e Prometheus
- **Monitoramento** de sistema (CPU, mem√≥ria, disco)
- **M√©tricas de opera√ß√£o** com contexto detalhado

### ‚úÖ Documenta√ß√£o Interativa
- **Sistema de ajuda integrado** (`help_llm()`, `docs_llm()`)
- **Busca na documenta√ß√£o** (`search_llm_docs()`)
- **Exemplos pr√°ticos** e guias de solu√ß√£o de problemas
- **Refer√™ncia da API** em tempo de desenvolvimento

### ‚úÖ Utilit√°rios Avan√ßados
- **Listagem de modelos** organizados por provedor
- **Registro de modelos customizados** com valida√ß√£o
- **Modelos otimizados** para portugu√™s e multil√≠ngues
- **An√°lise de compatibilidade** de depend√™ncias

### üéØ Arquitetura Melhorada
- **Separa√ß√£o clara de responsabilidades** entre m√≥dulos
- **Extensibilidade** para novos provedores e modelos
- **Testabilidade** com estrutura modular
- **Performance otimizada** com cache integrado
- **Robustez** com valida√ß√£o e logging estruturado

### üìä Exemplo de Uso Integrado

```python
from appserver_sdk_python_ai.llm import (
    get_metrics_summary,
    help_llm,
    docs_llm,
    list_available_models
)
from appserver_sdk_python_ai.llm.service.client import MockLLMClient

# Documenta√ß√£o interativa
help_llm()  # Ajuda r√°pida
docs_llm()  # Documenta√ß√£o completa

# Listar modelos dispon√≠veis
models = list_available_models()
print(f"Modelos dispon√≠veis: {len(models)}")

# Cliente com m√©tricas autom√°ticas
client = MockLLMClient()
client.authenticate()

# Opera√ß√µes monitoradas automaticamente
response = client.generate_text("Hello world", "gpt-4")

# Visualizar m√©tricas coletadas
summary = get_metrics_summary(include_system=True)
print(f"Opera√ß√µes realizadas: {len(summary['operation_stats'])}")
print(f"Uso de mem√≥ria: {summary['system']['memory']['percent']:.1f}%")
```

## üîê Seguran√ßa e Boas Pr√°ticas

### Gerenciamento Seguro de API Keys
```python
import os
from appserver_sdk_python_ai.llm import AIService

# Usar vari√°veis de ambiente
ai_service = AIService(
    api_key=os.getenv("APPSERVER_API_KEY"),
    base_url=os.getenv("APPSERVER_BASE_URL", "https://api.appserver.com.br/ai/v1")
)

# Verificar se a API key est√° configurada
if not os.getenv("APPSERVER_API_KEY"):
    raise ValueError("API key n√£o configurada. Defina a vari√°vel APPSERVER_API_KEY")
```

### Valida√ß√£o de Entrada
```python
from appserver_sdk_python_ai.llm import AIService
import re

def validar_prompt(prompt):
    """Valida e sanitiza o prompt antes de enviar"""
    if not prompt or not prompt.strip():
        raise ValueError("Prompt n√£o pode estar vazio")
    
    if len(prompt) > 10000:  # Limite de caracteres
        raise ValueError("Prompt muito longo")
    
    # Remover caracteres potencialmente problem√°ticos
    prompt_limpo = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', prompt)
    
    return prompt_limpo.strip()

def chat_seguro(prompt):
    try:
        prompt_validado = validar_prompt(prompt)
        ai_service = AIService(api_key=os.getenv("APPSERVER_API_KEY"))
        
        response = ai_service.chat(
            prompt=prompt_validado,
            model="gpt-3.5-turbo",
            max_tokens=500
        )
        
        return response.content
        
    except ValueError as e:
        return f"Erro de valida√ß√£o: {e}"
    except Exception as e:
        return f"Erro: {e}"
```

## üöÄ Performance e Otimiza√ß√£o

### Pool de Conex√µes
```python
from appserver_sdk_python_ai.llm import AsyncAIService

# Cliente com pool de conex√µes otimizado
ai_service = AsyncAIService(
    api_key="sua-api-key",
    max_connections=100,
    max_keepalive_connections=20,
    timeout=30
)
```

### Cache de Respostas
```python
from appserver_sdk_python_ai.llm import AIService
import hashlib
import json
from functools import lru_cache

class CachedAIService:
    def __init__(self, api_key):
        self.ai_service = AIService(api_key=api_key)
        self._cache = {}
    
    def _get_cache_key(self, prompt, model, **kwargs):
        """Gera chave √∫nica para o cache"""
        cache_data = {
            "prompt": prompt,
            "model": model,
            **kwargs
        }
        cache_str = json.dumps(cache_data, sort_keys=True)
        return hashlib.md5(cache_str.encode()).hexdigest()
    
    def chat(self, prompt, model="gpt-3.5-turbo", **kwargs):
        cache_key = self._get_cache_key(prompt, model, **kwargs)
        
        # Verificar cache
        if cache_key in self._cache:
            print("Cache hit!")
            return self._cache[cache_key]
        
        # Fazer requisi√ß√£o
        response = self.ai_service.chat(
            prompt=prompt,
            model=model,
            **kwargs
        )
        
        # Armazenar no cache
        self._cache[cache_key] = response
        return response

# Uso
cached_ai = CachedAIService(api_key="sua-api-key")
response1 = cached_ai.chat("O que √© Python?")  # Requisi√ß√£o √† API
response2 = cached_ai.chat("O que √© Python?")  # Cache hit!
```

## üß™ Testes

```python
import pytest
from unittest.mock import Mock, patch
from appserver_sdk_python_ai.llm import AIService, ChatResponse

@pytest.fixture
def ai_service():
    return AIService(api_key="test-key")

def test_chat_success(ai_service):
    # Mock da resposta
    mock_response = ChatResponse(
        content="Resposta de teste",
        model="gpt-3.5-turbo",
        usage={"total_tokens": 50}
    )
    
    with patch.object(ai_service, 'chat', return_value=mock_response):
        response = ai_service.chat("Teste")
        assert response.content == "Resposta de teste"
        assert response.model == "gpt-3.5-turbo"

def test_chat_error(ai_service):
    with patch.object(ai_service, 'chat', side_effect=Exception("API Error")):
        with pytest.raises(Exception):
            ai_service.chat("Teste")

# Executar testes
# pytest src/appserver_sdk_python_ai/llm/tests/ -v
```

## ü§ù Contribui√ß√£o

Para contribuir com o m√≥dulo LLM:

1. **Novos Provedores**: Adicione suporte a novos provedores de IA
2. **Otimiza√ß√µes**: Melhore performance e efici√™ncia
3. **Funcionalidades**: Adicione novos recursos e capacidades
4. **Testes**: Expanda a cobertura de testes
5. **Documenta√ß√£o**: Melhore exemplos e documenta√ß√£o

```bash
# Executar testes do m√≥dulo LLM
python -m pytest src/appserver_sdk_python_ai/llm/tests/ -v

# Verificar cobertura
python -m pytest src/appserver_sdk_python_ai/llm/tests/ --cov=llm --cov-report=html
```

## üìö Recursos Adicionais

- **[üìö Documenta√ß√£o da API](https://docs.appserver.com.br/ai/)** - Documenta√ß√£o oficial online
- **[üí° Exemplos Pr√°ticos](../../../examples/llm/)** - Exemplos de uso organizados por funcionalidade
- **[üîß Documenta√ß√£o Interativa](docs/interactive_docs.py)** - Acesso din√¢mico a esta documenta√ß√£o
- **[üìã Changelog](../../../CHANGELOG.md)** - Hist√≥rico de vers√µes e mudan√ßas
- **[üÜò Issues e Suporte](https://github.com/appserver/appserver-sdk-python-ai/issues)** - Reportar problemas e solicitar ajuda

> **Nota**: Esta documenta√ß√£o √© a fonte √∫nica e consolidada para o m√≥dulo LLM. A documenta√ß√£o interativa carrega dinamicamente o conte√∫do deste arquivo para evitar redund√¢ncia.

## üìÑ Licen√ßa

Este m√≥dulo faz parte do AppServer SDK Python AI e segue a mesma licen√ßa do projeto principal.

---

**Vers√£o**: 1.0.0  
**√öltima atualiza√ß√£o**: 2024  
**Compatibilidade**: Python 3.8+  
**Desenvolvido com ‚ù§Ô∏è pela equipe AppServer**