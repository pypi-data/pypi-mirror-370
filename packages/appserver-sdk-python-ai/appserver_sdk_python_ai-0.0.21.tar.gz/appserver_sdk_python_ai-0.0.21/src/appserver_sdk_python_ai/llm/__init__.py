"""M√≥dulo LLM do AppServer SDK Python AI"""

import logging

__version__ = "1.0.0"

# Imports absolutos dentro do m√≥dulo
# Importar funcionalidades do core (cache, valida√ß√£o, logging)
from appserver_sdk_python_ai.llm.core import (
    # Logging
    LogLevel,
    # Metrics
    MetricType,
    OperationStatus,
    OperationType,
    # Validation
    ValidationLevel,
    # Cache
    cache_result,
    clear_cache,
    export_metrics,
    get_cache,
    get_cache_memory_usage,
    get_cache_stats,
    get_logger,
    get_metrics_collector,
    get_metrics_summary,
    increment_counter,
    log_function_call,
    record_operation_metric,
    set_gauge,
    validate_config,
    validate_model_name,
    validate_text,
    validate_token_count,
)
from appserver_sdk_python_ai.llm.core.enums import (
    ModelCapability,
    ModelProvider,
    ModelType,
    SupportedLanguage,
    TokenizationMethod,
)

# Importar documenta√ß√£o interativa
from appserver_sdk_python_ai.llm.docs import (
    docs as docs_llm,
)
from appserver_sdk_python_ai.llm.docs import (
    help as help_llm,
)
from appserver_sdk_python_ai.llm.docs import (
    search as search_llm_docs,
)
from appserver_sdk_python_ai.llm.exceptions import (
    LLMAuthenticationError,
    LLMConfigurationError,
    LLMContentFilterError,
    LLMError,
    LLMInvalidInputError,
    LLMModelNotFoundError,
    LLMNetworkError,
    LLMProviderError,
    LLMRateLimitError,
    LLMResponseError,
    LLMStreamingError,
    LLMTimeoutError,
    LLMTokenLimitError,
    LLMValidationError,
)
from appserver_sdk_python_ai.llm.service.ai_service import (
    AIConfig,
    AIService,
    AsyncAIService,
    ChatResponse,
    Message,
    StreamChunk,
    Usage,
)
from appserver_sdk_python_ai.llm.service.client import (
    LLMClient,
    MockLLMClient,
)
from appserver_sdk_python_ai.llm.service.token_service import (
    get_model_info,
    get_token_count,
    get_token_count_with_model,
    is_model_registered,
    list_available_models,
    register_custom_model,
)

# Importar utilit√°rios avan√ßados
from appserver_sdk_python_ai.llm.utils import (
    get_model_details,
    get_multilingual_models,
    get_portuguese_models,
    print_models_summary,
    register_custom_model_advanced,
)
from appserver_sdk_python_ai.llm.utils import (
    list_available_models as list_models_detailed,
)

# Importar funcionalidades comuns do m√≥dulo shared
from appserver_sdk_python_ai.shared import (
    DependencyChecker,
    HealthChecker,
    SDKLogger,
    VersionInfo,
)


def get_version_info():
    """Retorna informa√ß√µes sobre a vers√£o e depend√™ncias."""
    return VersionInfo.create_version_info(
        module_name="llm",
        module_version=__version__,
        dependencies=check_dependencies(),
        additional_info={
            "available_models": len(list_available_models()),
            "portuguese_models": len(get_portuguese_models()),
        },
    )


def check_dependencies():
    """Verifica se todas as depend√™ncias est√£o instaladas."""
    return DependencyChecker.check_dependencies(
        ["tiktoken", "openai", "anthropic", "requests"]
    )


def health_check():
    """Verifica a sa√∫de do m√≥dulo e suas depend√™ncias."""
    dependencies = check_dependencies()
    features = {
        "token_counting": True,
        "model_management": True,
        "portuguese_support": len(get_portuguese_models()) > 0,
        "custom_models": True,
    }

    return HealthChecker.create_health_report(
        module_name="llm",
        version=__version__,
        dependencies=dependencies,
        features=features,
        critical_deps=[],  # Nenhuma depend√™ncia √© cr√≠tica por padr√£o
        optional_deps=["tiktoken", "openai", "anthropic", "requests"],
    )


def print_status():
    """Imprime status do m√≥dulo."""
    health = health_check()

    # Adicionar informa√ß√µes espec√≠ficas do LLM
    print("=" * 60)
    print("M√ìDULO LLM - appserver_sdk_python_ai")
    print("=" * 60)
    print(f"Vers√£o: {__version__}")
    print(f"Status: {health['status']}")

    # Usar o m√©todo padr√£o para o resto
    HealthChecker.print_health_status(
        health, show_dependencies=True, show_features=True
    )

    # Informa√ß√µes adicionais espec√≠ficas do LLM
    print("\nü§ñ Informa√ß√µes dos modelos:")
    print(f"  ‚Ä¢ Total de modelos dispon√≠veis: {len(list_available_models())}")
    print(f"  ‚Ä¢ Modelos com suporte ao portugu√™s: {len(get_portuguese_models())}")

    print("\nüìã Provedores suportados:")
    providers = set()
    for model_info in list_available_models():
        if hasattr(model_info, "provider"):
            providers.add(model_info.provider.value)
    for provider in sorted(providers):
        print(f"  ‚Ä¢ {provider}")


def setup_logging(level=logging.INFO, format_string=None):
    """
    Configura logging para o m√≥dulo LLM.

    Args:
        level: N√≠vel de logging
        format_string: Formato customizado para logs
    """
    return SDKLogger.setup_logging(
        level=level,
        format_string=format_string,
        logger_name="appserver_sdk_python_ai.llm",
    )


__all__ = [
    # Enums
    "ModelType",
    "ModelProvider",
    "ModelCapability",
    "TokenizationMethod",
    "SupportedLanguage",
    # Clientes
    "LLMClient",
    "MockLLMClient",
    "AIService",
    "AsyncAIService",
    "AIConfig",
    "ChatResponse",
    "Message",
    "StreamChunk",
    "Usage",
    # Fun√ß√µes principais
    "get_token_count",
    "get_token_count_with_model",
    "list_available_models",
    "get_model_info",
    "get_portuguese_models",
    "is_model_registered",
    "register_custom_model",
    # Fun√ß√µes avan√ßadas
    "list_models_detailed",
    "get_model_details",
    "get_multilingual_models",
    "register_custom_model_advanced",
    "print_models_summary",
    # Documenta√ß√£o interativa
    "help_llm",
    "docs_llm",
    "search_llm_docs",
    # Cache
    "cache_result",
    "get_cache",
    "clear_cache",
    "get_cache_stats",
    "get_cache_memory_usage",
    # Validation
    "ValidationLevel",
    "validate_model_name",
    "validate_token_count",
    "validate_config",
    "validate_text",
    # Logging
    "LogLevel",
    "OperationType",
    "get_logger",
    "log_function_call",
    # Exce√ß√µes
    "LLMError",
    "LLMProviderError",
    "LLMAuthenticationError",
    "LLMRateLimitError",
    "LLMModelNotFoundError",
    "LLMNetworkError",
    "LLMTokenLimitError",
    "LLMTimeoutError",
    "LLMValidationError",
    "LLMConfigurationError",
    "LLMInvalidInputError",
    "LLMResponseError",
    "LLMStreamingError",
    "LLMContentFilterError",
    # Utilit√°rios
    "get_version_info",
    "check_dependencies",
    "health_check",
    "print_status",
    "setup_logging",
    # Vers√£o
    "__version__",
]
