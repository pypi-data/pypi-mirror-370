# appserver_sdk_python_ai/webscraping/__init__.py
"""
M√≥dulo de Web Scraping para appserver_sdk_python_ai
============================================

Este m√≥dulo fornece funcionalidades avan√ßadas de web scraping com convers√£o
para markdown usando Docling e outras ferramentas.

Componentes principais:
- DoclingWebScraper: Scraper principal com Docling
- Utilit√°rios de limpeza e valida√ß√£o
- Sistema de cache robusto
- Processamento em lote
- Tratamento avan√ßado de erros

Exemplo de uso b√°sico:
    from appserver_sdk_python_ai.webscraping import DoclingWebScraper, ScrapingConfig

    config = ScrapingConfig(clean_html=True, enable_cache=True)
    scraper = DoclingWebScraper(config)
    result = scraper.scrape_to_markdown("https://example.com")

Exemplo de uso simplificado:
    from appserver_sdk_python_ai.webscraping import quick_scrape
    markdown = quick_scrape("https://example.com")
"""

# Importa√ß√µes padr√£o
import logging
import warnings
from typing import Any, Optional, Union

# Importar funcionalidades comuns do m√≥dulo shared
from appserver_sdk_python_ai.shared import (
    DependencyChecker,
    HealthChecker,
    SDKLogger,
    VersionInfo,
)

__version__ = "1.0.0"
__author__ = "appserver_sdk_python_ai"


# Importar DEFAULT_USER_AGENT
try:
    from appserver_sdk_python_ai.webscraping.core.config import DEFAULT_USER_AGENT
except ImportError:
    DEFAULT_USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"

# Importa√ß√µes principais
from appserver_sdk_python_ai.webscraping.core.config import (
    DoclingConfig,
    GlobalWebScrapingConfig,
    ScrapingConfig,
    global_config,
)
from appserver_sdk_python_ai.webscraping.core.models import (
    BatchScrapingResult,
    CacheEntry,
    ScrapingResult,
    ScrapingStatus,
    WebPageMetadata,
)
from appserver_sdk_python_ai.webscraping.exceptions import (
    AuthenticationError,
    CacheError,
    ContentTooLargeError,
    ConversionError,
    JavaScriptError,
    NetworkError,
    ParsingError,
    ProxyError,
    RateLimitError,
    RobotsTxtError,
    ScrapingConfigError,
    SSLVerificationError,
    TimeoutError,
    UnsupportedFormatError,
    ValidationError,
    WebScrapingError,
)

# Importa√ß√µes condicionais para evitar erros quando depend√™ncias n√£o est√£o instaladas
try:
    from appserver_sdk_python_ai.webscraping.docling.scraper import DoclingWebScraper

    SCRAPER_AVAILABLE = True
except ImportError as e:
    DoclingWebScraper = None  # type: ignore
    SCRAPER_AVAILABLE = False
    import warnings

    warnings.warn(f"DoclingWebScraper n√£o p√¥de ser importado: {e}", stacklevel=2)

# OCR agora √© um m√≥dulo independente
# Para usar OCR, importe diretamente: from appserver_sdk_python_ai.ocr import ...
OCR_AVAILABLE = False  # Mantido para compatibilidade, mas OCR n√£o est√° mais aqui

try:
    from appserver_sdk_python_ai.webscraping.utils.cache import (
        CacheManager,
        MemoryCache,
    )
    from appserver_sdk_python_ai.webscraping.utils.cleaner import ContentCleaner
    from appserver_sdk_python_ai.webscraping.utils.validators import (
        ContentValidator,
        RobotsTxtChecker,
        URLValidator,
    )

    UTILS_AVAILABLE = True
except ImportError as e:
    ContentCleaner = None  # type: ignore
    CacheManager = None  # type: ignore
    MemoryCache = None  # type: ignore
    URLValidator = None  # type: ignore
    ContentValidator = None  # type: ignore
    RobotsTxtChecker = None  # type: ignore
    UTILS_AVAILABLE = False
    import warnings

    warnings.warn(f"Utilit√°rios n√£o puderam ser importados: {e}", stacklevel=2)

# Verificar disponibilidade do Docling
try:
    import docling

    DOCLING_AVAILABLE = True
    DOCLING_VERSION = getattr(docling, "__version__", "unknown")
except ImportError:
    DOCLING_AVAILABLE = False
    DOCLING_VERSION = None


# Fun√ß√µes de conveni√™ncia
def quick_scrape(
    url: str,
    clean_html: bool = True,
    include_images: bool = True,
    enable_cache: bool = False,
) -> str:
    """
    Fun√ß√£o de conveni√™ncia para scraping r√°pido.

    Args:
        url: URL para fazer scraping
        clean_html: Se deve limpar HTML
        include_images: Se deve incluir imagens
        enable_cache: Se deve habilitar cache

    Returns:
        str: Conte√∫do em markdown

    Raises:
        WebScrapingError: Em caso de erro no scraping
    """
    if not SCRAPER_AVAILABLE or DoclingWebScraper is None:
        raise WebScrapingError(
            "DoclingWebScraper n√£o est√° dispon√≠vel. Verifique as depend√™ncias."
        )

    config = ScrapingConfig(
        clean_html=clean_html, include_images=include_images, enable_cache=enable_cache
    )

    scraper = DoclingWebScraper(config)
    result = scraper.scrape_to_markdown(url)

    if not result.success:
        raise WebScrapingError(f"Falha no scraping: {result.error}", url)

    return result.content


def batch_scrape_simple(
    urls: list, output_dir: str = "scraped_content", max_workers: int = 5
) -> dict:
    """
    Fun√ß√£o de conveni√™ncia para scraping em lote.

    Args:
        urls: Lista de URLs
        output_dir: Diret√≥rio de sa√≠da
        max_workers: N√∫mero m√°ximo de workers

    Returns:
        dict: Dicion√°rio com status de sucesso para cada URL
    """
    if not SCRAPER_AVAILABLE or DoclingWebScraper is None:
        raise WebScrapingError(
            "DoclingWebScraper n√£o est√° dispon√≠vel. Verifique as depend√™ncias."
        )

    scraper = DoclingWebScraper()
    results = scraper.batch_scrape(urls, output_dir, max_workers)

    return {result.url: result.success for result in results}


def create_custom_scraper(
    timeout: int = 30,
    user_agent: str | None = None,
    clean_html: bool = True,
    include_images: bool = True,
    enable_cache: bool = False,
    **kwargs,
):
    """
    Cria um scraper customizado com configura√ß√µes espec√≠ficas.

    Args:
        timeout: Timeout para requisi√ß√µes
        user_agent: User agent customizado
        clean_html: Se deve limpar HTML
        include_images: Se deve incluir imagens
        enable_cache: Se deve habilitar cache
        **kwargs: Outros par√¢metros de configura√ß√£o

    Returns:
        DoclingWebScraper: Inst√¢ncia configurada
    """
    if not SCRAPER_AVAILABLE or DoclingWebScraper is None:
        raise WebScrapingError(
            "DoclingWebScraper n√£o est√° dispon√≠vel. Verifique as depend√™ncias."
        )

    config = ScrapingConfig(
        timeout=timeout,
        user_agent=user_agent or global_config.default_user_agent,
        clean_html=clean_html,
        include_images=include_images,
        enable_cache=enable_cache,
        **kwargs,
    )

    return DoclingWebScraper(config)


# Fun√ß√µes de OCR foram movidas para o m√≥dulo independente appserver_sdk_python_ai.ocr
# Para usar OCR, importe diretamente:
# from appserver_sdk_python_ai.ocr import quick_ocr, batch_ocr, create_custom_ocr_processor


def _ocr_deprecated_warning():
    """Aviso sobre a mudan√ßa do OCR para m√≥dulo independente."""
    import warnings

    warnings.warn(
        "As fun√ß√µes de OCR foram movidas para o m√≥dulo independente 'appserver_sdk_python_ai.ocr'. "
        "Use: from appserver_sdk_python_ai.ocr import quick_ocr, batch_ocr, create_custom_ocr_processor",
        DeprecationWarning,
        stacklevel=3,
    )


def process_pdf_with_ocr(
    pdf_path: str,
    output_file: str | None = None,
    extract_images: bool = True,
    extract_tables: bool = True,
    **kwargs,
) -> "ScrapingResult":
    """
    Processa um PDF com OCR e extra√ß√£o de imagens/tabelas usando Docling.

    Args:
        pdf_path: Caminho para o arquivo PDF
        output_file: Caminho opcional para salvar o markdown
        extract_images: Se deve extrair imagens
        extract_tables: Se deve extrair tabelas
        **kwargs: Argumentos adicionais para o scraper

    Returns:
        ScrapingResult: Resultado do processamento

    Raises:
        ConversionError: Se o Docling n√£o estiver dispon√≠vel
        ValidationError: Se o arquivo n√£o for encontrado
    """
    if not SCRAPER_AVAILABLE or DoclingWebScraper is None:
        raise WebScrapingError(
            "DoclingWebScraper n√£o est√° dispon√≠vel. Verifique as depend√™ncias."
        )

    scraper = DoclingWebScraper(**kwargs)
    return scraper.process_pdf_with_ocr(
        pdf_path=pdf_path,
        output_file=output_file,
        extract_images=extract_images,
        extract_tables=extract_tables,
    )


def batch_process_pdfs(
    pdf_paths: list[str],
    output_dir: str | None = None,
    max_workers: int = 3,
    extract_images: bool = True,
    extract_tables: bool = True,
    progress_callback=None,
    **kwargs,
) -> list["ScrapingResult"]:
    """
    Processa m√∫ltiplos PDFs em paralelo com OCR.

    Args:
        pdf_paths: Lista de caminhos para PDFs
        output_dir: Diret√≥rio de sa√≠da (opcional)
        max_workers: N√∫mero m√°ximo de threads
        extract_images: Se deve extrair imagens
        extract_tables: Se deve extrair tabelas
        progress_callback: Callback para acompanhar progresso
        **kwargs: Argumentos adicionais para o scraper

    Returns:
        List[ScrapingResult]: Lista de resultados
    """
    if not SCRAPER_AVAILABLE or DoclingWebScraper is None:
        raise WebScrapingError(
            "DoclingWebScraper n√£o est√° dispon√≠vel. Verifique as depend√™ncias."
        )

    scraper = DoclingWebScraper(**kwargs)
    return scraper.batch_process_pdfs(
        pdf_paths=pdf_paths,
        output_dir=output_dir,
        max_workers=max_workers,
        extract_images=extract_images,
        extract_tables=extract_tables,
        progress_callback=progress_callback,
    )


# Fun√ß√µes de informa√ß√£o
def get_version_info():
    """Retorna informa√ß√µes sobre a vers√£o e depend√™ncias."""
    return VersionInfo.create_version_info(
        module_name="webscraping",
        module_version=__version__,
        dependencies=check_dependencies(),
        additional_info={
            "docling_available": DOCLING_AVAILABLE,
            "docling_version": DOCLING_VERSION,
        },
    )


def check_dependencies():
    """Verifica se todas as depend√™ncias est√£o instaladas."""
    return DependencyChecker.check_dependencies(
        ["requests", "beautifulsoup4", "lxml", "docling"]
    )


def health_check():
    """Verifica a sa√∫de do m√≥dulo e suas depend√™ncias."""
    dependencies = check_dependencies()
    features = {
        "docling_conversion": DOCLING_AVAILABLE,
        "cache_system": True,
        "batch_processing": True,
        "ocr_processing": OCR_AVAILABLE,
    }

    return HealthChecker.create_health_report(
        module_name="webscraping",
        version=__version__,
        dependencies=dependencies,
        features=features,
        critical_deps=["requests", "beautifulsoup4"],
        optional_deps=["lxml", "docling"],
    )


def print_status():
    """Imprime status do m√≥dulo."""
    health = health_check()

    # Adicionar informa√ß√µes espec√≠ficas do webscraping
    print("=" * 60)
    print("M√ìDULO WEB SCRAPING - appserver_sdk_python_ai")
    print("=" * 60)
    print(f"Vers√£o: {__version__}")
    print(f"Status: {health['status']}")
    print(f"Docling: {'‚úÖ Dispon√≠vel' if DOCLING_AVAILABLE else '‚ùå N√£o dispon√≠vel'}")
    print(f"OCR: {'‚úÖ Dispon√≠vel' if OCR_AVAILABLE else '‚ùå N√£o dispon√≠vel'}")

    # Usar o m√©todo padr√£o para o resto
    HealthChecker.print_health_status(
        health, show_dependencies=True, show_features=True
    )

    # Informa√ß√£o adicional espec√≠fica do webscraping
    print("\nüîç Informa√ß√µes adicionais:")
    print("  ‚Ä¢ OCR foi movido para m√≥dulo independente 'appserver_sdk_python_ai.ocr'")


# Configura√ß√£o de logging


def setup_logging(level=logging.INFO, format_string=None):
    """
    Configura logging para o m√≥dulo.

    Args:
        level: N√≠vel de logging
        format_string: Formato customizado para logs
    """
    if format_string is None:
        format_string = global_config.log_format

    return SDKLogger.setup_logging(
        level=level,
        format_string=format_string,
        logger_name="appserver_sdk_python_ai.webscraping",
    )


# Exportar tudo necess√°rio
__all__ = [
    # Classes principais
    "DoclingWebScraper",
    "ScrapingConfig",
    "DoclingConfig",
    "GlobalWebScrapingConfig",
    "global_config",
    # Modelos
    "ScrapingResult",
    "BatchScrapingResult",
    "WebPageMetadata",
    "CacheEntry",
    "ScrapingStatus",
    # Exce√ß√µes
    "WebScrapingError",
    "NetworkError",
    "TimeoutError",
    "AuthenticationError",
    "RateLimitError",
    "ProxyError",
    "SSLVerificationError",
    "ContentTooLargeError",
    "UnsupportedFormatError",
    "ValidationError",
    "ConversionError",
    "CacheError",
    "ScrapingConfigError",
    "JavaScriptError",
    "ParsingError",
    "RobotsTxtError",
    # Utilit√°rios
    "ContentCleaner",
    "CacheManager",
    "MemoryCache",
    "URLValidator",
    "ContentValidator",
    "RobotsTxtChecker",
    # Fun√ß√µes de conveni√™ncia
    "quick_scrape",
    "batch_scrape_simple",
    "create_custom_scraper",
    "process_pdf_with_ocr",
    "batch_process_pdfs",
    # Informa√ß√µes e configura√ß√£o
    "get_version_info",
    "check_dependencies",
    "health_check",
    "print_status",
    "setup_logging",
    # Constantes
    "__version__",
    "DOCLING_AVAILABLE",
    "DOCLING_VERSION",
    "OCR_AVAILABLE",  # Mantido para compatibilidade
    "DEFAULT_USER_AGENT",
]

# Inicializa√ß√£o autom√°tica
logger = logging.getLogger(__name__)
logger.info(f"M√≥dulo webscraping v{__version__} carregado")

if not DOCLING_AVAILABLE:
    logger.warning("Docling n√£o dispon√≠vel - usando convers√£o b√°sica")
