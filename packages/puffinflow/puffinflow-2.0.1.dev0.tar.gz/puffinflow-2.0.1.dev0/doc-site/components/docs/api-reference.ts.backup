export const apiReferenceMarkdown = `# API Reference

Comprehensive reference for all PuffinFlow classes, methods, and functions. This reference covers the complete API surface area including core agent functionality, coordination primitives, observability features, resource management, and reliability patterns.

## Core Classes

### Agent

The main orchestration class for creating and managing workflow agents. Agents are the primary building blocks for workflow orchestration in PuffinFlow.

\`\`\`python
from puffinflow import Agent

class Agent:
    def __init__(self, name: str, config: Optional[AgentConfig] = None)
\`\`\`

**Parameters:**
- \`name\` (str): Unique identifier for the agent. Used for logging, metrics, and coordination.
- \`config\` (AgentConfig, optional): Configuration settings for resource limits, execution mode, and observability.

**Properties:**
- \`name\` (str): Agent's unique identifier
- \`status\` (AgentStatus): Current execution status (PENDING, RUNNING, COMPLETED, FAILED)
- \`states\` (Dict[str, StateMetadata]): Registered state functions and their metadata
- \`context\` (Context): Shared context for data passing between states
- \`execution_mode\` (ExecutionMode): Execution strategy (PARALLEL or SEQUENTIAL)

**Methods:**

#### \`add_state(name: str, func: Callable, dependencies: Optional[List[str]] = None, **kwargs) -> None\`
Registers a state function with the agent, including optional dependencies and resource requirements.

**Parameters:**
- \`name\` (str): Unique state identifier within the agent
- \`func\` (Callable): Async function to execute for this state
- \`dependencies\` (List[str], optional): List of state names that must complete before this state can execute
- \`**kwargs\`: Additional state configuration (cpu, memory, timeout, max_retries, priority)

**State Function Signature:**
\`\`\`python
async def state_function(context: Context) -> Optional[Union[str, List[str]]]:
    # State logic here
    return "next_state"  # or ["state1", "state2"] or None
\`\`\`

**Example:**
\`\`\`python
# Basic state registration
async def data_processing(context):
    data = context.get_variable("input_data")
    processed = await process_data(data)
    context.set_variable("processed_data", processed)
    return "analysis_state"

agent.add_state("data_processing", data_processing)

# State with dependencies and resources
agent.add_state(
    "analysis_state",
    analysis_function,
    dependencies=["data_processing"],
    cpu=2.0,
    memory=1024,
    timeout=60.0,
    max_retries=3
)
\`\`\`

#### \`run(initial_context: Optional[Dict] = None, execution_mode: Optional[ExecutionMode] = None) -> Context\`
Executes the agent workflow with optional initial context and execution mode override.

**Parameters:**
- \`initial_context\` (Dict, optional): Initial context variables to set before execution
- \`execution_mode\` (ExecutionMode, optional): Override default execution mode for this run

**Returns:**
- \`Context\`: Final context containing all workflow results and state data

**Execution Modes:**
- \`ExecutionMode.PARALLEL\` (default): States without dependencies run concurrently
- \`ExecutionMode.SEQUENTIAL\`: Only the first state runs initially, flow controlled by return values

**Example:**
\`\`\`python
# Run with initial data
result = await agent.run(initial_context={
    "input_data": [1, 2, 3, 4, 5],
    "config": {"batch_size": 100}
})
output = result.get_variable("final_result")

# Run in sequential mode
result = await agent.run(execution_mode=ExecutionMode.SEQUENTIAL)

# Run without initial data
result = await agent.run()
\`\`\`

#### \`state(func: Optional[Callable] = None, **kwargs) -> Callable\`
Decorator to register state functions directly with the agent, including resource and retry configuration.

**Parameters:**
- \`func\` (Callable, optional): Function to decorate
- \`**kwargs\`: State configuration options

**Resource Configuration:**
- \`cpu\` (float): CPU units required (default: 1.0)
- \`memory\` (int): Memory in MB required (default: 512)
- \`gpu\` (float): GPU units required (default: 0.0)
- \`disk\` (int): Disk space in MB required (default: 0)

**Execution Configuration:**
- \`timeout\` (float): Maximum execution time in seconds
- \`max_retries\` (int): Number of retry attempts on failure
- \`priority\` (Priority): Execution priority (LOW, NORMAL, HIGH, CRITICAL)
- \`rate_limit\` (float): Maximum calls per second

**Example:**
\`\`\`python
from puffinflow import Priority

@agent.state(
    cpu=2.0,
    memory=1024,
    timeout=30.0,
    max_retries=3,
    priority=Priority.HIGH,
    rate_limit=10.0
)
async def cpu_intensive_task(context):
    # Resource-controlled state execution
    result = await heavy_computation()
    context.set_variable("computation_result", result)
    return "next_state"
\`\`\`

#### \`get_state_result(state_name: str) -> Optional[StateResult]\`
Retrieves the execution result for a specific state.

**Parameters:**
- \`state_name\` (str): Name of the state to get results for

**Returns:**
- \`StateResult\`: State execution metadata including status, duration, and errors

#### \`get_execution_summary() -> Dict[str, Any]\`
Returns a comprehensive summary of the workflow execution.

**Returns:**
- \`Dict\`: Execution summary with timing, resource usage, and state results

---

### Context

Provides type-safe data sharing and state management across workflow states. Context is the primary mechanism for data flow between states.

\`\`\`python
class Context:
    def __init__(self, workflow_id: str, initial_data: Optional[Dict] = None)
\`\`\`

**Properties:**
- \`workflow_id\` (str): Unique workflow identifier
- \`execution_id\` (str): Unique execution identifier for this run
- \`agent_name\` (str): Name of the agent executing this context
- \`start_time\` (float): Workflow start timestamp
- \`variables\` (Dict[str, Any]): Shared variables between states

#### Variable Management

#### \`set_variable(key: str, value: Any) -> None\`
Stores a variable in the context with type validation and serialization support.

**Parameters:**
- \`key\` (str): Variable name (must be a valid Python identifier)
- \`value\` (Any): Variable value (must be JSON-serializable for checkpointing)

**Example:**
\`\`\`python
# Basic variable storage
context.set_variable("user_id", 12345)
context.set_variable("user_data", {"name": "Alice", "role": "admin"})

# Complex data structures
context.set_variable("processing_results", {
    "metrics": {"accuracy": 0.95, "recall": 0.88},
    "model_info": {"version": "v2.1", "timestamp": time.time()}
})
\`\`\`

#### \`get_variable(key: str, default: Any = None) -> Any\`
Retrieves a variable from the context with optional default value.

**Parameters:**
- \`key\` (str): Variable name to retrieve
- \`default\` (Any): Default value if variable doesn't exist

**Returns:**
- \`Any\`: Variable value or default

**Example:**
\`\`\`python
# Get required variable
user_id = context.get_variable("user_id")

# Get with default value
batch_size = context.get_variable("batch_size", 100)

# Check if variable exists
if context.has_variable("optional_config"):
    config = context.get_variable("optional_config")
\`\`\`

#### \`has_variable(key: str) -> bool\`
Checks if a variable exists in the context.

**Parameters:**
- \`key\` (str): Variable name to check

**Returns:**
- \`bool\`: True if variable exists, False otherwise

#### \`delete_variable(key: str) -> bool\`
Removes a variable from the context.

**Parameters:**
- \`key\` (str): Variable name to remove

**Returns:**
- \`bool\`: True if variable was removed, False if it didn't exist

#### \`list_variables() -> List[str]\`
Returns a list of all variable names in the context.

**Returns:**
- \`List[str]\`: List of variable names

#### Output Management

#### \`set_output(key: str, value: Any) -> None\`
Marks a variable as a workflow output, making it easily accessible after execution.

**Parameters:**
- \`key\` (str): Output name
- \`value\` (Any): Output value

#### \`get_output(key: str) -> Any\`
Retrieves a workflow output value.

**Parameters:**
- \`key\` (str): Output name

**Returns:**
- \`Any\`: Output value

#### State Management

#### \`set_state(key: str, value: Any) -> None\`
Stores internal state data (separate from shared variables).

**Parameters:**
- \`key\` (str): State key
- \`value\` (Any): State value

#### \`get_state(key: str, default: Any = None) -> Any\`
Retrieves internal state data.

**Parameters:**
- \`key\` (str): State key
- \`default\` (Any): Default value if state doesn't exist

**Returns:**
- \`Any\`: State value or default

---

## Decorators and State Builders

### @state Decorator

Primary decorator for defining workflow states with resource requirements and execution parameters.

\`\`\`python
from puffinflow import state, Priority

@state(
    cpu: float = 1.0,
    memory: int = 512,
    gpu: float = 0.0,
    disk: int = 0,
    timeout: Optional[float] = None,
    max_retries: int = 0,
    priority: Priority = Priority.NORMAL,
    rate_limit: Optional[float] = None
)
async def state_function(context: Context) -> Optional[Union[str, List[str]]]:
    pass
\`\`\`

**Parameters:**
- \`cpu\` (float): CPU units required (1.0 = 1 CPU core)
- \`memory\` (int): Memory in MB required
- \`gpu\` (float): GPU units required
- \`disk\` (int): Disk space in MB required
- \`timeout\` (float): Maximum execution time in seconds
- \`max_retries\` (int): Number of retry attempts on failure
- \`priority\` (Priority): Execution priority level
- \`rate_limit\` (float): Maximum invocations per second

### Specialized State Decorators

#### \`@cpu_intensive\`
Pre-configured decorator for CPU-heavy tasks.

\`\`\`python
from puffinflow import cpu_intensive

@cpu_intensive(cores=4, memory=2048)
async def heavy_computation(context):
    # CPU-intensive work
    pass
\`\`\`

#### \`@memory_intensive\`
Pre-configured decorator for memory-heavy tasks.

\`\`\`python
from puffinflow import memory_intensive

@memory_intensive(memory=4096, timeout=120.0)
async def large_data_processing(context):
    # Memory-intensive work
    pass
\`\`\`

#### \`@gpu_accelerated\`
Pre-configured decorator for GPU-accelerated tasks.

\`\`\`python
from puffinflow import gpu_accelerated

@gpu_accelerated(gpu_units=1.0, memory=2048)
async def ml_inference(context):
    # GPU computation
    pass
\`\`\`

#### \`@io_intensive\`
Pre-configured decorator for I/O-heavy tasks.

\`\`\`python
from puffinflow import io_intensive

@io_intensive(timeout=60.0, max_retries=3)
async def file_processing(context):
    # I/O operations
    pass
\`\`\`

#### \`@network_intensive\`
Pre-configured decorator for network-heavy tasks.

\`\`\`python
from puffinflow import network_intensive

@network_intensive(timeout=30.0, rate_limit=10.0)
async def api_calls(context):
    # Network operations
    pass
\`\`\`

#### \`@critical_state\`
Pre-configured decorator for critical workflow states.

\`\`\`python
from puffinflow import critical_state

@critical_state(max_retries=5, timeout=180.0)
async def critical_operation(context):
    # Critical workflow step
    pass
\`\`\`

---

## Coordination and Multi-Agent Systems

### AgentTeam

Manages multiple agents working together on related tasks.

\`\`\`python
from puffinflow import AgentTeam

class AgentTeam:
    def __init__(self, name: str, agents: List[Agent])
\`\`\`

**Methods:**

#### \`add_agent(agent: Agent) -> None\`
Adds an agent to the team.

#### \`run_parallel() -> TeamResult\`
Executes all agents in parallel.

#### \`run_sequential() -> TeamResult\`
Executes agents one after another.

#### \`coordinate(strategy: str) -> TeamResult\`
Executes agents with coordination strategy.

### AgentPool

Manages a pool of identical agents for load distribution.

\`\`\`python
from puffinflow import AgentPool

class AgentPool:
    def __init__(self, name: str, agent_template: Agent, pool_size: int)
\`\`\`

**Methods:**

#### \`submit_work(work_item: WorkItem) -> Future\`
Submits work to the next available agent.

#### \`get_pool_status() -> PoolStatus\`
Returns current pool utilization status.

### Coordination Primitives

#### Semaphore
Controls concurrent access to limited resources.

\`\`\`python
from puffinflow.coordination import Semaphore

semaphore = Semaphore("resource_pool", max_count=5)

async with semaphore.acquire_context("requester_id"):
    # Protected resource access
    pass
\`\`\`

#### Mutex
Provides exclusive access to shared resources.

\`\`\`python
from puffinflow.coordination import Mutex

mutex = Mutex("exclusive_resource")

async with mutex.acquire_context("requester_id"):
    # Exclusive access
    pass
\`\`\`

#### Barrier
Synchronizes multiple agents at specific points.

\`\`\`python
from puffinflow.coordination import Barrier

barrier = Barrier("sync_point", parties=4)
await barrier.wait("participant_id", timeout=30.0)
\`\`\`

#### Event
Provides asynchronous signaling between agents.

\`\`\`python
from puffinflow.coordination import Event

event = Event("data_ready")
await event.wait(timeout=60.0)
await event.set({"data": "payload"})
\`\`\`

---

## Observability and Monitoring

### MetricsCollector

Collects and manages workflow metrics.

\`\`\`python
from puffinflow.observability import MetricsCollector

metrics = MetricsCollector(namespace="my_workflow")

# Counter metric
request_counter = metrics.counter("requests_total", "Total requests", ["method", "status"])
request_counter.increment(1.0, method="POST", status="success")

# Histogram metric
duration_histogram = metrics.histogram("request_duration_seconds", "Request duration")
with duration_histogram.timer():
    # Timed operation
    pass

# Gauge metric
active_connections = metrics.gauge("active_connections", "Active connections")
active_connections.set(42.0)
\`\`\`

### Tracing

Distributed tracing for workflow execution.

\`\`\`python
from puffinflow.observability import trace_state

@trace_state(operation_name="data_processing")
@state
async def traced_state(context):
    # Automatically traced state execution
    pass
\`\`\`

### Observability Configuration

\`\`\`python
from puffinflow.observability import ObservabilityConfig, setup_observability

config = ObservabilityConfig(
    metrics_enabled=True,
    tracing_enabled=True,
    logging_enabled=True,
    namespace="production_workflow"
)

setup_observability(config)
\`\`\`

---

## Resource Management

### ResourceRequirements

Defines resource requirements for states and agents.

\`\`\`python
from puffinflow import ResourceRequirements, ResourceType

requirements = ResourceRequirements(
    cpu=2.0,
    memory=1024,
    gpu=1.0,
    disk=500,
    custom_resources={
        ResourceType.NETWORK_BANDWIDTH: 100.0,
        ResourceType.API_QUOTA: 10.0
    }
)
\`\`\`

### ResourcePool

Manages shared resource pools across workflows.

\`\`\`python
from puffinflow import ResourcePool, AllocationStrategy

pool = ResourcePool(
    name="compute_pool",
    total_cpu=16.0,
    total_memory=32768,
    allocation_strategy=AllocationStrategy.FAIR_SHARE
)

# Request resource allocation
allocation = await pool.request_allocation(requirements)
try:
    # Use allocated resources
    pass
finally:
    await pool.release_allocation(allocation)
\`\`\`

### QuotaManager

Manages API quotas and rate limits.

\`\`\`python
from puffinflow import QuotaManager

quota_manager = QuotaManager()
quota_manager.set_quota("openai_api", limit=100, window_seconds=60)

# Check and consume quota
if await quota_manager.check_quota("openai_api", cost=1):
    # Make API call
    pass
\`\`\`

---

## Reliability Patterns

### CircuitBreaker

Prevents cascade failures by temporarily blocking failing operations.

\`\`\`python
from puffinflow import CircuitBreaker, CircuitBreakerConfig

config = CircuitBreakerConfig(
    failure_threshold=5,
    recovery_timeout=30.0,
    expected_exception=Exception
)

circuit_breaker = CircuitBreaker("external_service", config)

@circuit_breaker.protected
async def external_api_call():
    # Protected external call
    pass
\`\`\`

### Bulkhead

Isolates resources to prevent one failing component from affecting others.

\`\`\`python
from puffinflow import Bulkhead, BulkheadConfig

config = BulkheadConfig(
    max_concurrent_calls=10,
    max_wait_duration=5.0
)

bulkhead = Bulkhead("critical_service", config)

@bulkhead.isolate
async def critical_operation():
    # Isolated operation
    pass
\`\`\`

### ResourceLeakDetector

Monitors and detects resource leaks in workflows.

\`\`\`python
from puffinflow import ResourceLeakDetector

detector = ResourceLeakDetector(
    check_interval=60.0,
    memory_threshold=0.8,
    cpu_threshold=0.9
)

detector.start_monitoring()
\`\`\`

---

## Execution Modes and Strategies

### ExecutionMode

Controls how agents execute their states.

\`\`\`python
from puffinflow import ExecutionMode

# Parallel execution (default)
result = await agent.run(execution_mode=ExecutionMode.PARALLEL)

# Sequential execution
result = await agent.run(execution_mode=ExecutionMode.SEQUENTIAL)
\`\`\`

### Priority Levels

Controls state execution priority.

\`\`\`python
from puffinflow import Priority

@state(priority=Priority.CRITICAL)
async def critical_state(context):
    pass

@state(priority=Priority.HIGH)
async def important_state(context):
    pass

@state(priority=Priority.NORMAL)
async def normal_state(context):
    pass

@state(priority=Priority.LOW)
async def background_state(context):
    pass
\`\`\`

---

## Status and Result Types

### AgentStatus

Enumeration of agent execution states.

- \`PENDING\`: Agent created but not started
- \`RUNNING\`: Agent currently executing
- \`COMPLETED\`: Agent finished successfully
- \`FAILED\`: Agent execution failed
- \`CANCELLED\`: Agent execution was cancelled

### StateStatus

Enumeration of individual state execution states.

- \`PENDING\`: State waiting to execute
- \`RUNNING\`: State currently executing
- \`COMPLETED\`: State finished successfully
- \`FAILED\`: State execution failed
- \`SKIPPED\`: State was skipped due to conditions
- \`RETRYING\`: State is being retried after failure

### StateResult

Contains execution results and metadata for a state.

\`\`\`python
class StateResult:
    state_name: str
    status: StateStatus
    start_time: float
    end_time: Optional[float]
    duration: Optional[float]
    return_value: Any
    exception: Optional[Exception]
    retry_count: int
    resource_usage: ResourceUsage
\`\`\`

### AgentResult

Contains execution results and metadata for an entire agent.

\`\`\`python
class AgentResult:
    agent_name: str
    status: AgentStatus
    start_time: float
    end_time: Optional[float]
    duration: Optional[float]
    state_results: Dict[str, StateResult]
    context: Context
    exception: Optional[Exception]
\`\`\`

---

## Utility Functions

### Workflow Creation Helpers

#### \`create_pipeline(*agents: Agent) -> AgentTeam\`
Creates a sequential pipeline of agents.

#### \`create_team(*agents: Agent) -> AgentTeam\`
Creates a parallel team of agents.

#### \`run_agents_parallel(*agents: Agent) -> List[AgentResult]\`
Runs multiple agents in parallel.

#### \`run_agents_sequential(*agents: Agent) -> List[AgentResult]\`
Runs multiple agents sequentially.

### Configuration Helpers

#### \`get_settings() -> Settings\`
Returns current PuffinFlow settings.

#### \`get_features() -> Features\`
Returns enabled PuffinFlow features.

### Version Information

#### \`get_version() -> str\`
Returns the PuffinFlow version string.

#### \`get_info() -> Dict[str, str]\`
Returns comprehensive package information.

---

This API reference provides comprehensive coverage of PuffinFlow's functionality. For practical examples and patterns, see the [Getting Started Guide](#docs/getting-started) and [Best Practices](#docs/best-practices).`;

#### \`has_variable(key: str) -> bool\`
Checks if a variable exists in the context.

**Parameters:**
- \`key\` (str): Variable name

**Returns:**
- \`bool\`: True if variable exists

#### \`clear_variable(key: str) -> None\`
Removes a variable from the context.

**Parameters:**
- \`key\` (str): Variable name to remove

#### Type-Safe Variables

#### \`set_typed_variable(key: str, value: T) -> None\`
Stores a type-locked variable.

**Parameters:**
- \`key\` (str): Variable name
- \`value\` (T): Variable value (type is locked)

#### \`get_typed_variable(key: str, type_hint: Optional[Type[T]] = None) -> T\`
Retrieves a type-locked variable.

**Parameters:**
- \`key\` (str): Variable name
- \`type_hint\` (Type[T], optional): Type hint for IDE support

**Returns:**
- \`T\`: Variable value with type guarantee

#### Validated Data

#### \`set_validated_data(key: str, value: BaseModel) -> None\`
Stores Pydantic model data with validation.

**Parameters:**
- \`key\` (str): Variable name
- \`value\` (BaseModel): Pydantic model instance

#### \`get_validated_data(key: str, model_class: Type[BaseModel]) -> BaseModel\`
Retrieves and validates Pydantic model data.

**Parameters:**
- \`key\` (str): Variable name
- \`model_class\` (Type[BaseModel]): Pydantic model class

**Returns:**
- \`BaseModel\`: Validated model instance

#### Constants

#### \`set_constant(key: str, value: Any) -> None\`
Stores an immutable constant.

**Parameters:**
- \`key\` (str): Constant name
- \`value\` (Any): Constant value

#### \`get_constant(key: str) -> Any\`
Retrieves a constant value.

**Parameters:**
- \`key\` (str): Constant name

**Returns:**
- \`Any\`: Constant value

#### Secrets Management

#### \`set_secret(key: str, value: str) -> None\`
Stores sensitive data securely.

**Parameters:**
- \`key\` (str): Secret name
- \`value\` (str): Secret value

#### \`get_secret(key: str) -> str\`
Retrieves a secret value.

**Parameters:**
- \`key\` (str): Secret name

**Returns:**
- \`str\`: Secret value

#### Cached Data

#### \`set_cached(key: str, value: Any, ttl: float) -> None\`
Stores data with time-to-live expiration.

**Parameters:**
- \`key\` (str): Cache key
- \`value\` (Any): Cached value
- \`ttl\` (float): Time-to-live in seconds

#### \`get_cached(key: str, default: Any = None) -> Any\`
Retrieves cached data if not expired.

**Parameters:**
- \`key\` (str): Cache key
- \`default\` (Any): Default value if expired/missing

**Returns:**
- \`Any\`: Cached value or default

#### State-Local Data

#### \`set_state(key: str, value: Any) -> None\`
Stores data local to the current state.

**Parameters:**
- \`key\` (str): State variable name
- \`value\` (Any): State variable value

#### \`get_state(key: str, default: Any = None) -> Any\`
Retrieves state-local data.

**Parameters:**
- \`key\` (str): State variable name
- \`default\` (Any): Default value if not found

**Returns:**
- \`Any\`: State variable value or default

---

## Decorators

### @state

Decorator for configuring state functions with resource management and behavior options.

\`\`\`python
from puffinflow import state

@state(
    cpu: float = 1.0,
    memory: int = 512,
    gpu: float = 0.0,
    io: float = 1.0,
    priority: Priority = Priority.NORMAL,
    timeout: float = 300.0,
    max_retries: int = 0,
    retry_delay: float = 1.0,
    rate_limit: float = 0.0,
    burst_limit: int = 0,
    preemptible: bool = False
)
async def my_state(context: Context) -> Optional[Union[str, List[str]]]
\`\`\`

**Parameters:**

#### Resource Allocation
- \`cpu\` (float): CPU units to allocate (default: 1.0)
- \`memory\` (int): Memory in MB to allocate (default: 512)
- \`gpu\` (float): GPU units to allocate (default: 0.0)
- \`io\` (float): I/O bandwidth units (default: 1.0)

#### Execution Control
- \`priority\` (Priority): Execution priority (default: Priority.NORMAL)
- \`timeout\` (float): Maximum execution time in seconds (default: 300.0)
- \`preemptible\` (bool): Allow preemption for higher priority tasks (default: False)

#### Retry Configuration
- \`max_retries\` (int): Maximum retry attempts (default: 0)
- \`retry_delay\` (float): Delay between retries in seconds (default: 1.0)

#### Rate Limiting
- \`rate_limit\` (float): Operations per second limit (default: 0.0 = no limit)
- \`burst_limit\` (int): Burst capacity above rate limit (default: 0)

**Example:**
\`\`\`python
@state(
    cpu=2.0,
    memory=1024,
    priority=Priority.HIGH,
    max_retries=3,
    timeout=60.0
)
async def important_task(context):
    # High-priority task with retries
    result = await critical_operation()
    context.set_variable("result", result)
    return "next_state"
\`\`\`

---

## Enums and Constants

### Priority

Defines execution priority levels for states.

\`\`\`python
from puffinflow import Priority

class Priority(Enum):
    CRITICAL = 5
    HIGH = 4
    NORMAL = 3
    LOW = 2
    BACKGROUND = 1
\`\`\`

**Usage:**
\`\`\`python
@state(priority=Priority.HIGH)
async def high_priority_state(context):
    pass
\`\`\`

---

## Coordination

### AgentTeam

Manages coordinated execution of multiple agents.

\`\`\`python
from puffinflow import AgentTeam

class AgentTeam:
    def __init__(self, agents: List[Agent], name: str = "team")
\`\`\`

**Parameters:**
- \`agents\` (List[Agent]): List of agents to coordinate
- \`name\` (str): Team identifier

**Methods:**

#### \`execute_parallel() -> Dict[str, Context]\`
Executes all agents in parallel.

**Returns:**
- \`Dict[str, Context]\`: Results from each agent

#### \`execute_sequential() -> List[Context]\`
Executes agents one after another.

**Returns:**
- \`List[Context]\`: Ordered results from each agent

**Example:**
\`\`\`python
from puffinflow import Agent, AgentTeam

agent1 = Agent("worker1")
agent2 = Agent("worker2")

team = AgentTeam([agent1, agent2], name="processing_team")
results = await team.execute_parallel()
\`\`\`

### AgentPool

Manages a pool of identical agents for load balancing.

\`\`\`python
from puffinflow import AgentPool

class AgentPool:
    def __init__(self, agent_factory: Callable[[], Agent], size: int = 5)
\`\`\`

**Parameters:**
- \`agent_factory\` (Callable): Function that creates agent instances
- \`size\` (int): Number of agents in the pool

**Methods:**

#### \`submit_task(initial_context: Dict) -> Awaitable[Context]\`
Submits a task to the next available agent.

**Parameters:**
- \`initial_context\` (Dict): Initial context for the task

**Returns:**
- \`Awaitable[Context]\`: Task result

**Example:**
\`\`\`python
def create_worker():
    agent = Agent("worker")

    @agent.state
    async def process_task(context):
        data = context.get_variable("task_data")
        result = await process_data(data)
        context.set_variable("result", result)
        return None

    return agent

pool = AgentPool(create_worker, size=10)
result = await pool.submit_task({"task_data": "work_item"})
\`\`\`

---

## Observability

### MetricsCollector

Collects and tracks performance metrics.

\`\`\`python
from puffinflow.observability import MetricsCollector

class MetricsCollector:
    def __init__(self, namespace: str = "puffinflow")
\`\`\`

**Methods:**

#### \`increment(metric_name: str, value: float = 1.0, tags: Optional[Dict] = None) -> None\`
Increments a counter metric.

#### \`gauge(metric_name: str, value: float, tags: Optional[Dict] = None) -> None\`
Sets a gauge metric value.

#### \`timer(metric_name: str, tags: Optional[Dict] = None) -> ContextManager\`
Context manager for timing operations.

**Example:**
\`\`\`python
metrics = MetricsCollector()

@state
async def monitored_state(context):
    metrics.increment("state_executions")

    with metrics.timer("processing_time"):
        result = await process_data()

    metrics.gauge("result_size", len(result))
    return "next_state"
\`\`\`

---

## Configuration

### AgentConfig

Configuration settings for agent behavior.

\`\`\`python
from puffinflow import AgentConfig

class AgentConfig:
    def __init__(
        self,
        max_concurrent_states: int = 10,
        default_timeout: float = 300.0,
        enable_checkpointing: bool = True,
        checkpoint_interval: float = 30.0,
        enable_metrics: bool = True,
        enable_tracing: bool = False,
        log_level: str = "INFO"
    )
\`\`\`

**Parameters:**
- \`max_concurrent_states\` (int): Maximum states running concurrently
- \`default_timeout\` (float): Default timeout for states
- \`enable_checkpointing\` (bool): Enable automatic checkpointing
- \`checkpoint_interval\` (float): Checkpoint frequency in seconds
- \`enable_metrics\` (bool): Enable metrics collection
- \`enable_tracing\` (bool): Enable distributed tracing
- \`log_level\` (str): Logging level

**Example:**
\`\`\`python
config = AgentConfig(
    max_concurrent_states=20,
    default_timeout=600.0,
    enable_checkpointing=True,
    enable_metrics=True
)

agent = Agent("configured_agent", config=config)
\`\`\`

---

## Error Handling

### Common Exceptions

#### \`StateExecutionError\`
Raised when state execution fails.

\`\`\`python
from puffinflow.exceptions import StateExecutionError

try:
    await agent.run()
except StateExecutionError as e:
    print(f"State '{e.state_name}' failed: {e.message}")
\`\`\`

#### \`ResourceAllocationError\`
Raised when resource allocation fails.

\`\`\`python
from puffinflow.exceptions import ResourceAllocationError

try:
    await agent.run()
except ResourceAllocationError as e:
    print(f"Resource allocation failed: {e.message}")
\`\`\`

#### \`ContextVariableError\`
Raised when context variable operations fail.

\`\`\`python
from puffinflow.exceptions import ContextVariableError

try:
    value = context.get_variable("nonexistent_key")
except ContextVariableError as e:
    print(f"Context error: {e.message}")
\`\`\`

---

## Utilities

### Checkpoint Management

#### \`save_checkpoint(context: Context, filepath: str) -> None\`
Saves workflow state to file.

**Parameters:**
- \`context\` (Context): Context to save
- \`filepath\` (str): Path to save checkpoint

#### \`load_checkpoint(filepath: str) -> Context\`
Loads workflow state from file.

**Parameters:**
- \`filepath\` (str): Path to checkpoint file

**Returns:**
- \`Context\`: Restored context

**Example:**
\`\`\`python
from puffinflow.utils import save_checkpoint, load_checkpoint

# Save checkpoint
save_checkpoint(context, "workflow_checkpoint.json")

# Load checkpoint
restored_context = load_checkpoint("workflow_checkpoint.json")
\`\`\`

---

## Type Hints

Complete type definitions for better IDE support:

\`\`\`python
from typing import Any, Dict, List, Optional, Union, Callable, Awaitable
from puffinflow import Context, Agent, Priority

# State function signature
StateFunction = Callable[[Context], Awaitable[Optional[Union[str, List[str]]]]]

# Agent factory signature
AgentFactory = Callable[[], Agent]

# Context data types
ContextData = Dict[str, Any]
StateResult = Optional[Union[str, List[str]]]
\`\`\`

This reference covers all major PuffinFlow APIs. For complete implementation details, see the source code and additional documentation sections.
`.trim();
