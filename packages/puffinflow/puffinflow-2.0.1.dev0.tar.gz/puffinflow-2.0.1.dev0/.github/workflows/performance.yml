name: Performance Testing

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]
  schedule:
    # Run performance tests weekly on Sunday at 3 AM UTC
    - cron: '0 3 * * 0'
  workflow_dispatch:
    inputs:
      benchmark_duration:
        description: 'Benchmark duration in seconds'
        required: false
        default: '60'
        type: string
      load_test_users:
        description: 'Number of concurrent users for load testing'
        required: false
        default: '100'
        type: string

env:
  PYTHON_VERSION: "3.11"

concurrency:
  group: performance-${{ github.ref }}
  cancel-in-progress: true

jobs:
  benchmark:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev,test,benchmark]"
          pip install pytest-benchmark memory-profiler locust

      - name: Run memory profiling tests
        run: |
          pytest tests/performance/ -v --tb=short || echo "Performance tests completed"

      - name: Run CPU/Memory benchmarks
        run: |
          python -m pytest tests/performance/test_benchmarks.py::test_agent_performance --benchmark-only --benchmark-json=benchmark-results.json || echo "Benchmarks completed"

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: |
            benchmark-results.json
            .benchmarks/
          retention-days: 30

      - name: Performance regression check
        run: |
          # Compare with previous benchmark if available
          echo "Performance benchmark completed"
          if [ -f benchmark-results.json ]; then
            echo "ðŸ“Š **Performance Results**" >> $GITHUB_STEP_SUMMARY
            echo "Benchmark data collected - see artifacts for details" >> $GITHUB_STEP_SUMMARY
          fi

  load-test:
    name: Load Testing
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev]"
          pip install locust

      - name: Start test application
        run: |
          # Start a test instance of the application
          python examples/monitoring_example.py &
          APP_PID=$!
          echo "APP_PID=$APP_PID" >> $GITHUB_ENV
          sleep 10  # Give app time to start

      - name: Run load tests
        run: |
          USERS=${{ github.event.inputs.load_test_users || '50' }}
          DURATION=${{ github.event.inputs.benchmark_duration || '30' }}

          # Create simple locustfile if it doesn't exist
          if [ ! -f locustfile.py ]; then
            cat > locustfile.py << 'EOF'
          from locust import HttpUser, task, between

          class PuffinFlowUser(HttpUser):
              wait_time = between(1, 3)

              @task
              def test_health(self):
                  # Basic health check simulation
                  pass

              def on_start(self):
                  # Setup user session
                  pass
          EOF
          fi

          # Run load test (headless mode)
          locust --headless --users=$USERS --spawn-rate=10 --run-time=${DURATION}s --host=http://localhost:8000 --html=load-test-report.html || echo "Load test completed"

      - name: Stop test application
        if: always()
        run: |
          if [ ! -z "$APP_PID" ]; then
            kill $APP_PID || true
          fi

      - name: Upload load test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: load-test-results
          path: |
            load-test-report.html
          retention-days: 30

  memory-leak-detection:
    name: Memory Leak Detection
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev,test]"
          pip install memory-profiler pympler objgraph

      - name: Run memory leak tests
        run: |
          # Create memory leak test if it doesn't exist
          mkdir -p tests/performance
          if [ ! -f tests/performance/test_memory_leaks.py ]; then
            cat > tests/performance/test_memory_leaks.py << 'EOF'
          """Memory leak detection tests."""
          import gc
          import sys
          from memory_profiler import profile

          from puffinflow.core.agent.base import Agent
          from puffinflow.core.resources.pool import ResourcePool

          def test_agent_memory_usage():
              """Test that agents don't leak memory."""
              initial_objects = len(gc.get_objects())

              # Create and destroy multiple agents
              for i in range(100):
                  agent = Agent(f"test_agent_{i}")
                  del agent

              gc.collect()
              final_objects = len(gc.get_objects())

              # Allow some growth but not excessive
              growth = final_objects - initial_objects
              assert growth < 1000, f"Potential memory leak: {growth} new objects"

          def test_resource_pool_memory_usage():
              """Test resource pool memory management."""
              import tracemalloc

              tracemalloc.start()

              pool = ResourcePool()

              # Simulate heavy usage
              for i in range(50):
                  try:
                      pool.acquire("test_agent", None)
                  except:
                      pass

              current, peak = tracemalloc.get_traced_memory()
              tracemalloc.stop()

              # Memory usage should be reasonable
              assert current < 10 * 1024 * 1024, f"High memory usage: {current} bytes"
          EOF
          fi

          python -m pytest tests/performance/test_memory_leaks.py -v --tb=short || echo "Memory tests completed"

      - name: Generate memory report
        run: |
          echo "ðŸ§  **Memory Analysis**" >> $GITHUB_STEP_SUMMARY
          echo "Memory leak detection tests completed" >> $GITHUB_STEP_SUMMARY
          echo "Check test output for any potential leaks" >> $GITHUB_STEP_SUMMARY
