#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LLM Flow Engine ä½¿ç”¨æ¼”ç¤º - ä¸‰æ­¥ä¸Šæ‰‹
"""
import asyncio
import time
import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from loguru import logger
from llm_flow_engine import FlowEngine, ModelConfigProvider

logger.remove()
logger.add(sys.stderr, level="DEBUG")

async def demo_basic_usage():
    """æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ LLM Flow Engine"""
    logger.info("ğŸš€ LLM Flow Engine æ¼”ç¤º")
    logger.info("=" * 40)
    
    # ç¬¬1æ­¥: é…ç½®æ¨¡å‹
    logger.info("ç¬¬1æ­¥: é…ç½®æ¨¡å‹")
    
    # è‡ªåŠ¨å‘ç°æœ¬åœ°Ollamaæ¨¡å‹
    # model_provider = await ModelConfigProvider.from_host_async(
    #     api_host="http://127.0.0.1:11434", 
    #     platform="ollama"
    # )

    model_provider = ModelConfigProvider()
    platform = "openai"
    demo_host = "https://ai-proxy.4ba-cn.co/openrouter/v1/chat/completions"
    demo_free_key = "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"
    
    model_list = ["openai/gpt-oss-20b:free","moonshotai/kimi-k2:free", "google/gemma-3-12b-it:free","z-ai/glm-4.5-air:free"]
    for model in model_list:
        model_provider.add_single_model(model_name=model, platform=platform, 
            api_url=demo_host, api_key=demo_free_key)
    engine = FlowEngine(model_provider)
    models = engine.model_provider.list_supported_models()
    total = sum(len(models[p]) for p in models)
    logger.info(f"âœ… å·²é…ç½® {total} ä¸ªæ¨¡å‹")

    # ç¬¬2æ­¥: æ‰§è¡Œå·¥ä½œæµ
    logger.info("ç¬¬2æ­¥: æ‰§è¡Œå·¥ä½œæµ")
    
    dsl_file = './examples/demo_qa.yaml'
    if not os.path.exists(dsl_file):
        logger.error(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: {dsl_file}")
        return

    with open(dsl_file, 'r', encoding='utf-8') as f:
        dsl_content = f.read()
    
    # è¾“å…¥é—®é¢˜
    question = "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ"
    logger.info(f"â“ é—®é¢˜: {question}")
    
    # æ‰§è¡Œ
    start = time.time()
    flow_result = await engine.execute_dsl(
        dsl_content, 
        inputs={"workflow_input": {"question": question}}
    )
    logger.info(f"â±ï¸  ç”¨æ—¶: {time.time() - start:.1f}ç§’")
    
    # ç¬¬3æ­¥: æŸ¥çœ‹ç»“æœ
    logger.info("ç¬¬3æ­¥: æŸ¥çœ‹ç»“æœ")
    
    if flow_result['success']:
        logger.info("âœ… æ‰§è¡ŒæˆåŠŸ!")
        
        # æ˜¾ç¤ºä¸­é—´æ­¥éª¤çš„è¯¦ç»†ç»“æœ
        logger.info("ğŸ” è¯¦ç»†æ‰§è¡Œç»“æœ:")
        for step_name, result in flow_result['results'].items():
            if result.status == 'success':
                logger.info(f"  âœ… {step_name}: {result.output[:100]}..." if len(str(result.output)) > 100 else f"  âœ… {step_name}: {result.output}")
            else:
                logger.error(f"  âŒ {step_name}: {result.error}")
        
        # æ˜¾ç¤ºå·¥ä½œæµæœ€ç»ˆè¾“å‡º
        if 'workflow_output' in flow_result['results']:
            output_step = flow_result['results']['workflow_output']
            if output_step.status == 'success':
                logger.info(f"ğŸ“ æœ€ç»ˆç»“æœ: {output_step.output}")
            else:
                logger.error(f"âŒ å·¥ä½œæµè¾“å‡ºå¤±è´¥: {output_step.error}")
        # ç»Ÿè®¡
        success = sum(1 for r in flow_result['results'].values() if r.status == 'success')
        total = len(flow_result['results'])
        logger.info(f"ğŸ“Š {success}/{total} æ­¥éª¤æˆåŠŸ")
    else:
        logger.error(f"âŒ å¤±è´¥: {flow_result.get('error', 'æœªçŸ¥é”™è¯¯')}")

    logger.info("=" * 40)
    logger.info("ğŸ‰ æ¼”ç¤ºå®Œæˆ!")
    logger.info("ï¿½ æ ¸å¿ƒæ–¹æ³•:")
    logger.info("   â€¢ ModelConfigProvider.from_host_async() - è‡ªåŠ¨é…ç½®")
    logger.info("   â€¢ provider.add_single_model() - æ‰‹åŠ¨æ·»åŠ ")  
    logger.info("   â€¢ engine.execute_dsl() - æ‰§è¡Œå·¥ä½œæµ")

if __name__ == '__main__':
    asyncio.run(demo_basic_usage())
