#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¨¡å‹é…ç½®æ¼”ç¤ºç¤ºä¾‹
å±•ç¤ºä¸¤ç§æ¨¡å‹æ·»åŠ æ–¹å¼ï¼š
1. é€šè¿‡ api_host + api_key + å¹³å°è‡ªåŠ¨å‘ç°æ¨¡å‹
2. æ‰‹åŠ¨æ·»åŠ å•ä¸ªæ¨¡å‹çš„å®Œæ•´é…ç½®
"""
import asyncio
import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from loguru import logger
from llm_flow_engine import FlowEngine, ModelConfigProvider

logger.remove()
logger.add(sys.stderr, level="INFO")

async def demo_auto_discovery():
    """æ¼”ç¤ºæ–¹å¼1ï¼šé€šè¿‡APIä¸»æœºè‡ªåŠ¨å‘ç°æ¨¡å‹"""
    logger.info("=== æ–¹å¼1ï¼šé€šè¿‡APIä¸»æœºè‡ªåŠ¨å‘ç°æ¨¡å‹ ===")
    
    # æ–¹æ³•1ï¼šå¼‚æ­¥åˆ›å»ºï¼ˆæ¨èï¼‰
    logger.info("1.1 ä½¿ç”¨ from_host_async æ–¹æ³•ï¼ˆæ¨èï¼‰")
    try:
        provider1 = await ModelConfigProvider.from_host_async(
            api_host="http://localhost:11434",
            api_key="",  # Ollamaé€šå¸¸ä¸éœ€è¦å¯†é’¥
            platform="ollama"
        )
        
        models1 = provider1.list_supported_models()
        logger.info(f"è‡ªåŠ¨å‘ç°çš„Ollamaæ¨¡å‹: {models1}")
    except Exception as e:
        logger.warning(f"Ollamaè¿æ¥å¤±è´¥: {e}")
    
    # æ–¹æ³•2ï¼šåŒæ­¥åˆ›å»ºç„¶åå¼‚æ­¥åŠ è½½
    logger.info("\n1.2 åŒæ­¥åˆ›å»ºç„¶åå¼‚æ­¥åŠ è½½")
    provider2 = ModelConfigProvider.from_host(
        api_host="http://localhost:11434",
        api_key="",
        platform="ollama"
    )
    
    try:
        await provider2.load_models_from_simple_config()
        models2 = provider2.list_supported_models()
        logger.info(f"å¼‚æ­¥åŠ è½½çš„æ¨¡å‹: {models2}")
    except Exception as e:
        logger.warning(f"å¼‚æ­¥åŠ è½½å¤±è´¥: {e}")
    
    # æ–¹æ³•3ï¼šåŠ¨æ€æ·»åŠ åˆ°ç°æœ‰æä¾›è€…
    logger.info("\n1.3 åŠ¨æ€æ·»åŠ åˆ°ç°æœ‰æä¾›è€…")
    provider3 = ModelConfigProvider()
    logger.info(f"æ·»åŠ å‰çš„æ¨¡å‹: {provider3.list_supported_models()}")
    
    try:
        await provider3.add_models_from_host(
            api_host="http://localhost:11434",
            api_key="",
            platform="ollama"
        )
        models3 = provider3.list_supported_models()
        logger.info(f"åŠ¨æ€æ·»åŠ åçš„æ¨¡å‹: {models3}")
    except Exception as e:
        logger.warning(f"åŠ¨æ€æ·»åŠ å¤±è´¥: {e}")

def demo_single_model():
    """æ¼”ç¤ºæ–¹å¼2ï¼šæ‰‹åŠ¨æ·»åŠ å•ä¸ªæ¨¡å‹çš„å®Œæ•´é…ç½®"""
    logger.info("\n=== æ–¹å¼2ï¼šæ‰‹åŠ¨æ·»åŠ å•ä¸ªæ¨¡å‹é…ç½® ===")
    
    # åˆ›å»ºåŸºç¡€æä¾›è€…
    provider = ModelConfigProvider()
    logger.info(f"åˆå§‹é»˜è®¤æ¨¡å‹: {provider.list_supported_models()}")
    
    # æ–¹æ³•1ï¼šä½¿ç”¨ add_single_model æ–¹æ³•
    logger.info("\n2.1 ä½¿ç”¨ add_single_model æ–¹æ³•æ·»åŠ OpenAIæ¨¡å‹")
    provider.add_single_model(
        model_name="gpt-4",
        platform="openai",
        api_url="https://api.openai.com/v1/chat/completions",
        api_key="your-openai-key",
        auth_header="Bearer",
        message_format="openai",
        max_tokens=4096,
        supports=["temperature", "top_p", "frequency_penalty", "presence_penalty", "stop"]
    )
    
    # æ–¹æ³•2ï¼šæ·»åŠ DeepSeekæ¨¡å‹
    logger.info("2.2 æ·»åŠ DeepSeekæ¨¡å‹")
    provider.add_single_model(
        model_name="deepseek-chat",
        platform="openai_compatible",
        api_url="https://api.deepseek.com/v1/chat/completions",
        api_key="your-deepseek-key",
        auth_header="Bearer",
        message_format="openai",
        max_tokens=8192,
        supports=["temperature", "top_p", "frequency_penalty", "presence_penalty"]
    )
    
    # æ–¹æ³•3ï¼šæ·»åŠ æœ¬åœ°è‡ªå®šä¹‰æ¨¡å‹
    logger.info("2.3 æ·»åŠ æœ¬åœ°è‡ªå®šä¹‰æ¨¡å‹")
    provider.add_single_model(
        model_name="local-custom-llm",
        platform="custom",
        api_url="http://192.168.1.100:8080/v1/chat/completions",
        api_key="custom-token",
        auth_header="Bearer",
        message_format="openai",
        max_tokens=2048,
        supports=["temperature", "top_p"]
    )
    
    # æ˜¾ç¤ºæœ€ç»ˆçš„æ¨¡å‹åˆ—è¡¨
    final_models = provider.list_supported_models()
    logger.info(f"\næ·»åŠ å®Œæˆåçš„æ‰€æœ‰æ¨¡å‹: {final_models}")
    
    # è·å–ç‰¹å®šæ¨¡å‹é…ç½®
    gpt4_config = provider.get_model_config("gpt-4")
    logger.info(f"GPT-4æ¨¡å‹é…ç½®: {gpt4_config}")
    
    return provider

def demo_mixed_usage():
    """æ¼”ç¤ºæ··åˆä½¿ç”¨ä¸¤ç§æ–¹å¼"""
    logger.info("\n=== æ··åˆä½¿ç”¨æ¼”ç¤º ===")
    
    # å…ˆåˆ›å»ºåŸºç¡€æä¾›è€…
    provider = ModelConfigProvider()
    logger.info(f"åˆå§‹æ¨¡å‹: {provider.list_supported_models()}")
    
    # æ–¹å¼2ï¼šæ·»åŠ å‡ ä¸ªè‡ªå®šä¹‰æ¨¡å‹
    provider.add_single_model(
        model_name="claude-3",
        platform="anthropic",
        api_url="https://api.anthropic.com/v1/messages",
        api_key="your-anthropic-key",
        auth_header="x-api-key",
        message_format="anthropic",
        max_tokens=4096,
        supports=["temperature", "top_p", "max_tokens"]
    )
    
    logger.info("å·²æ·»åŠ Claude-3æ¨¡å‹")
    
    # æ£€æŸ¥æ‰€æœ‰é…ç½®
    all_models = provider.list_supported_models()
    logger.info(f"æ··åˆé…ç½®åçš„æ‰€æœ‰æ¨¡å‹: {all_models}")
    
    # éªŒè¯å„å¹³å°æ¨¡å‹æ•°é‡
    for platform, models in all_models.items():
        logger.info(f"{platform} å¹³å°: {len(models)} ä¸ªæ¨¡å‹")
    
    return provider

async def demo_with_flow_engine():
    """æ¼”ç¤ºä¸FlowEngineçš„é›†æˆä½¿ç”¨"""
    logger.info("\n=== ä¸FlowEngineé›†æˆæ¼”ç¤º ===")
    
    # æ–¹å¼1ï¼šè‡ªåŠ¨å‘ç°æ¨¡å‹
    try:
        provider = await ModelConfigProvider.from_host_async(
            api_host="http://localhost:11434",
            platform="ollama"
        )
        
        # æ–¹å¼2ï¼šæ·»åŠ é¢å¤–çš„è‡ªå®šä¹‰æ¨¡å‹
        provider.add_single_model(
            model_name="test-gpt",
            platform="test",
            api_url="https://test-api.com/v1/chat/completions",
            api_key="test-key",
            max_tokens=2048
        )
        
        # åˆ›å»ºFlowEngine
        engine = FlowEngine(provider)
        logger.info("âœ… FlowEngineåˆ›å»ºæˆåŠŸ")
        
        # æ˜¾ç¤ºå¼•æ“æ”¯æŒçš„æ¨¡å‹
        supported_models = engine.model_provider.list_supported_models()
        logger.info(f"FlowEngineæ”¯æŒçš„æ¨¡å‹: {supported_models}")
        
    except Exception as e:
        logger.warning(f"FlowEngineé›†æˆæ¼”ç¤ºå¤±è´¥: {e}")

async def main():
    """ä¸»å‡½æ•°"""
    logger.info("æ¨¡å‹é…ç½®æ¼”ç¤ºå¼€å§‹")
    logger.info("=" * 60)
    
    # æ–¹å¼1ï¼šè‡ªåŠ¨å‘ç°æ¨¡å‹æ¼”ç¤º
    await demo_auto_discovery()
    
    # æ–¹å¼2ï¼šæ‰‹åŠ¨æ·»åŠ å•ä¸ªæ¨¡å‹æ¼”ç¤º
    demo_single_model()
    
    # æ··åˆä½¿ç”¨æ¼”ç¤º
    demo_mixed_usage()
    
    # ä¸FlowEngineé›†æˆæ¼”ç¤º
    await demo_with_flow_engine()
    
    logger.info("\n" + "=" * 60)
    logger.info("æ¨¡å‹é…ç½®æ¼”ç¤ºå®Œæˆ")
    logger.info("\nğŸ“ æ€»ç»“:")
    logger.info("   æ–¹å¼1: é€šè¿‡ api_host + api_key + å¹³å° è‡ªåŠ¨å‘ç°æ¨¡å‹")
    logger.info("   æ–¹å¼2: æ‰‹åŠ¨æ·»åŠ å•ä¸ªæ¨¡å‹çš„å®Œæ•´é…ç½®")
    logger.info("   ä¸¤ç§æ–¹å¼å¯ä»¥æ··åˆä½¿ç”¨ï¼Œçµæ´»é…ç½®å„ç§æ¨¡å‹")

if __name__ == '__main__':
    asyncio.run(main())
