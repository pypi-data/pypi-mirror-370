# LLM Flow Engine

ğŸ‡¨ğŸ‡³ ä¸­æ–‡ç‰ˆæœ¬ | [ğŸ‡ºğŸ‡¸ English](../README.md)

ä¸€ä¸ªåŸºäº DSLï¼ˆé¢†åŸŸç‰¹å®šè¯­è¨€ï¼‰çš„ LLM å·¥ä½œæµå¼•æ“ï¼Œæ”¯æŒå¤šæ¨¡å‹åä½œã€ä¾èµ–ç®¡ç†å’Œç»“æœæ±‡æ€»ã€‚é€šè¿‡ YAML é…ç½®æ–‡ä»¶å®šä¹‰å¤æ‚çš„ AI å·¥ä½œæµï¼Œå®ç°å¤šä¸ª LLM æ¨¡å‹çš„ååŒå·¥ä½œã€‚

## âœ¨ æ ¸å¿ƒç‰¹æ€§

- **ğŸ”§ DSL å·¥ä½œæµå®šä¹‰** - ä½¿ç”¨ YAML æ ¼å¼å®šä¹‰å¤æ‚çš„ LLM å·¥ä½œæµ
- **ğŸ“Š DAG ä¾èµ–ç®¡ç†** - æ”¯æŒæœ‰å‘æ— ç¯å›¾çš„èŠ‚ç‚¹ä¾èµ–å…³ç³»å’Œå¹¶è¡Œæ‰§è¡Œ
- **ğŸ”— å ä½ç¬¦è§£æ** - ä½¿ç”¨ `${node.output}` è¯­æ³•å®ç°èŠ‚ç‚¹é—´æ•°æ®ä¼ é€’  
- **ğŸ¤– å¤šæ¨¡å‹æ”¯æŒ** - æ”¯æŒä¸åŒ LLM æ¨¡å‹çš„è°ƒç”¨å’Œç»“æœæ±‡æ€»
- **âš™ï¸ çµæ´»é…ç½®** - è‡ªå®šä¹‰æ¨¡å‹é…ç½®å’Œå‚æ•°ç®¡ç†
- **âš¡ å¼‚æ­¥æ‰§è¡Œ** - é«˜æ•ˆçš„å¼‚æ­¥ä»»åŠ¡å¤„ç†å’Œé”™è¯¯é‡è¯•
- **ğŸ“ˆ ç»“æœæ±‡æ€»** - å†…ç½®å¤šç§ç»“æœåˆå¹¶å’Œåˆ†æå‡½æ•°
- **ğŸ”§ å¯æ‰©å±•æ¶æ„** - æ”¯æŒè‡ªå®šä¹‰å‡½æ•°å’Œæ¨¡å‹é€‚é…å™¨

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒè¦æ±‚

- Python 3.8+
- aiohttp >= 3.8.0
- pyyaml >= 6.0
- loguru >= 0.7.0

### å®‰è£…

```bash
pip install llm-flow-engine
```

### åŸºç¡€ç”¨æ³•

```python
import asyncio
from llm_flow_engine import FlowEngine, ModelConfigProvider

async def main():
    # ç¬¬1æ­¥: é…ç½®æ¨¡å‹ï¼ˆè‡ªåŠ¨å‘ç°ï¼‰
    provider = await ModelConfigProvider.from_host_async(
        api_host="http://127.0.0.1:11434", 
        platform="ollama"
    )
    
    # ç¬¬2æ­¥: åˆ›å»ºå¼•æ“
    engine = FlowEngine(provider)
    
    # ç¬¬3æ­¥: æ‰§è¡Œå·¥ä½œæµ
    dsl_content = """
    metadata:
      version: "1.0"
      description: "ç®€å•é—®ç­”å·¥ä½œæµ"
    
    input:
      type: "start"
      name: "workflow_input"
      data:
        question: ""
    
    executors:
      - name: answer_step
        type: task
        func: llm_simple_call
        custom_vars:
          user_input: "${workflow_input.question}"
          model: "llama2"
    
    output:
      type: "end"
      name: "workflow_output"
      data:
        answer: "${answer_step.output}"
    """
    
    result = await engine.execute_dsl(
        dsl_content, 
        inputs={"workflow_input": {"question": "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ"}}
    )
    
    print(f"ç»“æœ: {result}")

if __name__ == "__main__":
    asyncio.run(main())
```

## ğŸ“‹ é¡¹ç›®ç»“æ„

```text
llm_flow_engine/
â”œâ”€â”€ __init__.py           # ä¸»åŒ…åˆå§‹åŒ–å’Œä¾¿æ·æ¥å£
â”œâ”€â”€ flow_engine.py        # ä¸»å¼•æ“å…¥å£
â”œâ”€â”€ dsl_loader.py         # DSL è§£æå™¨
â”œâ”€â”€ workflow.py           # ç»Ÿä¸€å·¥ä½œæµç®¡ç†(æ”¯æŒDAGå’Œç®€å•æ¨¡å¼)
â”œâ”€â”€ executor.py           # ä»»åŠ¡æ‰§è¡Œå™¨
â”œâ”€â”€ executor_result.py    # æ‰§è¡Œç»“æœå°è£…
â”œâ”€â”€ builtin_functions.py  # å†…ç½®å‡½æ•°åº“
â”œâ”€â”€ model_config.py       # æ¨¡å‹é…ç½®ç®¡ç†
â””â”€â”€ utils.py             # å·¥å…·å‡½æ•°

examples/
â”œâ”€â”€ demo_example.py       # å®Œæ•´ç¤ºä¾‹æ¼”ç¤º
â”œâ”€â”€ demo_qa.yaml          # å·¥ä½œæµDSLç¤ºä¾‹
â””â”€â”€ model_config_demo.py  # æ¨¡å‹é…ç½®æ¼”ç¤º
```

## ğŸ”§ æ¨¡å‹é…ç½®

### æ–¹å¼1: è‡ªåŠ¨å‘ç°ï¼ˆæ¨èï¼‰

```python
# è‡ªåŠ¨å‘ç° Ollama æ¨¡å‹
provider = await ModelConfigProvider.from_host_async(
    api_host="http://127.0.0.1:11434",
    platform="ollama"
)
```

### æ–¹å¼2: æ‰‹åŠ¨é…ç½®

```python
# åˆ›å»ºæä¾›è€…å¹¶æ‰‹åŠ¨æ·»åŠ æ¨¡å‹
provider = ModelConfigProvider()

# æ·»åŠ  OpenAI æ¨¡å‹
provider.add_single_model(
    model_name="gpt-4",
    platform="openai",
    api_url="https://api.openai.com/v1/chat/completions",
    api_key="your-api-key",
    max_tokens=4096
)

# æ·»åŠ è‡ªå®šä¹‰æ¨¡å‹
provider.add_single_model(
    model_name="custom-llm",
    platform="openai_compatible",
    api_url="https://your-api.com/v1/chat/completions",
    api_key="your-api-key",
    max_tokens=2048
)
```

## ğŸ“ DSL å·¥ä½œæµæ ¼å¼

### åŸºç¡€ç»“æ„

```yaml
metadata:
  version: "1.0"
  description: "å·¥ä½œæµæè¿°"

input:
  type: "start"
  name: "workflow_input"
  data:
    key: "value"

executors:
  - name: task1
    type: task
    func: function_name
    custom_vars:
      param1: "${input.key}"
      param2: "static_value"
    depends_on: []  # ä¾èµ–å…³ç³»
    timeout: 30     # è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    retry: 2        # é‡è¯•æ¬¡æ•°

output:
  type: "end"
  name: "workflow_output"
  data:
    result: "${task1.output}"
```

### å¤šæ¨¡å‹å·¥ä½œæµç¤ºä¾‹

```yaml
metadata:
  version: "1.0"
  description: "å¤šæ¨¡å‹é—®ç­”ä¸åˆ†æ"

input:
  type: "start"
  name: "workflow_input"
  data:
    question: ""

executors:
  # å¹¶è¡Œæ¨¡å‹è°ƒç”¨
  - name: model1_answer
    type: task
    func: llm_simple_call
    custom_vars:
      user_input: "${workflow_input.question}"
      model: "llama2"
    timeout: 30

  - name: model2_answer
    type: task
    func: llm_simple_call
    custom_vars:
      user_input: "${workflow_input.question}"
      model: "mistral"
    timeout: 30

  # åˆ†ææ­¥éª¤ï¼ˆä¾èµ–ä¸¤ä¸ªæ¨¡å‹ï¼‰
  - name: analysis
    type: task
    func: llm_simple_call
    custom_vars:
      user_input: "æ¯”è¾ƒè¿™äº›å›ç­”: 1) ${model1_answer.output} 2) ${model2_answer.output}"
      model: "llama2"
    depends_on: ["model1_answer", "model2_answer"]

output:
  type: "end"
  name: "workflow_output"
  data:
    original_question: "${workflow_input.question}"
    model1_response: "${model1_answer.output}"
    model2_response: "${model2_answer.output}"
    analysis: "${analysis.output}"
```

## ğŸ”Œ å†…ç½®å‡½æ•°

- **`llm_simple_call`** - åŸºç¡€ LLM æ¨¡å‹è°ƒç”¨
- **`text_process`** - æ–‡æœ¬é¢„å¤„ç†å’Œæ ¼å¼åŒ–
- **`result_summary`** - å¤šç»“æœæ±‡æ€»
- **`data_transform`** - æ•°æ®æ ¼å¼è½¬æ¢

## ğŸ§ª è¿è¡Œç¤ºä¾‹

```bash
# åŸºç¡€ç”¨æ³•æ¼”ç¤º
python examples/demo_example.py

# æ¨¡å‹é…ç½®æ¼”ç¤º
python examples/model_config_demo.py

# åŒ…ä½¿ç”¨æ–¹å¼æ¼”ç¤º
python examples/package_demo.py
```

## ğŸ“Š æ”¯æŒçš„å¹³å°

- **Ollama** - æœ¬åœ° LLM æ¨¡å‹
- **OpenAI** - GPT ç³»åˆ—æ¨¡å‹
- **OpenAI Compatible** - ä»»ä½• OpenAI å…¼å®¹çš„ API
- **Anthropic** - Claude ç³»åˆ—æ¨¡å‹
- **Custom** - è‡ªå®šä¹‰ API ç«¯ç‚¹

## ğŸ› ï¸ å¼€å‘

### æ­å»ºå¼€å‘ç¯å¢ƒ

```bash
git clone https://github.com/liguobao/llm-flow-engine.git
cd llm-flow-engine

# å®‰è£…å¼€å‘ä¾èµ–
pip install -e ".[dev]"

# è¿è¡Œæµ‹è¯•
pytest

# ä»£ç æ ¼å¼åŒ–
black .
```

### é¡¹ç›®éªŒè¯

```bash
# éªŒè¯é¡¹ç›®ç»“æ„å’Œé…ç½®
python validate_project.py
```

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ - è¯¦æƒ…è¯·æŸ¥çœ‹ [LICENSE](../LICENSE) æ–‡ä»¶ã€‚

## ğŸ¤ è´¡çŒ®

æ¬¢è¿è´¡çŒ®ï¼è¯·éšæ—¶æäº¤ Pull Requestã€‚

## ğŸ“ æ”¯æŒ

- ğŸ› Issues: [GitHub Issues](https://github.com/liguobao/llm-flow-engine/issues)
- ğŸ“– æ–‡æ¡£: [GitHub Wiki](https://github.com/liguobao/llm-flow-engine/wiki)

## ğŸŒŸ Star å†å²

å¦‚æœæ‚¨è§‰å¾—è¿™ä¸ªé¡¹ç›®æœ‰å¸®åŠ©ï¼Œè¯·è€ƒè™‘ç»™å®ƒä¸€ä¸ª starï¼â­

---

ç”± LLM Flow Engine å›¢é˜Ÿç”¨ â¤ï¸ åˆ¶ä½œ
