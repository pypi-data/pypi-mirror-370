name: Maintenance

on:
  schedule:
    # Run every Monday at 9 AM UTC
    - cron: '0 9 * * 1'
  workflow_dispatch:

jobs:
  dependency-check:
    name: Dependency Security Check
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
        pip install safety pip-audit

    - name: Run Safety check
      run: |
        safety check --json --output safety-report.json || true
        safety check --short-report

    - name: Run pip-audit
      run: |
        pip-audit --format=json --output=pip-audit-report.json || true
        pip-audit

    - name: Upload security reports
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: security-reports-${{ github.run_id }}
        path: |
          safety-report.json
          pip-audit-report.json

    - name: Create issue for vulnerabilities
      if: failure()
      uses: actions/github-script@v6
      with:
        script: |
          const title = `ðŸš¨ Security vulnerabilities detected - ${new Date().toISOString().split('T')[0]}`;
          const body = `
          ## Security Vulnerability Report
          
          The scheduled security scan has detected potential vulnerabilities in our dependencies.
          
          **Date**: ${new Date().toISOString()}
          **Workflow**: [${context.workflow}](${context.payload.repository.html_url}/actions/runs/${context.runId})
          
          ### Action Required
          
          1. Review the security reports in the workflow artifacts
          2. Update vulnerable dependencies
          3. Test the application after updates
          4. Close this issue once resolved
          
          ### Reports Available
          
          - Safety scan results
          - pip-audit results
          
          **Note**: This issue was created automatically by the maintenance workflow.
          `;
          
          github.rest.issues.create({
            owner: context.repo.owner,
            repo: context.repo.repo,
            title: title,
            body: body,
            labels: ['security', 'dependencies', 'automated']
          });

  code-quality:
    name: Code Quality Metrics
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
      with:
        # Fetch full history for better analysis
        fetch-depth: 0

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
        pip install radon xenon vulture

    - name: Calculate code complexity
      run: |
        echo "## Code Complexity Report" > complexity-report.md
        echo "" >> complexity-report.md
        
        echo "### Cyclomatic Complexity" >> complexity-report.md
        radon cc src/git_crossref/ --min B --show-complexity >> complexity-report.md || true
        
        echo "" >> complexity-report.md
        echo "### Maintainability Index" >> complexity-report.md
        radon mi src/git_crossref/ --show >> complexity-report.md || true
        
        echo "" >> complexity-report.md
        echo "### Halstead Metrics" >> complexity-report.md
        radon hal src/git_crossref/ >> complexity-report.md || true

    - name: Find dead code
      run: |
        echo "" >> complexity-report.md
        echo "### Dead Code Analysis" >> complexity-report.md
        vulture src/git_crossref/ --min-confidence 80 >> complexity-report.md || true

    - name: Check complexity thresholds
      run: |
        # Fail if there are any functions with high complexity
        xenon --max-absolute B --max-modules A --max-average A src/git_crossref/

    - name: Upload complexity report
      uses: actions/upload-artifact@v3
      with:
        name: code-quality-report-${{ github.run_id }}
        path: complexity-report.md

  test-coverage:
    name: Test Coverage Analysis
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"

    - name: Run tests with coverage
      run: |
        pytest tests/ \
          --cov=src/git_crossref \
          --cov-report=html \
          --cov-report=json \
          --cov-report=term-missing \
          --cov-fail-under=80

    - name: Coverage comment
      if: github.event_name == 'workflow_dispatch'
      uses: py-cov-action/python-coverage-comment-action@v3
      with:
        GITHUB_TOKEN: ${{ github.token }}

    - name: Upload coverage reports
      uses: actions/upload-artifact@v3
      with:
        name: coverage-report-${{ github.run_id }}
        path: |
          htmlcov/
          coverage.json

  performance-test:
    name: Performance Baseline
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
        pip install pytest-benchmark

    - name: Set up Git for performance tests
      run: |
        git config --global user.name "GitHub Actions"
        git config --global user.email "actions@github.com"

    - name: Run performance tests
      run: |
        # Create a simple performance test configuration
        mkdir -p perf-test
        cd perf-test
        git init
        echo "# Performance Test Repo" > README.md
        git add README.md
        git commit -m "Initial commit"
        
        cat > .gitcrossref << EOF
        remotes:
          test:
            url: "https://github.com/octocat/Hello-World.git"
            version: "master"
        files:
          test:
            - source: "README"
              destination: "vendor/README"
        EOF
        
        # Time the validation and dry-run operations
        echo "## Performance Baseline - $(date)" > ../performance-report.md
        echo "" >> ../performance-report.md
        
        echo "### Validation Performance" >> ../performance-report.md
        time git-crossref validate 2>&1 | tee -a ../performance-report.md
        
        echo "" >> ../performance-report.md
        echo "### Dry-run Performance" >> ../performance-report.md
        time git-crossref sync --dry-run 2>&1 | tee -a ../performance-report.md

    - name: Upload performance report
      uses: actions/upload-artifact@v3
      with:
        name: performance-report-${{ github.run_id }}
        path: performance-report.md

  documentation-check:
    name: Documentation Validation
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4

    - name: Check documentation links
      run: |
        # Check for broken internal links in README files
        echo "## Documentation Check Report" > doc-check-report.md
        echo "" >> doc-check-report.md
        
        echo "### README Files Found:" >> doc-check-report.md
        find . -name "README*.md" -type f | tee -a doc-check-report.md
        
        echo "" >> doc-check-report.md
        echo "### Link Validation:" >> doc-check-report.md
        
        # Simple check for relative links that might be broken
        for readme in README.md README.developers.md; do
          if [ -f "$readme" ]; then
            echo "Checking $readme..." >> doc-check-report.md
            grep -n "](\./" "$readme" >> doc-check-report.md || echo "No relative links found in $readme" >> doc-check-report.md
          fi
        done

    - name: Validate sample configurations
      run: |
        python -c "
        import yaml
        import re
        
        # Check if sample configs in README are valid YAML
        readme_files = ['README.md', 'README.developers.md']
        
        for readme in readme_files:
            try:
                with open(readme, 'r') as f:
                    content = f.read()
                
                # Extract YAML code blocks
                yaml_blocks = re.findall(r'```ya?ml\n(.*?)\n```', content, re.DOTALL)
                
                for i, block in enumerate(yaml_blocks):
                    try:
                        yaml.safe_load(block)
                        print(f'âœ“ YAML block {i+1} in {readme} is valid')
                    except yaml.YAMLError as e:
                        print(f'âœ— YAML block {i+1} in {readme} is invalid: {e}')
                        exit(1)
            except FileNotFoundError:
                print(f'File {readme} not found')
        
        print('âœ“ All documentation YAML samples are valid')
        "

    - name: Upload documentation report
      uses: actions/upload-artifact@v3
      with:
        name: documentation-report-${{ github.run_id }}
        path: doc-check-report.md

  cleanup:
    name: Cleanup Old Artifacts
    runs-on: ubuntu-latest
    steps:
    - name: Delete old workflow artifacts
      uses: actions/github-script@v6
      with:
        script: |
          const artifacts = await github.rest.actions.listArtifactsForRepo({
            owner: context.repo.owner,
            repo: context.repo.repo,
            per_page: 100
          });
          
          const cutoffDate = new Date();
          cutoffDate.setDate(cutoffDate.getDate() - 30); // Keep artifacts for 30 days
          
          for (const artifact of artifacts.data.artifacts) {
            const createdAt = new Date(artifact.created_at);
            if (createdAt < cutoffDate) {
              console.log(`Deleting artifact: ${artifact.name} (created: ${artifact.created_at})`);
              await github.rest.actions.deleteArtifact({
                owner: context.repo.owner,
                repo: context.repo.repo,
                artifact_id: artifact.id
              });
            }
          }
