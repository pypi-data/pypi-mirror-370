# This file was generated by Nuitka

# Stubs included by default


__name__ = ...



# Modules used internally, to allow implicit dependencies to be seen:
import os
import typing
import gllm_core
import gllm_core.utils
import gllm_inference.em_invoker.AzureOpenAIEMInvoker
import gllm_inference.em_invoker.GoogleEMInvoker
import gllm_inference.em_invoker.LangChainEMInvoker
import gllm_inference.em_invoker.OpenAICompatibleEMInvoker
import gllm_inference.em_invoker.OpenAIEMInvoker
import gllm_inference.em_invoker.TwelveLabsEMInvoker
import gllm_inference.em_invoker.VoyageEMInvoker
import gllm_inference.lm_invoker.AnthropicLMInvoker
import gllm_inference.lm_invoker.AzureOpenAILMInvoker
import gllm_inference.lm_invoker.BedrockLMInvoker
import gllm_inference.lm_invoker.DatasaurLMInvoker
import gllm_inference.lm_invoker.GoogleLMInvoker
import gllm_inference.lm_invoker.LangChainLMInvoker
import gllm_inference.lm_invoker.LiteLLMLMInvoker
import gllm_inference.lm_invoker.OpenAICompatibleLMInvoker
import gllm_inference.lm_invoker.OpenAILMInvoker
import gllm_inference.prompt_builder.PromptBuilder
import gllm_inference.output_parser.JSONOutputParser
import json
import abc
import pandas
import pydantic
import gllm_inference.lm_invoker.GoogleGenerativeAILMInvoker
import gllm_inference.lm_invoker.GoogleVertexAILMInvoker
import gllm_inference.lm_invoker.TGILMInvoker
import gllm_inference.multimodal_lm_invoker.AnthropicMultimodalLMInvoker
import gllm_inference.multimodal_lm_invoker.GoogleGenerativeAIMultimodalLMInvoker
import gllm_inference.multimodal_lm_invoker.GoogleVertexAIMultimodalLMInvoker
import gllm_inference.multimodal_lm_invoker.OpenAIMultimodalLMInvoker
import gllm_inference.multimodal_prompt_builder.MultimodalPromptBuilder
import gllm_inference.prompt_builder.AgnosticPromptBuilder
import gllm_inference.prompt_builder.HuggingFacePromptBuilder
import gllm_inference.prompt_builder.LlamaPromptBuilder
import gllm_inference.prompt_builder.MistralPromptBuilder
import gllm_inference.prompt_builder.OpenAIPromptBuilder
import re
import gllm_core.utils.retry
import gllm_inference.request_processor.LMRequestProcessor
import gllm_core.utils.imports
import gllm_inference.schema.ModelId
import gllm_inference.schema.ModelProvider
import openai
import importlib
import langchain_core
import langchain_core.embeddings
import gllm_inference.exceptions.parse_error_message
import gllm_inference.schema.Attachment
import gllm_inference.schema.AttachmentType
import gllm_inference.schema.EMContent
import gllm_inference.schema.Vector
import google
import google.auth
import google.genai
import google.genai.types
import asyncio
import concurrent
import concurrent.futures
import concurrent.futures.ThreadPoolExecutor
import typing_extensions
import gllm_inference.utils.preprocess_tei_input
import huggingface_hub
import gllm_inference.utils.load_langchain_model
import gllm_inference.utils.parse_model_data
import gllm_inference.em_invoker.langchain.TEIEmbeddings
import gllm_inference.utils.get_basic_auth_headers
import io
import twelvelabs
import base64
import sys
import voyageai
import voyageai.client_async
import asyncio.CancelledError
import asyncio.TimeoutError
import enum
import http
import http.HTTPStatus
import aiohttp
import httpx
import requests
import gllm_inference.schema.ErrorResponse
import gllm_core.constants
import gllm_core.event
import langchain_core.tools
import langchain_core.utils
import langchain_core.utils.function_calling
import gllm_inference.schema.EmitDataType
import gllm_inference.schema.LMOutput
import gllm_inference.schema.MultimodalPrompt
import gllm_inference.schema.Reasoning
import gllm_inference.schema.ResponseSchema
import gllm_inference.schema.TokenUsage
import gllm_inference.schema.ToolCall
import gllm_inference.schema.ToolResult
import anthropic
import aioboto3
import gllm_core.schema
import gllm_inference.schema.PromptRole
import langchain_core.language_models
import langchain_core.messages
import gllm_inference.schema.MultimodalOutput
import litellm
import pathlib
import pathlib.Path
import time
import jsonschema
import gllm_inference.schema.ContentPlaceholder
import gllm_inference.schema.MultimodalContent
import gllm_inference.utils.is_local_file_path
import gllm_inference.utils.is_remote_file_path
import gllm_inference.utils.validate_string_enum
import gllm_inference.schema.CodeExecResult
import gllm_inference.utils.get_mime_type
import gllm_inference.utils.load_google_vertexai_project_id
import vertexai
import vertexai.vision_models
import gllm_inference.utils.invoke_google_multimodal_lm
import vertexai.generative_models
import gllm_inference.utils.get_prompt_keys
import gllm_inference.utils.validate_prompt_builder_kwargs
import gllm_inference.prompt_formatter.AgnosticPromptFormatter
import gllm_inference.prompt_formatter.HuggingFacePromptFormatter
import gllm_inference.prompt_formatter.OpenAIPromptFormatter
import transformers
import gllm_core.utils.logger_manager
import mimetypes
import uuid
import filetype
import magic
import urllib
import urllib.parse