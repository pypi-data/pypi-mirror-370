# Example config.yaml for syftr with common locally overridden settings
# See `syftr/configuration.py::Settings for the full configuration object
# and instructions on how to configure Syftr

# Azure OpenAI API (required when using Azure OAI LLMs)
azure_oai:
  # Sensitive strings can also be placed in the appropriate file in runtime-secrets/
  # For example, put this key in a file named runtime-secrets/azure_oai__api_key
  api_key: "<your-api-key>"
  api_url: "https://my-azure-endpoint.openai.azure.com/"

# Google Cloud Platform VertexAI (required when using VertexAI LLMs)
gcp_vertex:
  project_id: "<your-project-id>"
  region: "europe-west1"
  # Put your GCP credentials file in runtime-secrets/gcp_vertex__credentials
  # Or uncomment the following:
  # credentials: >
  #   {
  #     "type": "service_account",
  #     "project_id": "<your-project-id>",
  #     "private_key_id": "<your-private-key-id>",
  #     "private_key": "<your-private-key>",
  #     "client_email": "<your-client-email>",
  #     "client_id": "<your-client-id>",
  #     "auth_uri": "https://accounts.google.com/o/oauth2/auth",
  #     "token_uri": "https://oauth2.googleapis.com/token",
  #     "auth_provider_x509_cert_url": "https://www.googleapis.com/oauth2/v1/certs",
  #     "client_x509_cert_url": "<your-cert-url>",
  #     "universe_domain": "googleapis.com"
  #   }

generative_models:
  # # azure_openai Provider Example
  # # See docs/llm-providers.md for full documentation of all provider configurations
  # azure_gpt_4o_mini:
  #   # Common configuration options
  #   provider: azure_openai            # Client type to use (required)
  #   model_name: "gpt-4o-mini"
  #   temperature: 0                    # Optional
  #   max_tokens: 2048                  # Optional - max output tokens
  #
  #   # Provider-specific configurations
  #   deployment_name: "gpt-4o-mini"    # Required for Azure OpenAI models
  #   api_version: "2024-06-01"         # Optional
  #   api_url: "https://my-endpoint.openai.azure.com" # Optional - defaults to azure_oai.api_url
  #   api_key: "asdf1234"                             # Optional - defaults to azure_oai.api_key
  #   additional_kwargs:                # Add additional parameters to OpenAI request body
  #     user: syftr
  #
  #   # Cost example - options are the same for all models (required)
  #   cost:
  #     type: tokens                    # tokens, characters, or hourly
  #     input: 1.00                     # Cost in USD per million
  #     output: 2.00                    # Cost in USD per million
  #     # rate: 12.00                   # Average cost per hour of inference server, when type is hourly

# Required for mistral-large LLM
azure_inference_mistral:
  api_key: "<your-api-key>"
  region_name: "northcentralus"
  default_deployment: "<your-Mistral-large-deployment>"
  model_name: "MistralLarge2411"

# Required for llama-33-70B LLM
azure_inference_llama33:
  api_key: "<your-api-key>"
  region_name: "northcentralus"
  default_deployment: "<your-Llama-3-3-70B-Instruct-deployment>"
  model_name: "AzureLlama3370B"

# Required for phi-4 LLM
azure_inference_phi4:
  api_key: "<your-api-key>"
  region_name: "northcentralus"
  default_deployment: "<your-Phi-4-deployment>"
  model_name: "AzurePhi-4"

# Required for cerebras-llama-31-8B and cerebras-llama-33-70B LLMs
cerebras:
  api_key: "<your-api-key>"

# Required for TogetherDeepseekR1 and TogetherDeepseekV3 LLMs
togetherai:
  api_key: "<your-api-key>"

# HuggingFace Embeddings (optional - required to use embedding models 
# which require a signed agreement, or HuggingFace inference endpoints)
# hf_embeddings:
#   api_key: "<your-api-key>"

# Use any relational DB provider supported by Optuna storage (recommended).
# If no dsn is provided, will use SQLite by default which allows running smaller
# examples locally.
# database:
#   dsn: "postgresql://user:pass@postgresserver:5432/syftr"
  # Optionally tune conection args.
  # Sessions must last a long time, so should be resilient to transient network interruptions
  #
  # engine_kwargs:
  #   pool_recycle: 300
  #   pool_pre_ping: true  # Important for PostgreSQL
  #   pool_size: 10
  #   max_overflow: 50
  #   # https://www.postgresql.org/docs/current/libpq-connect.html#LIBPQ-PARAMKEYWORDS
  #   connect_args:
  #     application_name: syftr
  #     connect_timeout: 60
  #     keepalives: 1  # Enable TCP keepalives
  #     keepalives_idle: 30  # Send keepalives after 30s of idle time
  #     keepalives_interval: 10  # Resend after 10s if no response
  #     keepalives_count: 5  # Give up after 5 failed keepalives

paths:
  tmp_dir: /tmp/syftr

# Instrumentation captures flow tracing information useful for debugging
# Run `phoenix serve` to launch Arize's open-source trace collection and viewing program,
# Or use an OTEL compatible endpoint like otel-collector.
# instrumentation:
#   tracing_enabled: true
#   otel_endpoint: http://localhost:6006/v1/traces

# Set local: true to automatically spin up a local Ray instance, or point this
# to your Ray cluster endpoint. Syftr uses the Ray Jobs Submission API to execute
# its workloads.
# ray:
#   remote_endpoint: "ray://myrayheadnode:10001"
#   local: false  # Set to False to use the remote cluster

# OpenAI-compatible endpoints listed here will automatically be added to your
# study search space, unless you have customized your search space LLMs.
# local_models:
#   default_api_key: "commonkey"
#   embedding:
#     - model_name: "BAAI/bge-small-en-v1.5"
#       api_base: "http://vllmhost:8001/v1"
#       additional_kwargs:
#         extra_body:
#           truncate_prompt_tokens: 512
#       api_key: "customapikeyhere"
