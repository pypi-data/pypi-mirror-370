import requests
from core.prompt_builder import build_prompt
from core.lang_detector import detect_language_from_extension


def summarize_code(code_chunk, file_path=None,environment="generic"):  # Summarizes code using Groq's LLaMA3 or Mixtral
    """
    Sends a code chunk to Groq API for summarization.

    Args:
        code_chunk (str): The code block to summarize.
        file_path (str): Optional path to display in the prompt context.

    Returns:
        str: The AI-generated summary.

    Notes:
        Uses build_prompt() from prompt_builder to format the LLM prompt.
    """
    GROQ_API_KEY = "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"
    MODEL = "llama3-70b-8192" # or "llama3-8b-8192"
    API_URL = "https://api.groq.com/openai/v1/chat/completions"

    headers = {
        "Authorization": f"Bearer {GROQ_API_KEY}",
        "Content-Type": "application/json"
    }

    lang = detect_language_from_extension(file_path or "")

    prompt = build_prompt(code_chunk, file_path=file_path, environment=environment)
    
    payload = {
        "model": MODEL,
        "messages": [
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.3
    }

    response = requests.post(API_URL, headers=headers, json=payload)

    if response.status_code == 200:
        return response.json()["choices"][0]["message"]["content"]
    else:
        raise RuntimeError(f"Groq API error: {response.status_code} - {response.text}")


def summarize_file_from_chunks(chunk_summaries, file_path=None, environment="generic"):
    """
    Performs a second summarization pass over combined chunk summaries.
    This improves coherence for long files.

    Args:
        chunk_summaries (List[str]): List of summaries from code chunks.
        file_path (str): Path to include in context.
        environment (str): Used in prompt customization.

    Returns:
        str: Coherent, high-level file summary.
    """
    combined_summary = "\n".join(chunk_summaries)
    if not combined_summary.strip():
        return ""

    prompt = (
        "You are a documentation assistant.\n"
        "Given the following partial code summaries from a file, generate a clean, high-level description of the file:\n\n"
        f"{combined_summary}\n\n"
        "Make it clear, concise, and useful to a developer."
    )

    # Simple call (Groq model)
    return summarize_code(prompt, file_path=file_path, environment=environment)



# import openai
# from core.prompt_builder import build_prompt

# def summarize_code(code_chunk, file_path=None):  # Summarizes code using OpenAI GPT-4 API
#     """
#     Sends a code chunk to OpenAI's GPT-4 API and returns a summary.

#     Args:
#         code_chunk (str): The code block to summarize.
#         file_path (str): Optional file path for prompt context.

#     Returns:
#         str: Summary generated by GPT-4.

#     Notes:
#         Uses build_prompt() to structure the LLM input.
#     """
#     openai.api_key = "your_openai_api_key_here"
#     MODEL = "gpt-4"

#     prompt = build_prompt(code_chunk, file_path)

#     response = openai.ChatCompletion.create(
#         model=MODEL,
#         messages=[
#             {"role": "user", "content": prompt}
#         ],
#         temperature=0.3
#     )

#     return response["choices"][0]["message"]["content"]
