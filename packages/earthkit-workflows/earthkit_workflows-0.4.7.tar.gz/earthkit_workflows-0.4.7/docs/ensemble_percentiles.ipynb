{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing ensemble percentiles from multiple netcdf files using cascade graph framework\n",
    "\n",
    "In this notebook we process multiple source files, each representing a ensemble forecast. Within each file we have 3-dimensional data in (time, y, x). \n",
    "\n",
    "We want to combine those the files along a new dimension 'member' and perform a quantile computation. We split the computation per time step to simulate larger data where loading everything into memory is not possible.\n",
    "\n",
    "We build the same workflow twice and compare the results from a direct computation using xarray/dask api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging as log\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "\n",
    "from cascade import Cascade\n",
    "from cascade.fluent import from_source, Payload, Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_data_files(n_members=3, n_leadtimes=7, shape=(4500, 5000)):\n",
    "    \"\"\"Create a test dataset with random data.\"\"\"\n",
    "    import tempfile\n",
    "    data = np.random.rand(n_members, n_leadtimes, *shape) * 100\n",
    "    coords = {\n",
    "        'member': np.arange(n_members),\n",
    "        'leadtime': pd.date_range(\"20240419T1200\", freq='h', periods=n_leadtimes),\n",
    "        'y': np.linspace(-90, 90, shape[0]),\n",
    "        'x': np.linspace(-180, 180, shape[1]),\n",
    "    }\n",
    "    ds = xr.Dataset(\n",
    "        data_vars={\n",
    "            'rainfall': (('member', 'leadtime', 'y', 'x'), data),\n",
    "        },\n",
    "        coords=coords,\n",
    "        attrs={'source': 'test', 'description': 'random data'}\n",
    "    )\n",
    "    tempdir = tempfile.mkdtemp()\n",
    "    outpaths, datasets = [], []\n",
    "    for mem in range(n_members):\n",
    "        outpaths += [os.path.join(tempdir, f'rainfall_ENS{mem:02d}.nc')]\n",
    "        datasets += [ds.sel(member=mem).drop_vars('member')]\n",
    "    xr.save_mfdataset(datasets, outpaths, compute=True)\n",
    "    return outpaths\n",
    "\n",
    "def write_netcdf(\n",
    "        nparray:np.ndarray, filename:str,\n",
    "        dims:list, coords:np.ndarray,\n",
    "        varname:str = 'rainfall', attrs:dict = {}, **kwargs\n",
    "    ):\n",
    "\n",
    "    xr.DataArray(\n",
    "        nparray,\n",
    "        dims=dims,\n",
    "        coords=coords,\n",
    "        name=varname,\n",
    "        attrs=attrs,\n",
    "    ).to_netcdf(filename, **kwargs)\n",
    "\n",
    "def compute_percentile(da: xr.DataArray, q: float | np.ndarray, **kwargs):\n",
    "    log.info(f'input array {da}')\n",
    "    return da.quantile(q, **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test parameters\n",
    "\n",
    "Here we define the size of our test. We can multiply this for even larger problems or just replace the `file_list` for an actual file list and test with your own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace this by a list of actual files and test on your own datasets\n",
    "members=3\n",
    "n_steps=7\n",
    "OUTFILE='rainfall_perc80.nc'\n",
    "quantiles=0.8\n",
    "file_list = create_test_data_files(members, n_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Graph with Cascade\n",
    "\n",
    "### Defining our data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payloads = []\n",
    "for fname in file_list:\n",
    "    payloads += [\n",
    "        Payload(\n",
    "            xr.open_dataarray,\n",
    "            [fname],\n",
    "            dict(mask_and_scale=False)\n",
    "        )\n",
    "    ]\n",
    "inputs = (\n",
    "    from_source(\n",
    "        payloads,\n",
    "        dims=['member'],\n",
    "    )\n",
    "    # we split the file over the forecast time steps\n",
    "    .expand(\n",
    "        dim='step', dim_size=n_steps, internal_dim=0, axis=0,\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting the members nodes together and doing the computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prev_node = None\n",
    "for step in range(n_steps):\n",
    "    # we first select the nodes from different ensemble members and same time step\n",
    "    step_nodes = inputs.select({'step': step}, drop=False)\n",
    "    # then we stack them along a member dimension\n",
    "    res = (\n",
    "        step_nodes.stack(\n",
    "            dim='member',\n",
    "            keep_dim=False,\n",
    "            axis=0,\n",
    "            backend_kwargs=dict(dim='member'),\n",
    "        )\n",
    "        # here we define the actual computation\n",
    "        .map(\n",
    "            Payload(\n",
    "                compute_percentile,\n",
    "                kwargs=dict(q=quantiles, dim='member', skipna=False)),\n",
    "        )\n",
    "    )\n",
    "    if prev_node is None:\n",
    "        prev_node = res\n",
    "    else:\n",
    "        prev_node = prev_node.join(\n",
    "            other_action=res,\n",
    "            dim='step',\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging final results and defining output in single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# output template\n",
    "tmplt_da = (\n",
    "    xr.open_dataset(file_list[0], chunks='auto')['rainfall']\n",
    ")\n",
    "coords = tmplt_da.coords\n",
    "dims = tmplt_da.dims\n",
    "attrs = tmplt_da.attrs\n",
    "\n",
    "actions = (\n",
    "    prev_node\n",
    "    .stack(\n",
    "        dim='step',\n",
    "        keep_dim=False,\n",
    "        axis=0,\n",
    "        backend_kwargs=dict(dim='leadtime'),\n",
    "    )\n",
    "    .map(\n",
    "        Payload(\n",
    "            write_netcdf,\n",
    "            [Node.input_name(0)],\n",
    "            kwargs=dict(\n",
    "                dims=dims,\n",
    "                attrs=attrs,\n",
    "                coords=coords,\n",
    "                varname='rainfall',\n",
    "                filename=OUTFILE,\n",
    "                mode='w', engine='netcdf4'\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The final Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cascade = Cascade.from_actions([actions])\n",
    "cascade.visualise(\"cascadegraph_percentile.html\", cdn_resources='in_line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from cascade.executors.dask import DaskLocalExecutor \n",
    "if os.path.exists(OUTFILE):\n",
    "    os.remove(OUTFILE)\n",
    "\n",
    "cascade.executor = DaskLocalExecutor(memory_limit=\"6GB\", n_workers=2, threads_per_worker=1)\n",
    "cascade.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct implementation with native xarray/dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_mfdataset(\n",
    "    file_list,\n",
    "    concat_dim='member',\n",
    "    combine='nested',\n",
    "    mask_and_scale=False,\n",
    "    chunks={'leadtime': 1},\n",
    ")\n",
    "ds = ds.chunk({'member': -1})\n",
    "\n",
    "res = list()\n",
    "for step in range(7):\n",
    "    da = ds['rainfall'].isel(leadtime=step).transpose('member', 'y', 'x')\n",
    "    perc = da.quantile(q=.80, dim='member', skipna=False)\n",
    "    res.append(perc)\n",
    "ref = (\n",
    "    xr.combine_nested(res, concat_dim='leadtime')\n",
    "    .assign_coords({'leadtime':coords['leadtime']})\n",
    "    .drop_vars(['quantile'])\n",
    ")\n",
    "ref.to_netcdf('dask_results.nc')\n",
    "ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref.data.visualize('daskgraph_percentile.png')  # you may need to install graphviz to make this work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "from dask.diagnostics import ProgressBar\n",
    "with ProgressBar():\n",
    "    ref.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = xr.open_dataset(OUTFILE, mask_and_scale=False)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = xr.open_dataset('dask_results.nc', mask_and_scale=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xr.testing.assert_allclose(ref, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ppcascade",
   "language": "python",
   "name": "ppcascade"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
