#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
åŸºäº1.txtæ–‡ä»¶ç”ŸæˆQA pairså¹¶è¿›è¡ŒéªŒè¯çš„å®Œæ•´è„šæœ¬
"""

import sys
import os
import json
from typing import List, Dict, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„ï¼ˆexamples çš„ä¸Šä¸€çº§ï¼‰
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from qa_gen_cn.generator import QAGenerator
from qa_gen_cn.llm_factory import LLMFactory
from qa_gen_cn.validator import QAPairValidator
from qa_gen_cn.utils import load_document

def load_document_content(doc_path: str) -> str:
    """åŠ è½½æ–‡æ¡£å†…å®¹"""
    docs = load_document(doc_path)
    return "\n".join([doc.page_content for doc in docs])

def generate_qa_pairs_with_validation(
    doc_path: str,
    llm_provider: str,
    llm_model: str ,
    show_chunks: bool ,
    chunk_size: int,
    chunk_overlap: int,
    validation_config: dict
) -> Dict[str, Any]:
    """
    ç”ŸæˆQA pairså¹¶è¿›è¡ŒéªŒè¯
    
    Args:
        doc_path: æ–‡æ¡£è·¯å¾„
        llm_provider: LLM provider('ollama' æˆ– 'openai')
        llm_model: æ¨¡å‹åç§°
        show_chunks: æ˜¯å¦æ˜¾ç¤ºæ–‡æ¡£å—
        chunk_size: æ–‡æ¡£å—å¤§å°
        chunk_overlap: æ–‡æ¡£å—é‡å å¤§å°
    
    Returns:
        åŒ…å«ç”Ÿæˆç»“æœå’ŒéªŒè¯ç»“æœçš„å­—å…¸
    """
    
    print(f"=== å¼€å§‹å¤„ç†æ–‡æ¡£: {doc_path} ===")
    
    # 1. åˆå§‹åŒ–LLM
    print("1. åˆå§‹åŒ–è¯­è¨€æ¨¡å‹...")
    try:
        llm = LLMFactory.create_llm(provider=llm_provider, model=llm_model)
        print(f"   âœ“ æˆåŠŸåˆå§‹åŒ– {llm_provider} æ¨¡å‹: {llm_model}")
    except Exception as e:
        print(f"   âœ— åˆå§‹åŒ–LLMå¤±è´¥: {e}")
        return {"error": f"LLMåˆå§‹åŒ–å¤±è´¥: {e}"}
    
    # 2. åˆå§‹åŒ–QAç”Ÿæˆå™¨
    print("2. åˆå§‹åŒ–QAç”Ÿæˆå™¨...")
    generator = QAGenerator(llm=llm, show_chunks=show_chunks)
    print("   âœ“ QAç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆ")
    
    # 3. ç”ŸæˆQA pairs
    print("3. ç”ŸæˆQA pairs...")
    # try:
    qa_pairs = generator.generate_from_document(
        doc_path=doc_path,
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap
    )
    # ç”Ÿæˆåè¿›è¡Œä¸€æ¬¡ç»“æ„æ¸…æ´—ï¼Œç¡®ä¿æ¯é¡¹éƒ½åŒ…å« question/answer
    qa_pairs = [
        p for p in qa_pairs
        if isinstance(p, dict) and ('question' in p) and ('answer' in p)
    ]
    print(f"   âœ“ æˆåŠŸç”Ÿæˆ {len(qa_pairs)} ä¸ªQA pairs")
    
    # 4. åŠ è½½åŸå§‹æ–‡æ¡£å†…å®¹ç”¨äºéªŒè¯
    print("4. åŠ è½½åŸå§‹æ–‡æ¡£å†…å®¹...")
    try:
        doc_content = load_document_content(doc_path)
        print(f"   âœ“ æ–‡æ¡£å†…å®¹åŠ è½½å®Œæˆï¼Œé•¿åº¦: {len(doc_content)} å­—ç¬¦")
    except Exception as e:
        print(f"   âœ— åŠ è½½æ–‡æ¡£å†…å®¹å¤±è´¥: {e}")
        return {"error": f"æ–‡æ¡£å†…å®¹åŠ è½½å¤±è´¥: {e}"}
    
    # 5. é…ç½®éªŒè¯å™¨
    print("5. é…ç½®éªŒè¯å™¨...")
    validation_config = validation_config
    
    validator = QAPairValidator(validation_config)
    print("   âœ“ éªŒè¯å™¨é…ç½®å®Œæˆ")
    
    # 6. éªŒè¯QA pairs
    print("6. éªŒè¯QA pairs...")
    try:
        validated_pairs = validator.validate(qa_pairs, doc_content)
        # éªŒè¯åå†åšä¸€æ¬¡å®‰å…¨è¿‡æ»¤ï¼Œé¿å…ç»Ÿè®¡é˜¶æ®µKeyError
        validated_pairs = [
            p for p in validated_pairs
            if isinstance(p, dict) and ('question' in p) and ('answer' in p)
        ]
        print(f"   âœ“ éªŒè¯å®Œæˆï¼Œé€šè¿‡éªŒè¯çš„QA pairs: {len(validated_pairs)}/{len(qa_pairs)}")
    except Exception as e:
        print(f"   âœ— éªŒè¯å¤±è´¥: {e}")
        return {"error": f"éªŒè¯å¤±è´¥: {e}"}
    
    # 7. ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
    print("7. ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š...")
    
    # ç»Ÿè®¡ä¿¡æ¯
    stats = {
        "total_generated": len(qa_pairs),
        "total_validated": len(validated_pairs),
        "validation_rate": (len(validated_pairs) / len(qa_pairs)) if qa_pairs else 0,
        "average_question_length": (
            sum(len(pair.get('question', '')) for pair in validated_pairs) / len(validated_pairs)
        ) if validated_pairs else 0,
        "average_answer_length": (
            sum(len(pair.get('answer', '')) for pair in validated_pairs) / len(validated_pairs)
        ) if validated_pairs else 0
    }
    
    # è´¨é‡åˆ†æ
    quality_analysis = []
    for i, pair in enumerate(validated_pairs):
        analysis = {
            "id": i + 1,
            "question_length": len(pair.get('question', '')),
            "answer_length": len(pair.get('answer', '')),
            "question": pair.get('question', ''),
            "answer": pair.get('answer', '')
        }
        quality_analysis.append(analysis)
    
    result = {
        "document_path": doc_path,
        "llm_config": {
            "provider": llm_provider,
            "model": llm_model
        },
        "generation_config": {
            "chunk_size": chunk_size,
            "chunk_overlap": chunk_overlap,
            "show_chunks": show_chunks
        },
        "validation_config": validation_config,
        "statistics": stats,
        "qa_pairs": validated_pairs,
        "quality_analysis": quality_analysis
    }
    
    print("   âœ“ æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
    return result

def save_results(result: Dict[str, Any], output_dir: str = "output"):
    """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    # ä¿å­˜å®Œæ•´ç»“æœ
    full_result_path = os.path.join(output_dir, "qa_generation_result.json")
    with open(full_result_path, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    # ä¿å­˜çº¯QA pairs
    qa_pairs_path = os.path.join(output_dir, "qa_pairs.json")
    with open(qa_pairs_path, 'w', encoding='utf-8') as f:
        json.dump(result.get("qa_pairs", []), f, ensure_ascii=False, indent=2)
    
    # ä¿å­˜ç»Ÿè®¡æŠ¥å‘Š
    stats_path = os.path.join(output_dir, "statistics.txt")
    with open(stats_path, 'w', encoding='utf-8') as f:
        f.write("=== QA Pairs ç”Ÿæˆå’ŒéªŒè¯ç»Ÿè®¡æŠ¥å‘Š ===\n\n")
        f.write(f"æ–‡æ¡£è·¯å¾„: {result.get('document_path', 'N/A')}\n")
        f.write(f"LLMé…ç½®: {result.get('llm_config', {})}\n\n")
        
        stats = result.get("statistics", {})
        f.write("ç»Ÿè®¡ä¿¡æ¯:\n")
        f.write(f"- æ€»ç”Ÿæˆæ•°é‡: {stats.get('total_generated', 0)}\n")
        f.write(f"- éªŒè¯é€šè¿‡æ•°é‡: {stats.get('total_validated', 0)}\n")
        f.write(f"- éªŒè¯é€šè¿‡ç‡: {stats.get('validation_rate', 0):.2%}\n")
        f.write(f"- å¹³å‡é—®é¢˜é•¿åº¦: {stats.get('average_question_length', 0):.1f} å­—ç¬¦\n")
        f.write(f"- å¹³å‡ç­”æ¡ˆé•¿åº¦: {stats.get('average_answer_length', 0):.1f} å­—ç¬¦\n\n")
        
        f.write("éªŒè¯é…ç½®:\n")
        for key, value in result.get("validation_config", {}).items():
            f.write(f"- {key}: {value}\n")
    
    print(f"ç»“æœå·²ä¿å­˜åˆ° {output_dir} ç›®å½•:")
    print(f"  - å®Œæ•´ç»“æœ: {full_result_path}")
    print(f"  - QA pairs: {qa_pairs_path}")
    print(f"  - ç»Ÿè®¡æŠ¥å‘Š: {stats_path}")

def print_summary(result: Dict[str, Any]):
    """æ‰“å°ç»“æœæ‘˜è¦"""
    if "error" in result:
        print(f"\nâŒ å¤„ç†å¤±è´¥: {result['error']}")
        return
    
    stats = result.get("statistics", {})
    print(f"\n=== å¤„ç†å®Œæˆæ‘˜è¦ ===")
    print(f"ğŸ“„ æ–‡æ¡£: {result.get('document_path', 'N/A')}")
    print(f"ğŸ¤– LLM: {result.get('llm_config', {}).get('model', 'N/A')}")
    print(f"ğŸ“Š ç”Ÿæˆç»Ÿè®¡:")
    print(f"   - æ€»ç”Ÿæˆ: {stats.get('total_generated', 0)} ä¸ªQA pairs")
    print(f"   - éªŒè¯é€šè¿‡: {stats.get('total_validated', 0)} ä¸ªQA pairs")
    print(f"   - é€šè¿‡ç‡: {stats.get('validation_rate', 0):.2%}")
    print(f"   - å¹³å‡é—®é¢˜é•¿åº¦: {stats.get('average_question_length', 0):.1f} å­—ç¬¦")
    print(f"   - å¹³å‡ç­”æ¡ˆé•¿åº¦: {stats.get('average_answer_length', 0):.1f} å­—ç¬¦")
    
    # æ˜¾ç¤ºå‰å‡ ä¸ªQA pairsä½œä¸ºç¤ºä¾‹
    qa_pairs = result.get("qa_pairs", [])
    if qa_pairs:
        print(f"\nğŸ“ ç¤ºä¾‹QA pairs (å‰3ä¸ª):")
        for i, pair in enumerate(qa_pairs[:3]):
            print(f"   {i+1}. Q: {pair['question']}")
            print(f"      A: {pair['answer'][:100]}{'...' if len(pair['answer']) > 100 else ''}")

def main():
    """ä¸»å‡½æ•°"""
    # æ–‡æ¡£è·¯å¾„
    doc_path = "examples/1.txt"
    
    # æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å­˜åœ¨
    if not os.path.exists(doc_path):
        print(f"âŒ æ–‡æ¡£ä¸å­˜åœ¨: {doc_path}")
        return
    # é…ç½® validation_config
    validation_config = {
        "uniqueness_distance_threshold": 0.1,
        "uniqueness_check_enabled":True,
        'similarity_model':'paraphrase-multilingual-MiniLM-L12-v2'
      
    }
    # é…ç½®å‚æ•°
    config = {
        "llm_provider": "ollama",  # æˆ–è€… "openai"
        "llm_model": "llama3.1:8b",  # ä½¿ç”¨æ›´ç¨³å®šçš„æ¨¡å‹
        "show_chunks": True,  # è®¾ç½®ä¸ºTrueå¯ä»¥çœ‹åˆ°æ–‡æ¡£åˆ†å—
        "chunk_size": 4000,
        "chunk_overlap": 20,
        "validation_config": validation_config
    }
    


    
    print("ğŸš€ å¼€å§‹QA pairsç”Ÿæˆå’ŒéªŒè¯æµç¨‹...")
    
    # ç”Ÿæˆå’ŒéªŒè¯QA pairs
    result = generate_qa_pairs_with_validation(
        doc_path=doc_path,
        **config
    )
    
    # æ‰“å°æ‘˜è¦
    print_summary(result)
    
    # ä¿å­˜ç»“æœ
    if "error" not in result:
        save_results(result)
        print(f"\nâœ… æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ° output ç›®å½•")

if __name__ == "__main__":
    main()
