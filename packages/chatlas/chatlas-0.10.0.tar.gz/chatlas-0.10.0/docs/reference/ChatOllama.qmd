# ChatOllama { #chatlas.ChatOllama }

```python
ChatOllama(
    model=None,
    *,
    system_prompt=None,
    turns=None,
    base_url='http://localhost:11434',
    seed=None,
    kwargs=None,
)
```

Chat with a local Ollama model.

[Ollama](https://ollama.com) makes it easy to run a wide-variety of
open-source models locally, making it a great choice for privacy
and security.

## Prerequisites {.doc-section .doc-section-prerequisites}


::: {.callout-note}
## Ollama runtime

`ChatOllama` requires the [ollama](https://ollama.com/download) executable
to be installed and running on your machine.
:::

::: {.callout-note}
## Pull model(s)

Once ollama is running locally, download a model from the command line
(e.g. `ollama pull llama3.2`).
:::

## Examples {.doc-section .doc-section-examples}

```python
from chatlas import ChatOllama

chat = ChatOllama(model="llama3.2")
chat.chat("What is the capital of France?")
```

## Parameters {.doc-section .doc-section-parameters}

| Name          | Type                                                                            | Description                                                                                                                                                                                                                                                                                                                                                                                                                 | Default                    |
|---------------|---------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------|
| model         | [Optional](`typing.Optional`)\[[str](`str`)\]                                   | The model to use for the chat. If `None`, a list of locally installed models will be printed.                                                                                                                                                                                                                                                                                                                               | `None`                     |
| system_prompt | [Optional](`typing.Optional`)\[[str](`str`)\]                                   | A system prompt to set the behavior of the assistant.                                                                                                                                                                                                                                                                                                                                                                       | `None`                     |
| turns         | [Optional](`typing.Optional`)\[[list](`list`)\[[Turn](`chatlas._turn.Turn`)\]\] | A list of turns to start the chat with (i.e., continuing a previous conversation). If not provided, the conversation begins from scratch. Do not provide non-`None` values for both `turns` and `system_prompt`. Each message in the list should be a dictionary with at least `role` (usually `system`, `user`, or `assistant`, but `tool` is also possible). Normally there is also a `content` field, which is a string. | `None`                     |
| base_url      | [str](`str`)                                                                    | The base URL to the endpoint; the default uses ollama's API.                                                                                                                                                                                                                                                                                                                                                                | `'http://localhost:11434'` |
| seed          | [Optional](`typing.Optional`)\[[int](`int`)\]                                   | Optional integer seed that helps to make output more reproducible.                                                                                                                                                                                                                                                                                                                                                          | `None`                     |
| kwargs        | [Optional](`typing.Optional`)\[\'ChatClientArgs\'\]                             | Additional arguments to pass to the `openai.OpenAI()` client constructor.                                                                                                                                                                                                                                                                                                                                                   | `None`                     |

## Note {.doc-section .doc-section-note}

This function is a lightweight wrapper around [](`~chatlas.ChatOpenAI`) with
the defaults tweaked for ollama.

## Limitations {.doc-section .doc-section-limitations}

`ChatOllama` currently doesn't work with streaming tools, and tool calling more
generally doesn't seem to work very well with currently available models.