#!/usr/bin/env python
# -*- coding: utf-8 -*-
# @Project      : AI.  @by PyCharm
# @File         : adapters
# @Time         : 2025/5/30 16:38
# @Author       : betterme
# @WeChat       : meutils
# @Software     : PyCharm
# @Description  :
import json

from meutils.pipe import *
from meutils.io.files_utils import to_url, to_url_fal
from meutils.str_utils.json_utils import json_path
from meutils.schemas.openai_types import CompletionRequest
from meutils.schemas.image_types import ImageRequest
from meutils.llm.openai_utils import chat_completion, chat_completion_chunk, create_chat_completion_chunk


async def stream_to_nostream(
        request: CompletionRequest,
):
    pass


async def chat_for_image(
        generate: Callable,
        request: CompletionRequest,
        api_key: Optional[str] = None,
):
    generate = partial(generate, api_key=api_key)

    if not request.stream or request.last_user_content.startswith(  # è·³è¿‡nextchat
            (
                    "hi",
                    "ä½¿ç”¨å››åˆ°äº”ä¸ªå­—ç›´æ¥è¿”å›è¿™å¥è¯çš„ç®€è¦ä¸»é¢˜",
                    "ç®€è¦æ€»ç»“ä¸€ä¸‹å¯¹è¯å†…å®¹ï¼Œç”¨ä½œåç»­çš„ä¸Šä¸‹æ–‡æç¤º promptï¼Œæ§åˆ¶åœ¨ 200 å­—ä»¥å†…"
            )):
        chat_completion.choices[0].message.content = "è¯·è®¾ç½®`stream=True`"
        return chat_completion

    prompt = request.last_user_content
    if request.last_urls:  # image_url
        urls = await to_url_fal(request.last_urls["image_url"], content_type="image/png")
        prompt = "\n".join(urls + [prompt])

    request = ImageRequest(
        model=request.model,
        prompt=prompt,
    )

    future_task = asyncio.create_task(generate(request))  # å¼‚æ­¥æ‰§è¡Œ

    async def gen():
        for i in f"> ğŸ–Œï¸æ­£åœ¨ç»˜ç”»\n\n```json\n{request.model_dump_json(exclude_none=True)}\n```\n\n":
            await asyncio.sleep(0.05)
            yield i

        response = await future_task

        for image in response.data:
            yield f"![{image.revised_prompt}]({image.url})\n\n"

    chunks = create_chat_completion_chunk(gen(), redirect_model=request.model)
    return chunks



async def chat_for_video(
        get_task: Callable,  # response
        taskid: str,
):
    """å¼‚æ­¥ä»»åŠ¡"""

    async def gen():

        # è·å–ä»»åŠ¡
        for i in f"""> VideoTask(id={taskid})\n""":
            await asyncio.sleep(0.03)
            yield i

        yield f"[ğŸ¤« ä»»åŠ¡è¿›åº¦]("
        for i in range(60):
            await asyncio.sleep(3)
            response = await get_task(taskid)  # åŒ…å«  "status"

            logger.debug(response)
            if response.get("status", "").lower().startswith(("succ", "fail")):

                yield ")ğŸ‰ğŸ‰ğŸ‰\n\n"

                yield f"""```json\n{json.dumps(response, indent=4, ensure_ascii=False)}\n```"""

                if urls := json_path(response, expr='$..[url,image_url,video_url]'):  # æ‰€æœ‰url
                    for i, url in enumerate(urls, 1):
                        yield f"\n\n[ä¸‹è½½é“¾æ¥{i}]({url})\n\n"

                break

            else:
                yield "ğŸš€"

    chunks = create_chat_completion_chunk(gen())
    return chunks


if __name__ == '__main__':
    request = CompletionRequest(
        model="deepseek-r1-Distill-Qwen-1.5B",
        messages=[
            {"role": "user", "content": "``hi"}
        ],
        stream=False,
    )
    arun(chat_for_image(None, request))
