{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenVINO-Easy: Stable Diffusion Example\n",
    "\n",
    "This notebook demonstrates how to use OpenVINO-Easy with Stable Diffusion for text-to-image generation.\n",
    "\n",
    "## Features Demonstrated\n",
    "- Automatic model downloading and conversion\n",
    "- Device selection (NPU → GPU → CPU)\n",
    "- INT8 quantization\n",
    "- Performance benchmarking\n",
    "- CLI usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "First, install OpenVINO-Easy with Stable Diffusion support:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install OpenVINO-Easy with Stable Diffusion support\n",
    "!pip install openvino-easy[sd]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import oe\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import json\n",
    "\n",
    "print(\n",
    "    f\"OpenVINO-Easy version: {oe.__version__ if hasattr(oe, '__version__') else 'dev'}\"\n",
    ")\n",
    "print(f\"Available devices: {oe.devices()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Stable Diffusion Model\n",
    "\n",
    "OpenVINO-Easy will automatically:\n",
    "1. Download the model from Hugging Face\n",
    "2. Convert it to OpenVINO format\n",
    "3. Select the best available device\n",
    "4. Cache the converted model for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Stable Diffusion model with FP16 precision\n",
    "print(\"Loading Stable Diffusion model...\")\n",
    "pipe = oe.load(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    dtype=\"fp16\",\n",
    "    device_preference=(\"NPU\", \"GPU\", \"CPU\"),\n",
    ")\n",
    "\n",
    "print(f\"Model loaded on device: {pipe.device}\")\n",
    "print(f\"Model info: {pipe.runtime.get_model_info()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Images\n",
    "\n",
    "Now let's generate some images using the loaded model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompts\n",
    "prompts = [\n",
    "    \"a beautiful sunset over mountains, digital art\",\n",
    "    \"a cyberpunk city at night with neon lights\",\n",
    "    \"a cute cat sitting in a garden, watercolor style\",\n",
    "]\n",
    "\n",
    "# Generate images\n",
    "images = []\n",
    "for i, prompt in enumerate(prompts):\n",
    "    print(f\"Generating image {i + 1}: {prompt}\")\n",
    "\n",
    "    # Run inference\n",
    "    result = pipe.infer(prompt)\n",
    "\n",
    "    # Convert result to PIL Image (assuming result is a numpy array)\n",
    "    if isinstance(result, np.ndarray):\n",
    "        # Normalize to 0-255 range if needed\n",
    "        if result.max() <= 1.0:\n",
    "            result = (result * 255).astype(np.uint8)\n",
    "\n",
    "        # Convert to PIL Image\n",
    "        if result.ndim == 3 and result.shape[2] == 3:\n",
    "            img = Image.fromarray(result)\n",
    "        else:\n",
    "            # Handle grayscale or other formats\n",
    "            img = Image.fromarray(result.squeeze())\n",
    "    else:\n",
    "        # Handle other result types\n",
    "        img = result\n",
    "\n",
    "    images.append(img)\n",
    "    print(f\"Image {i + 1} generated successfully!\")\n",
    "\n",
    "print(f\"Generated {len(images)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Generated Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display images\n",
    "fig, axes = plt.subplots(1, len(images), figsize=(15, 5))\n",
    "if len(images) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i, (img, prompt) in enumerate(zip(images, prompts)):\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].set_title(f\"{prompt[:30]}...\")\n",
    "    axes[i].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Performance\n",
    "\n",
    "Let's benchmark the model to see its performance characteristics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run benchmark\n",
    "print(\"Running benchmark...\")\n",
    "stats = pipe.benchmark(warmup=3, runs=10)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nBenchmark Results:\")\n",
    "print(f\"Device: {stats['device']}\")\n",
    "print(f\"Average Latency: {stats['mean_ms']:.2f} ms\")\n",
    "print(f\"FPS: {stats['fps']:.1f}\")\n",
    "print(f\"P50 Latency: {stats['p50_ms']:.2f} ms\")\n",
    "print(f\"P90 Latency: {stats['p90_ms']:.2f} ms\")\n",
    "print(f\"Min Latency: {stats['min_ms']:.2f} ms\")\n",
    "print(f\"Max Latency: {stats['max_ms']:.2f} ms\")\n",
    "print(f\"Standard Deviation: {stats['std_ms']:.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare FP16 vs INT8 Quantization\n",
    "\n",
    "Let's compare the performance of FP16 vs INT8 quantization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model with INT8 quantization\n",
    "print(\"Loading model with INT8 quantization...\")\n",
    "pipe_int8 = oe.load(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    dtype=\"int8\",\n",
    "    device_preference=(\"NPU\", \"GPU\", \"CPU\"),\n",
    ")\n",
    "\n",
    "# Benchmark INT8 model\n",
    "print(\"Benchmarking INT8 model...\")\n",
    "stats_int8 = pipe_int8.benchmark(warmup=3, runs=10)\n",
    "\n",
    "# Compare results\n",
    "print(\"\\nPerformance Comparison:\")\n",
    "print(\n",
    "    f\"FP16 - Device: {stats['device']}, FPS: {stats['fps']:.1f}, Latency: {stats['mean_ms']:.2f}ms\"\n",
    ")\n",
    "print(\n",
    "    f\"INT8 - Device: {stats_int8['device']}, FPS: {stats_int8['fps']:.1f}, Latency: {stats_int8['mean_ms']:.2f}ms\"\n",
    ")\n",
    "\n",
    "# Calculate improvement\n",
    "fps_improvement = (stats_int8[\"fps\"] - stats[\"fps\"]) / stats[\"fps\"] * 100\n",
    "latency_improvement = (\n",
    "    (stats[\"mean_ms\"] - stats_int8[\"mean_ms\"]) / stats[\"mean_ms\"] * 100\n",
    ")\n",
    "\n",
    "print(\"\\nINT8 vs FP16:\")\n",
    "print(f\"FPS Improvement: {fps_improvement:+.1f}%\")\n",
    "print(f\"Latency Improvement: {latency_improvement:+.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLI Usage\n",
    "\n",
    "You can also use OpenVINO-Easy from the command line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available devices\n",
    "!oe devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference via CLI\n",
    "!oe run runwayml/stable-diffusion-v1-5 -p \"a beautiful landscape\" --dtype fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark via CLI\n",
    "!oe bench runwayml/stable-diffusion-v1-5 --warmup 3 --runs 10 --output benchmark_results.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Load Benchmark Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save benchmark results\n",
    "with open(\"benchmark_results.json\", \"w\") as f:\n",
    "    json.dump(stats, f, indent=2)\n",
    "\n",
    "print(\"Benchmark results saved to benchmark_results.json\")\n",
    "\n",
    "# Load and display saved results\n",
    "with open(\"benchmark_results.json\", \"r\") as f:\n",
    "    loaded_stats = json.load(f)\n",
    "\n",
    "print(\"\\nLoaded benchmark results:\")\n",
    "print(json.dumps(loaded_stats, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Usage: Custom Cache Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model with custom cache directory\n",
    "pipe_custom = oe.load(\n",
    "    \"runwayml/stable-diffusion-v1-5\", dtype=\"fp16\", cache_dir=\"./my_cache\"\n",
    ")\n",
    "\n",
    "print(\"Model loaded with custom cache directory\")\n",
    "print(\"Cache location: ./my_cache\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Information\n",
    "\n",
    "Get detailed information about the loaded model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model information\n",
    "model_info = pipe.runtime.get_model_info()\n",
    "\n",
    "print(\"Model Information:\")\n",
    "print(f\"Device: {model_info['device']}\")\n",
    "print(f\"Model ID: {model_info['model_id']}\")\n",
    "print(f\"Data Type: {model_info['dtype']}\")\n",
    "print(\"\\nInput Information:\")\n",
    "for name, info in model_info[\"input_info\"].items():\n",
    "    print(f\"  {name}: shape={info['shape']}, dtype={info['dtype']}\")\n",
    "print(\"\\nOutput Information:\")\n",
    "for name, info in model_info[\"output_info\"].items():\n",
    "    print(f\"  {name}: shape={info['shape']}, dtype={info['dtype']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Easy Model Loading** - One line to download, convert, and load models\n",
    "2. **Automatic Device Selection** - NPU → GPU → CPU fallback\n",
    "3. **Quantization Support** - FP16 and INT8 precision\n",
    "4. **Performance Benchmarking** - Comprehensive latency and FPS analysis\n",
    "5. **CLI Integration** - Command-line tools for quick testing\n",
    "6. **Smart Caching** - Avoid redundant downloads and conversions\n",
    "\n",
    "OpenVINO-Easy makes it incredibly simple to use OpenVINO models with just three main functions:\n",
    "- `oe.load()` - Load any model\n",
    "- `pipe.infer()` - Run inference\n",
    "- `pipe.benchmark()` - Measure performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}