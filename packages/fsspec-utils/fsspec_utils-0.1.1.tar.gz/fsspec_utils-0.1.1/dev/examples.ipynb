{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fsspec import filesystem as fsspec_filesystem\n",
    "\n",
    "from fsspec_utils import filesystem\n",
    "\n",
    "from fsspec_utils.storage_options import StorageOptions, AwsStorageOptions\n",
    "\n",
    "# from fsspec.implementations.dirfs import DirFileSystem\n",
    "\n",
    "# --- Local Filesystem Examples ---\n",
    "\n",
    "# Local FileSystem\n",
    "fs_local = filesystem(\"file\")\n",
    "\n",
    "# DirFileSystem: local Directory FileSystem for /tmp\n",
    "fs_dir_local = filesystem(\"/tmp\", dirfs=True)\n",
    "\n",
    "# --- S3 Filesystem Examples ---\n",
    "\n",
    "# S3 FileSystem using dict storage_options\n",
    "# Note: In a real scenario, replace \"your_key\" and \"your_secret\" with actual AWS credentials.\n",
    "fs_s3_dict = filesystem(\"s3\", storage_options={\"key\": \"your_key\", \"secret\": \"your_secret\"})\n",
    "\n",
    "# S3 FileSystem using StorageOptions.create() and then to_filesystem()\n",
    "# This is the recommended type-safe way to create storage options.\n",
    "so_s3_create = StorageOptions.create(protocol=\"s3\", access_key_id=\"your_key\", secret_access_key=\"your_secret\")\n",
    "fs_s3_storage_options = so_s3_create.to_filesystem()\n",
    "\n",
    "# S3 FileSystem using AwsStorageOptions directly\n",
    "# This provides specific parameters for AWS.\n",
    "so_s3_aws = AwsStorageOptions(access_key_id=\"your_key\", secret_access_key=\"your_secret\")\n",
    "fs_s3_aws_direct = so_s3_aws.to_filesystem()\n",
    "\n",
    "# S3 FileSystem using AwsStorageOptions from AWS credentials profile\n",
    "# This assumes you have an AWS profile configured (e.g., in ~/.aws/credentials).\n",
    "# Replace \"lodl\" with your actual profile name.\n",
    "so_s3_profile = AwsStorageOptions.from_aws_credentials(profile=\"lodl\")\n",
    "fs_s3_aws_profile = so_s3_profile.to_filesystem()\n",
    "\n",
    "# S3 FileSystem using AWS credentials profile directly via filesystem() kwargs\n",
    "# The filesystem() function can sometimes infer from kwargs if they match StorageOptions parameters.\n",
    "fs_s3_kwargs_profile = filesystem(\"s3\", profile=\"lodl\", allow_invalid_certificates=True)\n",
    "\n",
    "# --- DirFileSystem for S3 ---\n",
    "\n",
    "# S3 Directory FileSystem for my-bucket. dirfs=True is optional, as it is often the default behavior for paths ending with a slash.\n",
    "# Replace \"my-bucket\" with your actual S3 bucket name.\n",
    "fs_dir_s3 = filesystem(\"s3://my-bucket\", dirfs=True)\n",
    "# S3 Directory FileSystem with storage_options\n",
    "fs_dir_s3_so = filesystem(\"s3://my-bucket\", storage_options={\"key\": \"your_key\", \"secret\": \"your_secret\"})\n",
    "print(f\"S3 DirFileSystem (default): {fs_dir_s3}\")\n",
    "print(f\"S3 DirFileSystem (with storage_options): {fs_dir_s3_so}\")\n",
    "\n",
    "# --- DirFileSystem for local path ---\n",
    "\n",
    "# Local Directory FileSystem.\n",
    "fs_dir_local = filesystem(\"./my_local_dir/\", dirfs=True)\n",
    "\n",
    "print(f\"Local DirFileSystem: {fs_dir_local}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data I/O Examples (requires actual S3 resources to run) ---\n",
    "\n",
    "# Read Parquet files from S3 into a pyarrow table\n",
    "df_parquet = fs_s3_storage_options.read_parquet(\"s3://test/ewn/mms2/raw/*.parquet\")\n",
    "\n",
    "# Creates an iterator to read Parquet files from S3 into a pyarrow table with batch size\n",
    "df_parquet_batch = fs_s3_storage_options.read_parquet(\"s3://test/ewn/mms2/raw/*.parquet\", batch_size=1)\n",
    "\n",
    "# Same is possible with other file formats (csv, json). There are also methods for writing data back to S3 in these formats.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a pyarrow dataset from parquet files. This dataset is lazy and can be used for efficient batch processing.\n",
    "ds_pyarrow = fs_s3_storage_options.pyarrow_dataset(\"s3://test/ewn/mms2/raw/\", format=\"parquet\")\n",
    "\n",
    "# Create a pyarrow parquet dataset from a metadata file that holds the schema and the metadata\n",
    "# (schema, column stats, row group size, offset,...) for all parquet files in the path.\n",
    "# This dataset is lazy and can be used for efficient batch processing, using pyarrow scanner, polars or duckdb.\n",
    "# Thanks to the column stats for all files, this is very efficient for querying and processing large datasets.\n",
    "ds_pyarrow_parquet = fs_s3_storage_options.pyarrow_parquet_dataset(\"s3://test/ewn/mms2/raw/\")\n",
    "\n",
    "# Creates a PyDDataset from all files in the path. A PyDala dataset is a specialized dataset for efficient data processing and querying.\n",
    "# It creates a metadata file that holds the schema and the metadata (column stats, row group size, offset,...) for all files in the path\n",
    "# which then can be used for efficient querying and processing. It also provides nice helper functions for working with the dataset,\n",
    "# like converting the dataset to a polars or pandas DataFrame, registering the dataset with a catalog, and more.\n",
    "ds_pydala = fs_s3_storage_options.pydala_dataset(\"s3://test/ewn/mms2/raw/\")\n",
    "\n",
    "# There are also methods for writing data back to S3 in these formats.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Read Parquet files from S3 into a pyarrow table using the filesystem directly\n",
    "table_pyarrow = pq.read_table(\"s3://test/ewn/mms2/raw/*.parquet\", filesystem=fs_s3_storage_options)\n",
    "\n",
    "# Write a pyarrow table to S3 as a Parquet file using the filesystem directly\n",
    "pq.write_table(table_pyarrow, \"s3://test/ewn/mms2/raw/output.parquet\", filesystem=fs_s3_storage_options)\n",
    "\n",
    "# Same is possible with other file formats (csv, json). There are also methods for writing data back to S3 in these formats.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "# Create a StorageOptions object for S3 with a profile and allow_invalid_certificates\n",
    "# This object can then be passed to libraries that accept fsspec-compatible storage options.\n",
    "so_s3_polars = AwsStorageOptions.create(profile=\"lodl\", allow_invalid_certificates=True)\n",
    "\n",
    "# Scan Parquet files from S3 using Polars, passing storage options\n",
    "df_polars = pl.scan_parquet(\"s3://test/ewn/mms2/raw/*.parquet\", storage_options=so_s3_polars.to_object_store_kwargs())\n",
    "# df_polars.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Automatic DirFileSystem Creation ---\n",
    "\n",
    "# When a path is provided as the protocol_or_path argument, \n",
    "# filesystem() automatically creates a DirFileSystem instance.\n",
    "# This provides a convenient way to work with directory-based filesystems.\n",
    "\n",
    "fs_auto_dir = filesystem(\"/tmp/test\")\n",
    "print(f\"Filesystem type: {type(fs_auto_dir).__name__}\")\n",
    "print(f\"Base path: {fs_auto_dir.path}\")\n",
    "\n",
    "# This is equivalent to explicitly creating a DirFileSystem:\n",
    "# from fsspec.implementations.dirfs import DirFileSystem\n",
    "# fs_explicit_dir = DirFileSystem(path=\"/tmp/test\", fs=filesystem(\"file\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from deltalake import DeltaTable\n",
    "\n",
    "# Create a DeltaTable instance from S3, passing storage options\n",
    "dt_delta = DeltaTable(\"s3://pu1/aumann/process_monitoring/results_delta\", storage_options=so_s3_polars.to_object_store_kwargs())\n",
    "dt_delta.file_uris()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fsspec-utils",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
