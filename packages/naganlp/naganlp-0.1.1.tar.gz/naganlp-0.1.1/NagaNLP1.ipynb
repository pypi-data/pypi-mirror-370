{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "00b07e31057744e68d72157bd6037d4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ac6b69881c394cc9ab2726ec75966879",
              "IPY_MODEL_f88ec086797f4bfdb75894ad445cfc22",
              "IPY_MODEL_6288fbf72b0d44adb4801742a776e4fa"
            ],
            "layout": "IPY_MODEL_35af9d9c68624875b99dbe863d097349"
          }
        },
        "ac6b69881c394cc9ab2726ec75966879": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0928511860b43f09ff29298678c92e8",
            "placeholder": "​",
            "style": "IPY_MODEL_8ab8743ecb9b4fbfb9f0d6918e9b5b34",
            "value": "Map: 100%"
          }
        },
        "f88ec086797f4bfdb75894ad445cfc22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd38522e4882400381839852950cb503",
            "max": 6743,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_72004a169c9147dfb1c508cd20a1d6ad",
            "value": 6743
          }
        },
        "6288fbf72b0d44adb4801742a776e4fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f02a420ded64ec1986a42a15f530f5e",
            "placeholder": "​",
            "style": "IPY_MODEL_5fb0dcf1b50444b0866a5fbd3fbb2406",
            "value": " 6743/6743 [00:03&lt;00:00, 1679.13 examples/s]"
          }
        },
        "35af9d9c68624875b99dbe863d097349": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0928511860b43f09ff29298678c92e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ab8743ecb9b4fbfb9f0d6918e9b5b34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bd38522e4882400381839852950cb503": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72004a169c9147dfb1c508cd20a1d6ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6f02a420ded64ec1986a42a15f530f5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5fb0dcf1b50444b0866a5fbd3fbb2406": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Colab Setup Cell\n",
        "!pip install -q transformers[torch] datasets scikit-learn pandas nltk\n",
        "\n",
        "import nltk\n",
        "# Download the 'punkt' tokenizer data if not already present\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "print(\"Setup complete. Please upload your 'merged.csv' and 'nagamese_manual_enriched.conll' files.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ne03jzJFtWHk",
        "outputId": "367d7ac2-81c5-43f0-e2d3-142dd088e52c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cudnn-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cudnn-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cudnn-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cudnn-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mSetup complete. Please upload your 'merged.csv' and 'nagamese_manual_enriched.conll' files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 662,
          "referenced_widgets": [
            "00b07e31057744e68d72157bd6037d4a",
            "ac6b69881c394cc9ab2726ec75966879",
            "f88ec086797f4bfdb75894ad445cfc22",
            "6288fbf72b0d44adb4801742a776e4fa",
            "35af9d9c68624875b99dbe863d097349",
            "f0928511860b43f09ff29298678c92e8",
            "8ab8743ecb9b4fbfb9f0d6918e9b5b34",
            "bd38522e4882400381839852950cb503",
            "72004a169c9147dfb1c508cd20a1d6ad",
            "6f02a420ded64ec1986a42a15f530f5e",
            "5fb0dcf1b50444b0866a5fbd3fbb2406"
          ]
        },
        "id": "SQdaxT80pdvD",
        "outputId": "b62c038f-78cd-4459-adb6-9fdaa3ed8e4f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "00b07e31057744e68d72157bd6037d4a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/6743 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-1471455664.py:111: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Starting Model Training ---\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='142' max='1140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 142/1140 03:46 < 26:58, 0.62 it/s, Epoch 0.37/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1140' max='1140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1140/1140 32:43, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.018466</td>\n",
              "      <td>0.995544</td>\n",
              "      <td>0.995560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.158900</td>\n",
              "      <td>0.011189</td>\n",
              "      <td>0.997125</td>\n",
              "      <td>0.997127</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.011300</td>\n",
              "      <td>0.010059</td>\n",
              "      <td>0.997491</td>\n",
              "      <td>0.997493</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Training Complete ---\n",
            "Model and tokenizer saved to 'nagamese_pos_model'\n",
            "\n",
            "--- Loading Trained Model for Inference ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Sentence: 'moi ghor te jai ase aru apuni'\n",
            "Predicted POS Tags:\n",
            "[('moi', 'PRON'), ('ghor', 'NOUN'), ('te', 'ADP'), ('jai', 'VERB'), ('ase', 'VERB'), ('aru', 'CCONJ'), ('apuni', 'PRON')]\n"
          ]
        }
      ],
      "source": [
        "# file: transformer_tagger.py\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForTokenClassification,\n",
        "    DataCollatorForTokenClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import torch\n",
        "import re\n",
        "import os\n",
        "\n",
        "def read_conll(path: str) -> Dataset:\n",
        "    \"\"\"Reads a CoNLL-formatted file and returns a Hugging Face Dataset.\"\"\"\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"The CoNLL file was not found at: {path}\")\n",
        "\n",
        "    sentences, tags = [], []\n",
        "    with open(path, encoding='utf-8') as f:\n",
        "        sent, sent_tags = [], []\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                if sent:\n",
        "                    sentences.append(sent)\n",
        "                    tags.append(sent_tags)\n",
        "                    sent, sent_tags = [], []\n",
        "            else:\n",
        "                parts = line.split()\n",
        "                if len(parts) >= 2:\n",
        "                    token, tag = parts[0], parts[-1]\n",
        "                    sent.append(token)\n",
        "                    sent_tags.append(tag)\n",
        "        if sent:\n",
        "            sentences.append(sent)\n",
        "            tags.append(sent_tags)\n",
        "\n",
        "    return Dataset.from_dict({'tokens': sentences, 'pos_tags': tags})\n",
        "\n",
        "def train_transformer_tagger(conll_path: str, model_save_path: str):\n",
        "    \"\"\"Trains and saves a Transformer-based POS tagger.\"\"\"\n",
        "    dataset = read_conll(conll_path)\n",
        "    unique_tags = sorted({tag for tag_list in dataset['pos_tags'] for tag in tag_list})\n",
        "    label2id = {label: i for i, label in enumerate(unique_tags)}\n",
        "    id2label = {i: label for i, label in enumerate(unique_tags)}\n",
        "\n",
        "    checkpoint = 'bert-base-multilingual-cased'\n",
        "    tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "    def tokenize_and_align_labels(examples):\n",
        "        tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True, padding=\"max_length\")\n",
        "        labels = []\n",
        "        for i, label in enumerate(examples[f\"pos_tags\"]):\n",
        "            word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "            previous_word_idx = None\n",
        "            label_ids = []\n",
        "            for word_idx in word_ids:\n",
        "                if word_idx is None or word_idx == previous_word_idx:\n",
        "                    label_ids.append(-100)\n",
        "                else:\n",
        "                    label_ids.append(label2id[label[word_idx]])\n",
        "                previous_word_idx = word_idx\n",
        "            labels.append(label_ids)\n",
        "        tokenized_inputs[\"labels\"] = labels\n",
        "        return tokenized_inputs\n",
        "\n",
        "    tokenized_ds = dataset.map(tokenize_and_align_labels, batched=True)\n",
        "    split = tokenized_ds.train_test_split(test_size=0.1, seed=42)\n",
        "\n",
        "    data_collator = DataCollatorForTokenClassification(tokenizer)\n",
        "    model = AutoModelForTokenClassification.from_pretrained(\n",
        "        checkpoint, num_labels=len(unique_tags), id2label=id2label, label2id=label2id\n",
        "    )\n",
        "\n",
        "    def compute_metrics(p):\n",
        "        predictions, labels = p\n",
        "        predictions = np.argmax(predictions, axis=2)\n",
        "        true_predictions = [\n",
        "            [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "            for prediction, label in zip(predictions, labels)\n",
        "        ]\n",
        "        true_labels = [\n",
        "            [id2label[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "            for prediction, label in zip(predictions, labels)\n",
        "        ]\n",
        "        results = classification_report(sum(true_labels, []), sum(true_predictions, []), output_dict=True, zero_division=0)\n",
        "        return {\"f1\": results[\"weighted avg\"][\"f1-score\"], \"accuracy\": accuracy_score(sum(true_labels, []), sum(true_predictions, []))}\n",
        "\n",
        "\n",
        "    # --- CORRECTION IS HERE ---\n",
        "    # Added 'report_to=\"none\"' to disable wandb logging.\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=os.path.join(model_save_path, 'results'),\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        learning_rate=2e-5,\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=16,\n",
        "        num_train_epochs=3,\n",
        "        weight_decay=0.01,\n",
        "        save_total_limit=1,\n",
        "        load_best_model_at_end=True,\n",
        "        report_to=\"none\",  # This disables wandb integration\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=split[\"train\"],\n",
        "        eval_dataset=split[\"test\"],\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    print(\"--- Starting Model Training ---\")\n",
        "    trainer.train()\n",
        "    print(\"--- Training Complete ---\")\n",
        "\n",
        "    trainer.save_model(model_save_path)\n",
        "    print(f\"Model and tokenizer saved to '{model_save_path}'\")\n",
        "\n",
        "\n",
        "class NagamesePosTagger:\n",
        "    \"\"\"A POS tagger for Nagamese using a fine-tuned Transformer model.\"\"\"\n",
        "    def __init__(self, model_path: str):\n",
        "        if not os.path.isdir(model_path):\n",
        "            raise OSError(f\"Trained model directory not found at: {model_path}\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "        self.model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
        "        self.model.eval()\n",
        "\n",
        "    def _simple_word_tokenize(self, text: str):\n",
        "        return re.findall(r\"\\w+|[^\\w\\s]\", text, re.UNICODE)\n",
        "\n",
        "    def predict(self, text: str):\n",
        "        \"\"\"Predicts POS tags for a given text, returning a list of (word, tag) tuples.\"\"\"\n",
        "        words = self._simple_word_tokenize(text)\n",
        "        inputs = self.tokenizer(\n",
        "            words, is_split_into_words=True, return_tensors='pt', truncation=True\n",
        "        )\n",
        "        with torch.no_grad():\n",
        "            logits = self.model(**inputs).logits\n",
        "\n",
        "        predictions = torch.argmax(logits, dim=2)\n",
        "        word_ids = inputs.word_ids()\n",
        "\n",
        "        results = []\n",
        "        previous_word_idx = None\n",
        "        for word_idx, pred_idx in zip(word_ids, predictions[0].tolist()):\n",
        "            if word_idx is not None and word_idx != previous_word_idx:\n",
        "                tag = self.model.config.id2label[pred_idx]\n",
        "                results.append((words[word_idx], tag))\n",
        "                previous_word_idx = word_idx\n",
        "        return results\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    conll_file = 'nagamese_manual_enriched.conll'\n",
        "    model_dir = 'nagamese_pos_model'\n",
        "\n",
        "    train_transformer_tagger(conll_file, model_dir)\n",
        "\n",
        "    print(\"\\n--- Loading Trained Model for Inference ---\")\n",
        "    try:\n",
        "        tagger = NagamesePosTagger(model_dir)\n",
        "        test_sentence = \"moi ghor te jai ase aru apuni\"\n",
        "        pos_tags = tagger.predict(test_sentence)\n",
        "\n",
        "        print(f\"\\nTest Sentence: '{test_sentence}'\")\n",
        "        print(\"Predicted POS Tags:\")\n",
        "        print(pos_tags)\n",
        "\n",
        "    except OSError as e:\n",
        "        print(f\"\\nError loading the model: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# file: nltk_tagger.py\n",
        "\n",
        "import nltk\n",
        "import random\n",
        "import pickle\n",
        "from nltk.tag import DefaultTagger, UnigramTagger, BigramTagger, TrigramTagger\n",
        "import os\n",
        "\n",
        "def read_conll_for_nltk(path: str):\n",
        "    \"\"\"Reads a CoNLL file into a list of tagged sentences for NLTK.\"\"\"\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"The CoNLL file was not found at: {path}\")\n",
        "\n",
        "    tagged_sents = []\n",
        "    with open(path, encoding='utf-8') as f:\n",
        "        sent = []\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                if sent:\n",
        "                    tagged_sents.append(sent)\n",
        "                    sent = []\n",
        "            else:\n",
        "                parts = line.split()\n",
        "                if len(parts) >= 2:\n",
        "                    token, tag = parts[0], parts[-1]\n",
        "                    sent.append((token, tag))\n",
        "        if sent:\n",
        "            tagged_sents.append(sent)\n",
        "    return tagged_sents\n",
        "\n",
        "def train_and_save_nltk_tagger(conll_path: str, model_path: str):\n",
        "    \"\"\"Trains and saves an NLTK backoff tagger.\"\"\"\n",
        "    tagged_sentences = read_conll_for_nltk(conll_path)\n",
        "    random.seed(42)\n",
        "    random.shuffle(tagged_sentences)\n",
        "\n",
        "    # Simple 90/10 split for training and testing\n",
        "    split_idx = int(len(tagged_sentences) * 0.9)\n",
        "    train_sents = tagged_sentences[:split_idx]\n",
        "    test_sents = tagged_sentences[split_idx:]\n",
        "\n",
        "    # Build the backoff tagger chain\n",
        "    default_tagger = DefaultTagger('NOUN') # Default to NOUN if unknown\n",
        "    unigram_tagger = UnigramTagger(train_sents, backoff=default_tagger)\n",
        "    bigram_tagger = BigramTagger(train_sents, backoff=unigram_tagger)\n",
        "    trigram_tagger = TrigramTagger(train_sents, backoff=bigram_tagger)\n",
        "\n",
        "    print(\"--- Evaluating NLTK Tagger ---\")\n",
        "    accuracy = trigram_tagger.accuracy(test_sents)\n",
        "    print(f\"Trigram Backoff Tagger Accuracy: {accuracy:.2%}\")\n",
        "\n",
        "    # Save the trained tagger using pickle\n",
        "    with open(model_path, 'wb') as f:\n",
        "        pickle.dump(trigram_tagger, f)\n",
        "    print(f\"NLTK model saved to '{model_path}'\")\n",
        "    return trigram_tagger\n",
        "\n",
        "\n",
        "class NltkPosTagger:\n",
        "    \"\"\"A POS Tagger for Nagamese using a pickled NLTK tagger object.\"\"\"\n",
        "    def __init__(self, model_path: str):\n",
        "        if not os.path.exists(model_path):\n",
        "            raise FileNotFoundError(f\"Tagger model file not found at: {model_path}\")\n",
        "        with open(model_path, 'rb') as f:\n",
        "            self.tagger = pickle.load(f)\n",
        "\n",
        "    def predict(self, tokens: list[str]):\n",
        "        \"\"\"\n",
        "        Tags a list of tokens.\n",
        "\n",
        "        Args:\n",
        "            tokens (list[str]): A list of pre-tokenized words.\n",
        "\n",
        "        Returns:\n",
        "            list[tuple[str, str]]: A list of (word, tag) tuples.\n",
        "        \"\"\"\n",
        "        return self.tagger.tag(tokens)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    conll_file = 'nagamese_manual_enriched.conll'\n",
        "    nltk_model_file = 'nagamese_nltk_tagger.pkl'\n",
        "\n",
        "    # --- Step 1: Train and save the NLTK tagger ---\n",
        "    train_and_save_nltk_tagger(conll_file, nltk_model_file)\n",
        "\n",
        "    # --- Step 2: Load the tagger and perform inference ---\n",
        "    print(\"\\n--- Loading NLTK Tagger for Inference ---\")\n",
        "    try:\n",
        "        nltk_tagger = NltkPosTagger(nltk_model_file)\n",
        "        test_tokens = ['moi', 'ghor', 'te', 'jai', 'ase']\n",
        "        tagged_sentence = nltk_tagger.predict(test_tokens)\n",
        "\n",
        "        print(f\"\\nTest Tokens: {test_tokens}\")\n",
        "        print(f\"Predicted POS Tags: {tagged_sentence}\")\n",
        "\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"\\nError loading the NLTK model: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_oswXQWOspln",
        "outputId": "105ef238-b16f-45bc-e643-fca080e56b43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Evaluating NLTK Tagger ---\n",
            "Trigram Backoff Tagger Accuracy: 99.64%\n",
            "NLTK model saved to 'nagamese_nltk_tagger.pkl'\n",
            "\n",
            "--- Loading NLTK Tagger for Inference ---\n",
            "\n",
            "Test Tokens: ['moi', 'ghor', 'te', 'jai', 'ase']\n",
            "Predicted POS Tags: [('moi', 'PRON'), ('ghor', 'NOUN'), ('te', 'ADP'), ('jai', 'VERB'), ('ase', 'VERB')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# file: nmt_translator.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from collections import Counter\n",
        "import random\n",
        "import os\n",
        "import pickle\n",
        "import re\n",
        "import pandas as pd # <-- MISSING IMPORT ADDED HERE\n",
        "\n",
        "# --- Data Loading ---\n",
        "def load_and_prep_data(filepath: str):\n",
        "    \"\"\"\n",
        "    Loads and preprocesses the parallel corpus from a CSV file.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(filepath):\n",
        "        print(f\"Error: The file at {filepath} was not found.\")\n",
        "        return None\n",
        "    df = pd.read_csv(filepath)\n",
        "    def clean_text(text):\n",
        "        if not isinstance(text, str): return \"\"\n",
        "        text = re.sub(r'<[^>]+>', '', text)\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        return text.strip().lower()\n",
        "\n",
        "    df['english_cleaned'] = df['english'].apply(clean_text)\n",
        "    df['nagamese_cleaned'] = df['nagamese'].apply(clean_text)\n",
        "    # Simple split by space for tokenization\n",
        "    df['english_tokens'] = df['english_cleaned'].apply(lambda x: x.split())\n",
        "    df['nagamese_tokens'] = df['nagamese_cleaned'].apply(lambda x: x.split())\n",
        "    return df\n",
        "\n",
        "# --- Vocabulary and Dataset ---\n",
        "class Vocab:\n",
        "    def __init__(self, tokens, min_freq=2):\n",
        "        self.pad_token, self.sos_token, self.eos_token, self.unk_token = '<pad>', '<sos>', '<eos>', '<unk>'\n",
        "        self.pad_idx, self.sos_idx, self.eos_idx, self.unk_idx = 0, 1, 2, 3\n",
        "\n",
        "        specials = [self.pad_token, self.sos_token, self.eos_token, self.unk_token]\n",
        "        counter = Counter(tok for seq in tokens for tok in seq)\n",
        "        vocab = sorted([tok for tok, freq in counter.items() if freq >= min_freq])\n",
        "\n",
        "        self.idx_to_token = specials + vocab\n",
        "        self.token_to_idx = {tok: idx for idx, tok in enumerate(self.idx_to_token)}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx_to_token)\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, df, src_vocab, tgt_vocab):\n",
        "        self.src_sents = df['nagamese_tokens'].tolist()\n",
        "        self.tgt_sents = df['english_tokens'].tolist()\n",
        "        self.src_vocab = src_vocab\n",
        "        self.tgt_vocab = tgt_vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src_sents)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src_tokens = [self.src_vocab.token_to_idx.get(tok, self.src_vocab.unk_idx) for tok in self.src_sents[idx]]\n",
        "        tgt_tokens = [self.tgt_vocab.token_to_idx.get(tok, self.tgt_vocab.unk_idx) for tok in self.tgt_sents[idx]]\n",
        "        return torch.tensor(src_tokens), torch.tensor(tgt_tokens)\n",
        "\n",
        "# --- Model Components ---\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.rnn = nn.GRU(emb_dim, hid_dim, bidirectional=True)\n",
        "        self.fc = nn.Linear(hid_dim * 2, hid_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src):\n",
        "        # src = [src_len, batch_size]\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        # embedded = [src_len, batch_size, emb_dim]\n",
        "        outputs, hidden = self.rnn(embedded)\n",
        "        # outputs = [src_len, batch_size, hid_dim * 2]\n",
        "        # hidden = [n_layers * 2, batch_size, hid_dim]\n",
        "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)))\n",
        "        # hidden = [batch_size, hid_dim]\n",
        "        return outputs, hidden\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hid_dim):\n",
        "        super().__init__()\n",
        "        self.attn = nn.Linear(hid_dim * 3, hid_dim) # hid_dim * 2 (encoder) + hid_dim (decoder)\n",
        "        self.v = nn.Parameter(torch.rand(hid_dim))\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        # hidden = [batch_size, hid_dim]\n",
        "        # encoder_outputs = [src_len, batch_size, hid_dim * 2]\n",
        "        batch_size = encoder_outputs.shape[1]\n",
        "        src_len = encoder_outputs.shape[0]\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        # hidden = [batch_size, src_len, hid_dim]\n",
        "        # encoder_outputs = [batch_size, src_len, hid_dim * 2]\n",
        "        energy = torch.tanh(self.attn(torch.cat([hidden, encoder_outputs], dim=2)))\n",
        "        # energy = [batch_size, src_len, hid_dim]\n",
        "        energy = energy.permute(0, 2, 1)\n",
        "        # energy = [batch_size, hid_dim, src_len]\n",
        "        v = self.v.repeat(batch_size, 1).unsqueeze(1)\n",
        "        # v = [batch_size, 1, hid_dim]\n",
        "        attention = torch.bmm(v, energy).squeeze(1)\n",
        "        # attention = [batch_size, src_len]\n",
        "        return torch.softmax(attention, dim=1)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, dropout, attention):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.attention = attention\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.rnn = nn.GRU(hid_dim * 2 + emb_dim, hid_dim)\n",
        "        self.fc_out = nn.Linear(hid_dim * 3 + emb_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        input = input.unsqueeze(0)\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        a = self.attention(hidden, encoder_outputs).unsqueeze(1)\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        weighted = torch.bmm(a, encoder_outputs).permute(1, 0, 2)\n",
        "        rnn_input = torch.cat((embedded, weighted), dim=2)\n",
        "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
        "        prediction = self.fc_out(torch.cat((output.squeeze(0), weighted.squeeze(0), embedded.squeeze(0)), dim=1))\n",
        "        return prediction, hidden.squeeze(0)\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "        batch_size = src.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        encoder_outputs, hidden = self.encoder(src)\n",
        "        input = trg[0,:]\n",
        "        for t in range(1, trg_len):\n",
        "            output, hidden = self.decoder(input, hidden, encoder_outputs)\n",
        "            outputs[t] = output\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top1 = output.argmax(1)\n",
        "            input = trg[t] if teacher_force else top1\n",
        "        return outputs\n",
        "\n",
        "def collate_fn(batch, src_vocab, tgt_vocab, device):\n",
        "    \"\"\"Pads sequences, adds SOS/EOS, and moves tensors to the correct device.\"\"\"\n",
        "    src_batch, tgt_batch = [], []\n",
        "    for src_sample, tgt_sample in batch:\n",
        "        src_batch.append(torch.cat([torch.tensor([src_vocab.sos_idx]), src_sample, torch.tensor([src_vocab.eos_idx])], dim=0))\n",
        "        tgt_batch.append(torch.cat([torch.tensor([tgt_vocab.sos_idx]), tgt_sample, torch.tensor([tgt_vocab.eos_idx])], dim=0))\n",
        "\n",
        "    src_padded = pad_sequence(src_batch, padding_value=src_vocab.pad_idx)\n",
        "    tgt_padded = pad_sequence(tgt_batch, padding_value=tgt_vocab.pad_idx)\n",
        "    return src_padded.to(device), tgt_padded.to(device)\n",
        "\n",
        "def train_model(model, loader, optimizer, criterion):\n",
        "    \"\"\"Main training loop for one epoch.\"\"\"\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for src, trg in loader:\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, trg)\n",
        "        output_dim = output.shape[-1]\n",
        "        # Flatten the output and target tensors\n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        trg = trg[1:].view(-1)\n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1) # Clip gradients\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    return epoch_loss / len(loader)\n",
        "\n",
        "class Translator:\n",
        "    \"\"\"Class to handle translation inference.\"\"\"\n",
        "    def __init__(self, model_path: str, vocabs_path: str, device):\n",
        "        if not os.path.exists(model_path): raise FileNotFoundError(f\"NMT model not found: {model_path}\")\n",
        "        if not os.path.exists(vocabs_path): raise FileNotFoundError(f\"Vocab file not found: {vocabs_path}\")\n",
        "\n",
        "        with open(vocabs_path, 'rb') as f:\n",
        "            self.src_vocab, self.tgt_vocab = pickle.load(f)\n",
        "\n",
        "        self.device = device\n",
        "\n",
        "        # Re-initialize model architecture to match training\n",
        "        ENC_EMB_DIM = 256\n",
        "        DEC_EMB_DIM = 256\n",
        "        HID_DIM = 512\n",
        "        DROPOUT = 0.5\n",
        "        enc = Encoder(len(self.src_vocab), ENC_EMB_DIM, HID_DIM, DROPOUT)\n",
        "        attn = Attention(HID_DIM)\n",
        "        dec = Decoder(len(self.tgt_vocab), DEC_EMB_DIM, HID_DIM, DROPOUT, attn)\n",
        "        self.model = Seq2Seq(enc, dec, device).to(device)\n",
        "        self.model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "        self.model.eval()\n",
        "\n",
        "    def translate(self, sentence: str, max_len=50):\n",
        "        \"\"\"Translates a single Nagamese sentence to English.\"\"\"\n",
        "        tokens = [tok.lower() for tok in sentence.split()]\n",
        "        src_indexes = [self.src_vocab.sos_idx] + [self.src_vocab.token_to_idx.get(t, self.src_vocab.unk_idx) for t in tokens] + [self.src_vocab.eos_idx]\n",
        "        src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            encoder_outputs, hidden = self.model.encoder(src_tensor)\n",
        "\n",
        "        trg_indexes = [self.tgt_vocab.sos_idx]\n",
        "        for _ in range(max_len):\n",
        "            trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(self.device)\n",
        "            with torch.no_grad():\n",
        "                output, hidden = self.model.decoder(trg_tensor, hidden, encoder_outputs)\n",
        "\n",
        "            pred_token = output.argmax(1).item()\n",
        "            trg_indexes.append(pred_token)\n",
        "            if pred_token == self.tgt_vocab.eos_idx:\n",
        "                break\n",
        "\n",
        "        trg_tokens = [self.tgt_vocab.idx_to_token[i] for i in trg_indexes]\n",
        "        return \" \".join(trg_tokens[1:-1])\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # --- Configuration ---\n",
        "    N_EPOCHS = 10\n",
        "    MODEL_PATH = 'nmt-nagamese-english.pt'\n",
        "    VOCABS_PATH = 'nmt-vocabs.pkl'\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "    # --- 1. Load and Prepare Data ---\n",
        "    df = load_and_prep_data('merged.csv')\n",
        "    if df is not None:\n",
        "        src_vocab = Vocab(df['nagamese_tokens'].tolist())\n",
        "        tgt_vocab = Vocab(df['english_tokens'].tolist())\n",
        "\n",
        "        with open(VOCABS_PATH, 'wb') as f:\n",
        "            pickle.dump((src_vocab, tgt_vocab), f)\n",
        "        print(f\"Source vocab size: {len(src_vocab)}\")\n",
        "        print(f\"Target vocab size: {len(tgt_vocab)}\")\n",
        "\n",
        "        dataset = TranslationDataset(df, src_vocab, tgt_vocab)\n",
        "        train_size = int(0.9 * len(dataset))\n",
        "        test_size = len(dataset) - train_size\n",
        "        train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "        # Correctly create the collate function with arguments\n",
        "        collate_with_args = lambda batch: collate_fn(batch, src_vocab, tgt_vocab, DEVICE)\n",
        "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_with_args)\n",
        "\n",
        "        # --- 2. Initialize Model ---\n",
        "        ENC_EMB_DIM = 256\n",
        "        DEC_EMB_DIM = 256\n",
        "        HID_DIM = 512\n",
        "        DROPOUT = 0.5\n",
        "\n",
        "        enc = Encoder(len(src_vocab), ENC_EMB_DIM, HID_DIM, DROPOUT)\n",
        "        attn = Attention(HID_DIM)\n",
        "        dec = Decoder(len(tgt_vocab), DEC_EMB_DIM, HID_DIM, DROPOUT, attn)\n",
        "        model = Seq2Seq(enc, dec, DEVICE).to(DEVICE)\n",
        "\n",
        "        optimizer = optim.Adam(model.parameters())\n",
        "        criterion = nn.CrossEntropyLoss(ignore_index=src_vocab.pad_idx)\n",
        "\n",
        "        # --- 3. Train the Model ---\n",
        "        print(\"\\n--- Starting NMT Model Training ---\")\n",
        "        for epoch in range(N_EPOCHS):\n",
        "            train_loss = train_model(model, train_loader, optimizer, criterion)\n",
        "            print(f'Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f}')\n",
        "\n",
        "        torch.save(model.state_dict(), MODEL_PATH)\n",
        "        print(f\"Model saved to {MODEL_PATH}\")\n",
        "\n",
        "        # --- 4. Load and Test the Translator ---\n",
        "        print(\"\\n--- Loading Trained Model for Inference ---\")\n",
        "        translator = Translator(MODEL_PATH, VOCABS_PATH, DEVICE)\n",
        "        test_sentence = \"moi ghor te jai ase\"\n",
        "        translation = translator.translate(test_sentence)\n",
        "        print(f\"Nagamese Input: '{test_sentence}'\")\n",
        "        print(f\"Predicted English Translation: '{translation}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Ici3TZOsz44",
        "outputId": "0e356344-c2e0-4f93-aadc-ccc69444ce11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Source vocab size: 3049\n",
            "Target vocab size: 3797\n",
            "\n",
            "--- Starting NMT Model Training ---\n",
            "Epoch: 01 | Train Loss: 5.616\n",
            "Epoch: 02 | Train Loss: 4.918\n",
            "Epoch: 03 | Train Loss: 4.429\n",
            "Epoch: 04 | Train Loss: 4.029\n",
            "Epoch: 05 | Train Loss: 3.715\n",
            "Epoch: 06 | Train Loss: 3.447\n",
            "Epoch: 07 | Train Loss: 3.263\n",
            "Epoch: 08 | Train Loss: 3.102\n",
            "Epoch: 09 | Train Loss: 2.953\n",
            "Epoch: 10 | Train Loss: 2.849\n",
            "Model saved to nmt-nagamese-english.pt\n",
            "\n",
            "--- Loading Trained Model for Inference ---\n",
            "Nagamese Input: 'moi ghor te jai ase'\n",
            "Predicted English Translation: 'i am going to my house'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# file: subword_tokenizer.py\n",
        "\n",
        "import pandas as pd\n",
        "import sentencepiece as spm\n",
        "import os\n",
        "import re\n",
        "\n",
        "def load_data_for_spm(filepath: str):\n",
        "    \"\"\"Loads and cleans data specifically for SentencePiece training.\"\"\"\n",
        "    if not os.path.exists(filepath):\n",
        "        print(f\"Error: The file at {filepath} was not found.\")\n",
        "        return None\n",
        "    df = pd.read_csv(filepath)\n",
        "    def clean_text(text):\n",
        "        if not isinstance(text, str): return \"\"\n",
        "        text = re.sub(r'<[^>]+>', '', text)\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        return text\n",
        "\n",
        "    df['english_cleaned'] = df['english'].apply(clean_text)\n",
        "    df['nagamese_cleaned'] = df['nagamese'].apply(clean_text)\n",
        "    return df\n",
        "\n",
        "def train_sentencepiece_model(df, model_prefix='naga_eng_bpe', vocab_size=8000):\n",
        "    \"\"\"\n",
        "    Trains a joint SentencePiece BPE model on the Nagamese and English text.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): DataFrame containing 'nagamese_cleaned' and 'english_cleaned' columns.\n",
        "        model_prefix (str): Prefix for the saved model files (.model, .vocab).\n",
        "        vocab_size (int): The target size of the vocabulary.\n",
        "    \"\"\"\n",
        "    # 1. Prepare a joint corpus file\n",
        "    joint_corpus_path = 'joint_corpus.txt'\n",
        "    with open(joint_corpus_path, 'w', encoding='utf-8') as f:\n",
        "        for text in df['nagamese_cleaned'].tolist():\n",
        "            f.write(f\"{text}\\n\")\n",
        "        for text in df['english_cleaned'].tolist():\n",
        "            f.write(f\"{text}\\n\")\n",
        "\n",
        "    print(f\"Joint corpus file created at '{joint_corpus_path}'\")\n",
        "\n",
        "    # 2. Train the SentencePiece model\n",
        "    spm.SentencePieceTrainer.Train(\n",
        "        f'--input={joint_corpus_path} '\n",
        "        f'--model_prefix={model_prefix} '\n",
        "        f'--vocab_size={vocab_size} '\n",
        "        f'--model_type=bpe '\n",
        "        f'--character_coverage=1.0'\n",
        "    )\n",
        "    print(f\"SentencePiece model trained. Files '{model_prefix}.model' and '{model_prefix}.vocab' are saved.\")\n",
        "\n",
        "class SubwordTokenizer:\n",
        "    \"\"\"A wrapper for a trained SentencePiece model.\"\"\"\n",
        "    def __init__(self, model_path: str):\n",
        "        if not os.path.exists(model_path):\n",
        "            raise FileNotFoundError(f\"SentencePiece model file not found at: {model_path}\")\n",
        "        self.sp = spm.SentencePieceProcessor()\n",
        "        self.sp.load(model_path)\n",
        "\n",
        "    def tokenize(self, text: str):\n",
        "        \"\"\"Tokenizes text into subword pieces.\"\"\"\n",
        "        return self.sp.encode_as_pieces(text)\n",
        "\n",
        "    def detokenize(self, pieces: list[str]):\n",
        "        \"\"\"Converts a list of pieces back into a string.\"\"\"\n",
        "        return self.sp.decode_pieces(pieces)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # --- 1. Load Data ---\n",
        "    dataframe = load_data_for_spm('merged.csv')\n",
        "\n",
        "    if dataframe is not None:\n",
        "        # --- 2. Train the Model ---\n",
        "        model_prefix = 'nagamese_english_spm'\n",
        "        train_sentencepiece_model(dataframe, model_prefix=model_prefix, vocab_size=8000)\n",
        "\n",
        "        # --- 3. Load and Test the Tokenizer ---\n",
        "        print(\"\\n--- Loading and Testing the Subword Tokenizer ---\")\n",
        "        try:\n",
        "            tokenizer = SubwordTokenizer(f'{model_prefix}.model')\n",
        "\n",
        "            test_sentence = \"abraham laga chokra david laga chokra\"\n",
        "            tokens = tokenizer.tokenize(test_sentence)\n",
        "            reconstructed = tokenizer.detokenize(tokens)\n",
        "\n",
        "            print(f\"\\nOriginal: {test_sentence}\")\n",
        "            print(f\"Tokens: {tokens}\")\n",
        "            print(f\"Reconstructed: {reconstructed}\")\n",
        "\n",
        "        except FileNotFoundError as e:\n",
        "            print(f\"Error: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6f11Oc39wtw",
        "outputId": "ada9fe7e-47f8-4380-ce1b-f3f0f2c1475f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Joint corpus file created at 'joint_corpus.txt'\n",
            "SentencePiece model trained. Files 'nagamese_english_spm.model' and 'nagamese_english_spm.vocab' are saved.\n",
            "\n",
            "--- Loading and Testing the Subword Tokenizer ---\n",
            "\n",
            "Original: abraham laga chokra david laga chokra\n",
            "Tokens: ['▁ab', 'raham', '▁laga', '▁chokra', '▁da', 'vid', '▁laga', '▁chokra']\n",
            "Reconstructed: abraham laga chokra david laga chokra\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab Setup Cell\n",
        "!pip install -q transformers[torch] datasets scikit-learn pandas nltk sentencepiece\n",
        "# Clone and install awesome-align\n",
        "!git clone https://github.com/neulab/awesome-align.git\n",
        "%cd awesome-align\n",
        "!pip install -e .\n",
        "%cd ..\n",
        "\n",
        "import nltk\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "print(\"\\nSetup complete. Ensure 'merged.csv' is uploaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuqjOwll-jCN",
        "outputId": "6be55ea3-796f-4e61-9886-1fe15ffb4739"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cudnn-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cudnn-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cudnn-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cudnn-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mCloning into 'awesome-align'...\n",
            "remote: Enumerating objects: 343, done.\u001b[K\n",
            "remote: Counting objects: 100% (115/115), done.\u001b[K\n",
            "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
            "remote: Total 343 (delta 100), reused 86 (delta 86), pack-reused 228 (from 1)\u001b[K\n",
            "Receiving objects: 100% (343/343), 596.38 KiB | 17.04 MiB/s, done.\n",
            "Resolving deltas: 100% (207/207), done.\n",
            "/content/awesome-align\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cudnn-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cudnn-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mObtaining file:///content/awesome-align\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  WARNING: Ignoring invalid distribution ~vidia-cudnn-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: tokenizers>=0.5.2 in /usr/local/lib/python3.11/dist-packages (from awesome_align==0.1.7) (0.21.2)\n",
            "Requirement already satisfied: torch>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from awesome_align==0.1.7) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from awesome_align==0.1.7) (4.67.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from awesome_align==0.1.7) (2.0.2)\n",
            "Collecting boto3 (from awesome_align==0.1.7)\n",
            "  Downloading boto3-1.39.17-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from awesome_align==0.1.7) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from awesome_align==0.1.7) (2.32.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.5.2->awesome_align==0.1.7) (0.34.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.2.0->awesome_align==0.1.7) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.2.0->awesome_align==0.1.7) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.2.0->awesome_align==0.1.7) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.2.0->awesome_align==0.1.7) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.2.0->awesome_align==0.1.7) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.2.0->awesome_align==0.1.7) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.2.0->awesome_align==0.1.7) (12.4.127)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.2.0->awesome_align==0.1.7)\n",
            "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.2.0->awesome_align==0.1.7) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.2.0->awesome_align==0.1.7) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.2.0->awesome_align==0.1.7) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.2.0->awesome_align==0.1.7) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.2.0->awesome_align==0.1.7) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.2.0->awesome_align==0.1.7) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.2.0->awesome_align==0.1.7) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.2.0->awesome_align==0.1.7) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.2.0->awesome_align==0.1.7) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.2.0->awesome_align==0.1.7) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.2.0->awesome_align==0.1.7) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.2.0->awesome_align==0.1.7) (1.3.0)\n",
            "Collecting botocore<1.40.0,>=1.39.17 (from boto3->awesome_align==0.1.7)\n",
            "  Downloading botocore-1.39.17-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3->awesome_align==0.1.7)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.14.0,>=0.13.0 (from boto3->awesome_align==0.1.7)\n",
            "  Downloading s3transfer-0.13.1-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->awesome_align==0.1.7) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->awesome_align==0.1.7) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->awesome_align==0.1.7) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->awesome_align==0.1.7) (2025.7.14)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.11/dist-packages (from botocore<1.40.0,>=1.39.17->boto3->awesome_align==0.1.7) (2.9.0.post0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.5.2->awesome_align==0.1.7) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.5.2->awesome_align==0.1.7) (6.0.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.5.2->awesome_align==0.1.7) (1.1.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.2.0->awesome_align==0.1.7) (3.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.40.0,>=1.39.17->boto3->awesome_align==0.1.7) (1.17.0)\n",
            "Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "Downloading boto3-1.39.17-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m162.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.39.17-py3-none-any.whl (13.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.9/13.9 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading s3transfer-0.13.1-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.3/85.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cudnn-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: nvidia-cudnn-cu12, jmespath, botocore, s3transfer, boto3, awesome_align\n",
            "  Running setup.py develop for awesome_align\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cudnn-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cudnn-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cudnn-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cudnn-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cudnn-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cudnn-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mSuccessfully installed awesome_align-0.1.7 boto3-1.39.17 botocore-1.39.17 jmespath-1.0.1 nvidia-cudnn-cu12 s3transfer-0.13.1\n",
            "/content\n",
            "\n",
            "Setup complete. Ensure 'merged.csv' is uploaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# file: word_aligner.py\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import subprocess\n",
        "\n",
        "def load_data_for_aligner(filepath: str):\n",
        "    \"\"\"Loads and cleans data for the word aligner.\"\"\"\n",
        "    if not os.path.exists(filepath):\n",
        "        print(f\"Error: The file at {filepath} was not found.\")\n",
        "        return None\n",
        "    df = pd.read_csv(filepath).dropna(subset=['english', 'nagamese'])\n",
        "    def clean_text(text):\n",
        "        if not isinstance(text, str): return \"\"\n",
        "        return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    df['english_cleaned'] = df['english'].apply(clean_text)\n",
        "    df['nagamese_cleaned'] = df['nagamese'].apply(clean_text)\n",
        "    return df\n",
        "\n",
        "def align_corpus(df, output_file='alignments.txt'):\n",
        "    \"\"\"\n",
        "    Runs awesome-align on the parallel corpus to generate word alignments.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): DataFrame with 'nagamese_cleaned' and 'english_cleaned' columns.\n",
        "        output_file (str): The file to save the alignments to.\n",
        "    \"\"\"\n",
        "    # 1. Prepare the input file for awesome-align\n",
        "    # --- CORRECTION 1: The format must be tab-separated ('\\t') ---\n",
        "    input_file = 'aligner_input.txt'\n",
        "    with open(input_file, 'w', encoding='utf-8') as f:\n",
        "        for _, row in df.iterrows():\n",
        "            f.write(f\"{row['english_cleaned']}\\t{row['nagamese_cleaned']}\\n\")\n",
        "\n",
        "    print(f\"Input file for aligner created at '{input_file}'\")\n",
        "\n",
        "    # 2. Run the awesome-align command\n",
        "    # --- CORRECTION 2: The path to the script is at the root of the cloned repo ---\n",
        "    align_script_path = 'awesome-align/run_align.py'\n",
        "    if not os.path.exists(align_script_path):\n",
        "        print(f\"Error: Alignment script not found at '{align_script_path}'.\")\n",
        "        print(\"Please ensure you have run the setup cell to clone and install awesome-align correctly.\")\n",
        "        return\n",
        "\n",
        "    model_name = 'bert-base-multilingual-cased'\n",
        "    command = [\n",
        "        'python3', align_script_path,\n",
        "        '--model_name_or_path', model_name,\n",
        "        '--data_file', input_file,\n",
        "        '--output_file', output_file,\n",
        "        '--extraction', 'softmax',\n",
        "        '--batch_size', '32'\n",
        "    ]\n",
        "\n",
        "    print(\"\\n--- Starting Word Alignment (this may take several minutes) ---\")\n",
        "    try:\n",
        "        # Using subprocess to run the command\n",
        "        process = subprocess.run(\n",
        "            command, check=True, capture_output=True, text=True\n",
        "        )\n",
        "        print(process.stdout) # Print the output from the script\n",
        "        print(f\"--- Alignment Complete. Results saved to '{output_file}' ---\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(\"--- An error occurred during alignment. ---\")\n",
        "        print(f\"Return Code: {e.returncode}\")\n",
        "        print(\"----- STDOUT -----\")\n",
        "        print(e.stdout)\n",
        "        print(\"----- STDERR -----\")\n",
        "        print(e.stderr)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # --- 1. Load Data ---\n",
        "    dataframe = load_data_for_aligner('merged.csv')\n",
        "\n",
        "    if dataframe is not None:\n",
        "        # --- 2. Generate Alignments ---\n",
        "        # We align a smaller subset for a quick demonstration.\n",
        "        # To run on the full dataset, use: align_corpus(dataframe)\n",
        "        align_corpus(dataframe.head(100), output_file='alignments_sample.txt')\n",
        "\n",
        "        # --- 3. Display Sample Alignments ---\n",
        "        print(\"\\n--- Sample of Generated Alignments ---\")\n",
        "        try:\n",
        "            with open('alignments_sample.txt', 'r', encoding='utf-8') as f:\n",
        "                for i, line in enumerate(f):\n",
        "                    if i >= 5: break\n",
        "                    print(line.strip())\n",
        "        except FileNotFoundError:\n",
        "            print(\"Alignment file not found. The alignment process may have failed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqXjsVak9xJa",
        "outputId": "ed275b13-94d1-4b9a-dbfc-d82da4d2e64d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input file for aligner created at 'aligner_input.txt'\n",
            "\n",
            "--- Starting Word Alignment (this may take several minutes) ---\n",
            "Loading the dataset...\n",
            "Line \"The book of the genealogy of Jesus Christ son of David son of Abraham\tAbraham laga chokra, David laga chokra, Jisu Khrista laga purbo khandan laga likhikena rakha kitab.\" (offset in bytes: 170) is not in the correct format. Skipping...\n",
            "Line \"Abraham fathered Isaac and Isaac fathered Jacob and Jacob fathered Judah and his brothers\tAbraham, Isaac laga baba hoise, aru Isaac, Jacob laga baba hoise, aru Jacob, Judah aru tai laga bhai-kokai khan laga baba hoise.\" (offset in bytes: 389) is not in the correct format. Skipping...\n",
            "Line \"and Judah fathered Perez and Zerah by Tamar and Perez fathered Hezrom and Hezrom fathered Aram\tJudah pora Perez laga baba, aru Zerah pora Tamar laga baba, Perez pora Hezron laga baba hoise, aru Hezron, Ram laga baba hoise.\" (offset in bytes: 612) is not in the correct format. Skipping...\n",
            "Line \"and Aram fathered Amminadab and Amminadab fathered Nahshon and Nahshon fathered Salmon\tRam, Amminadab laga baba hoise, aru Amminadab, Nahshon laga baba hoise, aru Nahshon, Salmon laga baba hoise.\" (offset in bytes: 808) is not in the correct format. Skipping...\n",
            "Line \"and Salmon fathered Boaz by Rahab and Boaz fathered Obed by Ruth and Obed fathered Jesse\tSalmon, Boaz laga baba hoise, itu to Rahab logot pora hoise, aru Boaz, Obed laga baba Ruth logot pora hoise, aru Obed, Jesse laga baba hoise.\" (offset in bytes: 1039) is not in the correct format. Skipping...\n",
            "Line \"and Jesse fathered David the king and David fathered Solomon by the wife of Uriah\tJesse, Raja David laga baba hoise, aru David, Solomon laga baba, Uriah laga maiki logot pora hoise.\" (offset in bytes: 1221) is not in the correct format. Skipping...\n",
            "Line \"and Solomon fathered Rehoboam and Rehoboam fathered Abijah and Abijah fathered Asaph\tSolomon pora Rehoboam laga baba hoise, aru Rehoboam pora Abijah laga baba hoise, aru Abijah pora Asa laga baba hoise.\" (offset in bytes: 1424) is not in the correct format. Skipping...\n",
            "Line \"and Asaph fathered Jehoshaphat and Jehoshaphat fathered Joram and Joram fathered Ozias\tAsa pora Jehoshaphat laga baba hoise, aru Jehoshaphat pora Joram laga baba hoise, aru Joram pora Uzziah laga baba hoise.\" (offset in bytes: 1632) is not in the correct format. Skipping...\n",
            "Line \"and Ozias fathered Jotham and Jotham fathered Ahaz and Ahaz fathered Hezekiah\tUzziah pora Jotham laga baba hoise, aru Jotham pora Ahaz laga baba hoise, aru Ahaz pora Hezekiah laga baba hoise.\" (offset in bytes: 1824) is not in the correct format. Skipping...\n",
            "Line \"and Hezekiah fathered Manasseh and Manasseh fathered Amos and Amos fathered Josiah\tHezekiah pora Manasseh laga baba hoise, aru Manasseh pora Amon laga baba hoise, aru Amon pora Josiah laga baba hoise.\" (offset in bytes: 2025) is not in the correct format. Skipping...\n",
            "Line \"and Josiah fathered Jechoniah and his brothers at the Babylonian deportation\tJosiah pora Jechoniah aru tailaga bhai-kokai khan laga baba hoise jitia taikhan ke Babylon te loi jaise.\" (offset in bytes: 2207) is not in the correct format. Skipping...\n",
            "Line \"and after the Babylonian deportation Jechoniah fathered Salathiel and Salathiel fathered Zerubbabel\tBabylon te loija pichete, Jechoniah pora Shealtiel laga baba hoise, aru Shealtiel pora Zerubbabel laga baba hoise.\" (offset in bytes: 2422) is not in the correct format. Skipping...\n",
            "Line \"and Zerubbabel fathered Abiud and Abiud fathered Eliakim and Eliakim fathered Azor\tZerubbabel pora Abiud laga baba hoise, aru Eliakim pora Azor laga baba hoise.\" (offset in bytes: 2583) is not in the correct format. Skipping...\n",
            "Line \"and Azor fathered Zadok and Zadok fathered Achim and Achim fathered Eliud\tAzor pora Zadok laga baba hoise, aru Zadok pora Achim laga baba hoise, aru Achim pora Eliud laga baba hoise.\" (offset in bytes: 2766) is not in the correct format. Skipping...\n",
            "Line \"and Eliud fathered Eleazar and Eleazar fathered Matthan and Matthan fathered Jacob\tEliud pora Eleazar laga baba hoise, aru Eleazar pora Matthan laga baba hoise, aru Matthan pora Jacob laga baba hoise.\" (offset in bytes: 2967) is not in the correct format. Skipping...\n",
            "Line \"and Jacob fathered Joseph the husband of Mary by whom Jesus was born the one called Christ all the generations from Abraham until David were 14 generations and from David until the Babylonian deportation were 14 generations and from the Babylonian deportation until the Christ were 14 generations\tJacob pora Joseph laga baba hoise, jun laga maiki Mary asele, jun pora Jisu ke jonom dise, aru Khrista koikena matise.\" (offset in bytes: 3383) is not in the correct format. Skipping...\n",
            "Line \"Now the birth of Jesus Christ was thus His mother Mary having been engaged to marry Joseph before they came together was found having in the womb from the Holy Spirit\tJisu Khrista laga jonom eneka pora hoise. Tai laga ama, Mary Joseph ke shadi koribole kotha milai loise, kintu tai duijon shadi nohua agete, Pobitro Atma dwara bacha pet te bukhi ja jani jaise.\" (offset in bytes: 3744) is not in the correct format. Skipping...\n",
            "Line \"Now Joseph her husband being righteous and not wanting to publicly disgrace her intended to divorce her secretly\tKintu Joseph, tai laga mota, ekjon dharmik manu asele aru sob manu agete taike bodnam nadibo nimite, tai lukaikena taike chari bole bhabona kori loise.\" (offset in bytes: 4009) is not in the correct format. Skipping...\n",
            "Line \"But he having reflected on these things behold an angel of the Lord appeared to him by way of a dream saying Joseph son of David you should not fear to take Mary as your wife because the one having been conceived in her is from the Holy Spirit\tJitia tai itu khan bhabi thakise, Probhu laga ekjon sorgoduth tai laga sapna te ahise, aru eneka koise, “Joseph, David laga chokra, Mary ke tumi laga maiki koribole bhoi nakoribi, kilekoile juntu ekjon bacha tai laga pet te bukhikena ase itu Pobitro Atma pora ase.\" (offset in bytes: 4520) is not in the correct format. Skipping...\n",
            "Line \"And she will bear a son and you will call his name Jesus for he will save his people from their sins\tTai ekjon chokra bacha ke jonom dibo, aru tumi tai laga naam Jisu rakhibo, kilemane Tai he Tailaga manukhan ke taikhan laga paap pora bachai lobo.”\" (offset in bytes: 4771) is not in the correct format. Skipping...\n",
            "Line \"Now all this happened so that might be fulfilled what was spoken by the Lord through the prophet saying the virgin will have in her womb and bear a son and they will call his name Immanuel which is translated God with us Joseph having been awakened from sleep did as the angel of the Lord commanded him and took her as his wife\tIsor pora bhabobadi khan dwara kowa kotha khan pura hobole karone itu khan sob tineka he hoi jaise. Eneka likha ase,\" (offset in bytes: 5216) is not in the correct format. Skipping...\n",
            "Line \"And he did not know her until she bore a son And he called his name Jesus\tKintu tailaga maiki pora bacha jonom nohua tak, tailaga usorte ja nai aru tai bacha laga naam Jisu matise.\" (offset in bytes: 5397) is not in the correct format. Skipping...\n",
            "Line \"Now Jesus having been born in Bethlehem of Judea in the days of Herod the king behold learned men from the east arrived in Jerusalem\tJitia Herod raja thakise, itu somoi te Judea laga Bethlehem te Jisu jonom hoise. Aru gyaan thaka manukhan jonom laga kotha huni kena Jerusalem te ahise,\" (offset in bytes: 5683) is not in the correct format. Skipping...\n",
            "Line \"saying Where is the one having been born King of the Jews For we saw his star in the east and came to worship him having heard this Herod the king became troubled and all Jerusalem with him\taru eneka hudise, “Yehudi khan laga Raja hobole jonom huwa to kot te ase? Kilekoile amikhan Tai laga tara purab te dikhise, aru amikhan Tai ke aradhana koribole ahise.”\" (offset in bytes: 6046) is not in the correct format. Skipping...\n",
            "Line \"And having brought together all the chief priests and scribes of the people he inquired from them Where is the Christ being born they said to him In Bethlehem of Judea for thus it has been written through the prophet you Bethlehem land of Judah are by no means least among the leaders of Judah for from you will come out a ruling one who will shepherd my people Israel\tItu pichete Herod raja pora mukhyo purohit aru kanun likha manukhan ke eke logote mati kena hudise, “Khrista to kun jagate jonom hobo?”\" (offset in bytes: 6555) is not in the correct format. Skipping...Line \"whose winnowing fork is } in his hand and he will thoroughly clear off his threshing floor and gather his wheat into the storehouse But he will burn up the chaff with unquenchable fire\tTai dhaan kati bole dao loikena ahi ase, sob dhaan khan ke dhaan ghor te joma korikena gash khan ke kitia bi khotom nohobole jui te phelai dibo.”\" (offset in bytes: 14575) is not in the correct format. Skipping...\n",
            "Line \"Then Jesus comes from Galilee to the Jordan to John to be baptized by him\tItu pichete Jisu Galilee pora ulaikena baptizma lobole Jordan Nodi te John logot ahise.\" (offset in bytes: 14737) is not in the correct format. Skipping...\n",
            "Line \"But John was hindering him saying I have need to be baptized by you and yet you come to me answering Jesus said to him Permit it now for in this way it is fitting for us to fulfill all righteousness Then he permits him\tHoilebi John to Jisu ke rukhabole bisi kosis korise, aru eneka koise, “Ami he Apni laga hath pora baptizma lobo lage, hoilebi Apni he moi logote ahise?”\" (offset in bytes: 15113) is not in the correct format. Skipping...\n",
            "Line \"Now having been baptized Jesus immediately came up from the water and behold the heavens were opened to him and he saw the Spirit of God coming down like a dove resting upon him\tJitia Jisu baptizma loikena pani pora ulaise, sorgo pora ekta pohor ahise aru Isor laga Atma kopu chiriya nisena koikena Tai uporte rukhi jaise.\" (offset in bytes: 15436) is not in the correct format. Skipping...\n",
            "Line \"and behold a voice from the heavens saying This is my beloved Son with whom I am well pleased\tItu pichete, sabi, ekta awaj sorgo pora koise, “Itu Ami laga bisi morom thaka Chokra ase. Ami Tai uporte bisi khushi ase.”\" (offset in bytes: 15657) is not in the correct format. Skipping...\n",
            "Line \"Then Jesus was led up by the Spirit into the wilderness to be tempted by the devil\tItu pichete Jisu ke Atma pora ekta sunsan jagate loi jaise juntu jagate saitan pora taike porikha koribo.\" (offset in bytes: 15846) is not in the correct format. Skipping...\n",
            "Line \"And having fasted 40 days and 40 nights afterward he was hungry\tAru chalis din aru chalis rati Jisu upwas korise, itu karone Tai bhuk lagi jaise.\" (offset in bytes: 15992) is not in the correct format. Skipping...\n",
            "Line \"And having approached the one tempting said to him If you are the Son of God speak so that these stones might become loaves\tTitia porikha kora jon ahikena Tai ke eneka koise, “Apni Isor laga Chokra ase koile, itu pathor khan ke roti hobole kobi.”\" (offset in bytes: 16243) is not in the correct format. Skipping...\n",
            "Line \"But he answering said It is written Man will not live on bread alone but by every word coming through the mouth of God\tKintu Jisu tai ke jowab dikena koise, “Pobitro kitab te eneka likhi kena ase, ‘Manu khali roti dwara jinda nathakibo, kintu ki kotha Isor laga mukh pora ulai itu pora he jinda thakibo.’”\" (offset in bytes: 16557) is not in the correct format. Skipping...\n",
            "Line \"Then the devil takes him into the holy city and he set him on the highest point of the temple\tItu pichete saitan pora Jisu ke pobitro sheher te loi jaise aru mondir laga sob pora uporte khara koribo dise,\" (offset in bytes: 16762) is not in the correct format. Skipping...\n",
            "Line \"and says to him If you are the Son of God throw yourself down for it is written will command his angels concerning you will lift you up in their hands you strike your foot against a stone\taru Tai ke koise, “Apni Isor laga Chokra ase koile, yate pora nijor ke nichete phelai dibi, kilekoile eneka likha ase,\" (offset in bytes: 17071) is not in the correct format. Skipping...\n",
            "Line \"Jesus said to him Again it is written You will not test the Lord your God\tJisu tai ke eneka koise, “Aru itu bi likhikena ase, ‘Apni laga Probhu Isor ke porikha nakoribi.’”\" (offset in bytes: 17251) is not in the correct format. Skipping...\n",
            "Line \"Again the devil takes him to a very high mountain and shows him all the kingdoms of the world and their glory\tAru, saitan pora Jisu ke untcha pahar te loi jaise aru Tai ke prithibi laga rajyokhan laga dangor mohima thaka dikhai dise.\" (offset in bytes: 17485) is not in the correct format. Skipping...\n",
            "Line \"And he said to him All these things I will give you if having fallen down you would worship me\tSaitan pora Tai ke koise, “Apni moike athukari kena aradhana korile, itu khan sob ami Apnike di dibo.”\" (offset in bytes: 17687) is not in the correct format. Skipping...\n",
            "Line \"Then Jesus says to him Go away Satan For it is written You will worship the Lord your God and you will serve only him\tTitia Jisu koise tai ke, “Yate pora jai jabi, Saitan! Kilekoile eneka likhi kena ase,\" (offset in bytes: 17893) is not in the correct format. Skipping...\n",
            "Line \"Then the devil leaves him and behold angels came and began to minister to him\tTitia saitan Taike chari kena jai jaise, aru sabi, sorgoduth khan ahi kena Tai ke modot korise.\" (offset in bytes: 18067) is not in the correct format. Skipping...\n",
            "Line \"Now having heard that John had been arrested he withdrew into Galilee\tJitia John ke dhurise koikena Jisu khobor paise, Tai Galilee te wapas jai jaise.\" (offset in bytes: 18218) is not in the correct format. Skipping...\n",
            "Line \"And having left Nazareth having come he lived in Capernaum by the sea in the territories of Zebulun and Naphtali\tTai Nazareth chari kena Galilee Samundar laga usorte thaka, Capernaum te jaise, itu jaga to Zebulun aru Naphtali laga majot te ase.\" (offset in bytes: 18463) is not in the correct format. Skipping...\n",
            "Line \"so that it might be fulfilled what was spoken through Isaiah the prophet saying land of Zebulun and the land of Naphtali the } way of the sea beyond the Jordan Galilee of the Gentiles people sitting in darkness have seen a great light and to the ones sitting in the region and shadow of death upon them has a light arisen\tIsaiah bhabobadi dwara Isor pora kowa kotha khan pura hobole karone itu khan sob eneka he hoi jaise,\" (offset in bytes: 18886) is not in the correct format. Skipping...\n",
            "Line \"From that time Jesus began to preach and to say Repent for the kingdom of the heavens has come near\tUtu somoi pora he Jisu prochar koribole shuru hoise aru koise, “Mon ghura bi, kilekoile sorgo laga rajyo to usor hoise.”\" (offset in bytes: 19111) is not in the correct format. Skipping...\n",
            "Line \"Now walking beside the Sea of Galilee he saw two brothers Simon called Peter and Andrew his brother casting a net into the sea for they were fishermen\tJitia Tai Galilee Samundar phale berai thakisele, Tai Simon jun ke Peter koi, aru tailaga bhai Andrew khan ke maas dhora jaal loikena samundar te phelai thaka dikhise, kilekoile taikhan maas dhora manu thakise.\" (offset in bytes: 19473) is not in the correct format. Skipping...\n",
            "Line \"And he says to them Come after me and I will make you fishers of men\tJisu taikhan ke koise, “Ahibi, Ami laga piche koribi, aru Moi tumi khan ke manu dhori bole bonai dibo.”\" (offset in bytes: 19650) is not in the correct format. Skipping...\n",
            "Line \"And immediately they having left the nets followed him\tLoge-loge taikhan laga maas dhura jaal khan phelai dise aru Jisu laga piche koribole shuru hoise.\" (offset in bytes: 19803) is not in the correct format. Skipping...\n",
            "Line \"And having gone on from there he saw two other brothers James the son of Zebedee and John his brother in the boat with Zebedee their father mending their nets and he called them\tJitia Jisu tate pora jai thakise, tai duijon bhai ke dikhise, James, Zebedee laga chokra, aru tai laga bhai John. Tai ekta naw te taikhan laga baba Zebedee logote maas dhura jaal khan milai thakise. Jisu taikhan ke mati se,\" (offset in bytes: 20205) is not in the correct format. Skipping...\n",
            "Line \"and immediately they having left the boat and their father followed him\taru taikhan loge-loge taikhan laga naw aru baba ke chari kena Jisu piche kori loise.\" (offset in bytes: 20362) is not in the correct format. Skipping...\n",
            "Line \"And he was going around in all of Galilee teaching in their synagogues and preaching the gospel of the kingdom and healing every disease and every sickness among the people\tTitia Jisu Galilee laga sob jagate berai jaise, mondoli khan te sikhai se, aru rajyo laga susamachar prochar korise, aru sob kisim laga ghao bemar aru bemar manukhan ke changai korise.\" (offset in bytes: 20720) is not in the correct format. Skipping...Line \"Now having seen the crowds he went up on the mountain and he having sat down his disciples came to him\tBhir bisi dangor hoi ja pora, Jisu to ekta untcha pahar te uthi jaise. Tai tate boha pichete, tailaga chela khan bi tai usorte ahise.\" (offset in bytes: 21670) is not in the correct format. Skipping...\n",
            "Line \"And having opened his mouth he taught them saying Blessed are the poor in spirit for theirs is the kingdom of the heavens the ones mourning for they will be comforted the meek for they will inherit the earth the ones hungering and thirsting for righteousness for they will be fed the merciful for they will obtain mercy the pure in heart for they will see God the peacemakers for they will be called sons of God the ones persecuted for the sake of righteousness for theirs is the kingdom of the heavens Blessed are you when they insult you and persecute you and say every evil thing against you lying because of me Rejoice and be very glad for your reward is } great in the heavens for in this way they persecuted the prophets before you\tAru Tai mukh khuli se aru taikhan ke sikhai se, eneka koikena,\" (offset in bytes: 22471) is not in the correct format. Skipping...\n",
            "Line \"You are the salt of the earth But if the salt is made tasteless with what might it be made salty again It is good for nothing any longer except having been thrown out to be trampled by men\tTumikhan itu prithibi laga nimok nisena ase. Kintu nimok laga swadh harai se koile, kineka tai laga swadh to arubi anibo? Itu to eku kaam laga nohoi jai, kintu itu ke phelai dibo aru manu khan pora chipai dibo.\" (offset in bytes: 22871) is not in the correct format. Skipping...\n",
            "Line \"You are the light of the world A city being set on top of a mountain is not able to be hidden\tTumi khan itu prithibi laga puhor ase. Ekta sheher ke pahar uporte bonaile lukabo napare.\" (offset in bytes: 23055) is not in the correct format. Skipping...\n",
            "Line \"Neither do they light a lamp and put it under a basket but rather on the lampstand and it shines for all the ones in the house\tNohoile bi manukhan tailaga saaki julai kena kiba ekta dabba pora bondh korikena narakhe, kintu itu saaki ke khamba te rakhikena pura ghor ke ujala koribole diye.\" (offset in bytes: 23345) is not in the correct format. Skipping...\n",
            "Line \"Let your light shine before men in such a way so that they might see your good deeds and might glorify your Father in the heavens\tTumi laga pohor ke eneka ujala kori dibi manu khan agete, titia taikhan tumi laga bhal kaam khan sob dikhi jabo aru tumi laga Baba jun sorgo te ase, Tai ke mohima dibo.\" (offset in bytes: 23644) is not in the correct format. Skipping...\n",
            "Line \"Do not think that I came to destroy the law or the prophets I came not to destroy but to fulfill\tEneka nabhabi Moi Isor laga niom aru Tai laga bhabobadi khan ke khotom kori dibo nimite ahise. Moi khotom kori dibole aha nohoi, kintu itu khan sob pura kori dibo nimite ahise.\" (offset in bytes: 23918) is not in the correct format. Skipping...\n",
            "Line \"For truly I say to you until the heaven and the earth passes away one jot or one tittle will certainly not pass away from the law until all things are accomplished\tKilekoile moi hosa pora tumi khan ke koi ase jitia tak prithibi aru akas khan khotom nohoi, ekta dagi nohoile ekta chutu kotha bi naharabo, jitia tak itu kotha khan sob pura nohoi.\" (offset in bytes: 24263) is not in the correct format. Skipping...\n",
            "Line \"Therefore whoever annuls one of the least of these commandments and teaches men to do so will be called least in the kingdom of the heavens But whoever does and teaches them that one will be called great in the kingdom of the heavens\tItu nimite, kun pora itu niom aru hukum khan bhangabo aru dusra khan ke bi eneka koribole sikhabo, taikhan laga naam bi sorgo laga rajyo pora hatai dibo. Kintu kun pora itu hukum aru niom sob rakhe, tai ke sorgo laga rajyo te mohan matibo.\" (offset in bytes: 24737) is not in the correct format. Skipping...\n",
            "Line \"For I say to you that unless your righteousness abounds more than that of the scribes and Pharisees you will certainly not enter into the kingdom of the heavens\tItu nimite Moi tumi khan ke koi ase, tumi khan laga dharmikta to niom likha khan aru pharisee khan pora bi bhal hobo lage, eneka nohoile tumi khan sorgo laga rajyo te ahibo naparibo.\" (offset in bytes: 25081) is not in the correct format. Skipping...\n",
            "Line \"You have heard that it was said to the ancient ones Do not kill and Whoever kills will be subject to the judgment\tTumi khan poila te hunise utu purana somoite eneka koi dise, ‘Morai diya kaam nakoribi,’ aru, ‘Jun pora manu ke morai taikhan Isor laga bisar te ahibo.’\" (offset in bytes: 25356) is not in the correct format. Skipping...\n",
            "Line \"But I say to you that everyone being angry with his brother will be subject to the judgment And whoever says to his brother Raka will be subject to the council And whoever says You fool will be subject to the Gehenna of fire\tKintu Moi tumi khan ke koi ase, sob manu jun khan tai nijor bhai ke gusa kore, tai logot Isor laga bisar ahibo, aru nijor bhai ke eneka kowa khan to sabha age te bisi digdar hobo, ‘Tumi kaam nathaka manu!’ Aru tai norok laga jui te juli bo jodi ekjon pora tai bhai ke eneka koi, ‘Tumi murkho manu!’\" (offset in bytes: 25888) is not in the correct format. Skipping...\n",
            "Line \"Therefore if you offer your gift at the altar and there you remember that your brother has something against you\tItu karone, jitia tumi Isor laga agete daan dibole ane, aru bhai ke kiba biya korise koikena mon te ahise koile,\" (offset in bytes: 26114) is not in the correct format. Skipping...\n",
            "Line \"leave your gift there in front of the altar and go first be reconciled with your brother and then having come offer your gift\ttumi laga dhun khan bedi usorte chari dibi. Aru poila jaikena tumi laga bhai logote mon milai lobi, itu pichete ahikena Isor ke daan dibi.\" (offset in bytes: 26379) is not in the correct format. Skipping...\n",
            "Line \"Be in agreement with your accuser quickly while you are with him on the way lest your accuser might hand you over to the judge and the judge to the officer and you will be thrown into prison\tKunba tumike bisar ghor te loi jabo koi ase koile, na ja-a agete tai logote kotha milai lobi, kilekoile ekbar bisar ghor te jaise koile tai pora tumike ukil laga hath te di dibo, aru ukil pora tumike sipahi khan logote di dibo aru sipahi khan pora tumi ke bondhi ghor te rakhi dibo.\" (offset in bytes: 26853) is not in the correct format. Skipping...\n",
            "Line \"Truly I say to you you will certainly not come out from there until you have paid the last quadrans\tHosa pora Ami tumikhan ke koi di ase, tumi bondhi ghor pora bahar naulabo jitia tak tumi laga dhar sob tirai nadibo.\" (offset in bytes: 27070) is not in the correct format. Skipping...\n",
            "Line \"You have heard that it was said Do not commit adultery\tTumikhan eneka kowa hunise, ‘Bebichari nakoribi.’\" (offset in bytes: 27179) is not in the correct format. Skipping...\n",
            "Line \"But I say to you that everyone looking at a woman to lust after her has already committed adultery with her in his heart\tKintu Moi tumikhan ke koi di ase, jun manu ekjon mahila ke biya kaam koribole bhabi loi, tai mon bhitor te bebichar kori loi.\" (offset in bytes: 27426) is not in the correct format. Skipping...\n",
            "Line \"But if your right eye causes you to stumble pluck it out and throw it away from you For it is better for you that one of your members would perish and your whole body would not be thrown into Gehenna\tJitia ekta suku pora tumi ke biya kaam koribo di ase, utu suku ke ulai kena dhur te phelai dibi. Kilekoile gaw te ekta biya saman nimite pura gaw norok te ja pora to, ekta suku nathaki kena jinda thaka he bhal hobo.\" (offset in bytes: 27842) is not in the correct format. Skipping...\n",
            "Line \"And if your right hand causes you to stumble cut it off and throw it away from you For it is better for you that one of your members would perish and your whole body would not go into Gehenna\tTumi laga ekta hath nimite paap kaam kori ase koile, itu ke kati kena phelai dibi. Kilekoile pura gaw norok te ja pora, ekta hath nathaki kena jinda thakile he bhal ase.\" (offset in bytes: 28204) is not in the correct format. Skipping...Line \"And having seen the star they rejoiced with very great joy\tJitia taikhan tara ke dikhi se, taikhan bisi khushi hoise aru khushi pora rukhibo napara hoise.\" (offset in bytes: 7487) is not in the correct format. Skipping...\n",
            "Line \"And having gone into the house they saw the young child with Mary his mother And having fallen down they worshiped him and having opened their treasures they offered him gifts gold and frankincense and myrrh\tTitia taikhan ghor bhitorte ghusi se chutu bacha ke Tailaga ama Mary pora dhuri thaka dikhise. Taikhan athukari kena Tai ke aradhana korise. Taikhan khushi pora taikhan laga khajana khan khuli kena suna, aru dhuna, aru bhal gundh laga tel khan daan korise.\" (offset in bytes: 7952) is not in the correct format. Skipping...\n",
            "Line \"And having been warned through a dream not to return to Herod they departed to their country by another way\tKintu Isor pora taikhan laga sapna te Herod logote wapas najabi koikena hoshiar kori dise, itu nimite dusra rasta pora tai khan nijor desh te wapas jai jaise.\" (offset in bytes: 8219) is not in the correct format. Skipping...\n",
            "Line \"Now they having departed behold an angel of the Lord appears to Joseph by means of a dream saying Having gotten up take the young child and his mother and flee to Egypt and remain there until I tell you for Herod is going to seek the young child to kill him\tKintu taikhan jai ja-a pichete, Isor laga sorgoduth sapna te ahikena Joseph ke koise, “Uthibi, bacha aru ama ke loikena Egypt te polai jabi aru ami nokuwa tak tate thakibi, kilekoile Herod Raja pora itu bacha ke morabole bisaribole ase.”\" (offset in bytes: 8719) is not in the correct format. Skipping...\n",
            "Line \"And having gotten up he took the young child and his mother at night and departed for Egypt\tTitia utu rati te, Joseph uthikena bacha aru ama ke loikena Egypt te polai jaise.\" (offset in bytes: 8893) is not in the correct format. Skipping...\n",
            "Line \"and he was there until the death of Herod in order that might be fulfilled what was spoken by the Lord through the prophet saying of Egypt I called my son\tTai Herod namora tak tate he thakise. Juntu kotha Isor laga bhabobadi pora koi disele, itu pura hoi jaise, “Egypt pora Ami laga chokra ke mati loise.”\" (offset in bytes: 9203) is not in the correct format. Skipping...\n",
            "Line \"Then Herod having seen that he had been mocked by the learned men was very angry and having sent forth he killed all the male children in Bethlehem and in all its region from two years and under according to the time that he had determined exactly from the learned men\tJitia Herod gyaani manukhan pora taike thogaise koikena janise, tai bisi khong uthise. Itu nimite tailaga sipahi khan ke pathaikena jiman dui saal pora niche mota bacha Bethlehem te ase, sob ke morai dibole hukum dise. Kile koile utu somoi te Jisu jonom koikena dui saal ase koikena Herod khobor paikena thakise.\" (offset in bytes: 9785) is not in the correct format. Skipping...\n",
            "Line \"Then it was fulfilled what was spoken through Jeremiah the prophet saying voice was heard in Ramah weeping and great mourning Rachel weeping for her children and not willing to be comforted because they are no more\tBhabobadi Jeremiah pora ki kotha koi dise itu khan sob pura hoi jaise, juntu eneka likha asele,\" (offset in bytes: 10096) is not in the correct format. Skipping...\n",
            "Line \"Now Herod having died behold an angel of the Lord appears by means of a dream to Joseph in Egypt\tAru jitia Herod mori jaise, sabi, Isor laga ekjon sorgoduth, Joseph to Egypt te thaka somoite tai laga sapna te ahise,\" (offset in bytes: 10312) is not in the correct format. Skipping...\n",
            "Line \"saying Having gotten up take the young child and his mother and go to the land of Israel for the ones seeking the life of the child have died\taru eneka koise, “Uthikena bacha aru tai laga ama ke loikena Israel desh te jabi, kilekoile jun manu bacha ke morabole thakise taikhan mori se.”\" (offset in bytes: 10603) is not in the correct format. Skipping...\n",
            "Line \"And he having gotten up took the young child and his mother and entered into the land of Israel\tItu pichete Joseph uthikena, bacha aru ama ke loi jaise, aru Israel laga desh te jaise.\" (offset in bytes: 10787) is not in the correct format. Skipping...\n",
            "Line \"But having heard that Archelaus is reigning over Judea in the place of his father Herod he was afraid to go there But having been warned by means of a dream he withdrew to the region of Galilee\tKintu jitia tai hunise Herod laga chokra Archelaus Judea laga Raja ase, tai tate jabole bhoi hoi jaise. Hoilebi Isor pora tailaga sapna te hoshiar kori dise, aru itu jaga chari kena, tai Galilee te jaise.\" (offset in bytes: 11186) is not in the correct format. Skipping...\n",
            "Line \"and having gone he lived in a city being called Nazareth so that it might be fulfilled what was spoken through the prophets that he will be called a Nazarene\tAru Nazareth laga sheher te jaise aru tate thaki jaise. Eneka pora, juntu bhabobadi khan pora Isor laga kotha koi dise itu pura hoise, juntu asele, Tai ke Nazareth laga manu koi kena matibo.\" (offset in bytes: 11535) is not in the correct format. Skipping...\n",
            "Line \"Now in those days John the Baptist comes preaching in the wilderness of Judea\tEtiya utu somoite Baptizma diya John ahise aru Isor laga kotha prochar korikena Judea laga sunsan jaga te ahise,\" (offset in bytes: 11726) is not in the correct format. Skipping...\n",
            "Line \"saying Repent for the kingdom of the heavens is near\taru eneka koise, “Mon ghura bi, kile koile sorgo laga rajyo to usor te ahi jaise.”\" (offset in bytes: 11866) is not in the correct format. Skipping...\n",
            "Line \"For this is the one spoken of through Isaiah the prophet saying A voice of one calling out in the wilderness Make ready the way of the Lord make his paths straight\tKile koile bhabobadi Isaiah pora tailaga he kotha koi diya asele, eneka likha ase,\" (offset in bytes: 12113) is not in the correct format. Skipping...\n",
            "Line \"Now this John had his clothing from the hair of a camel and a leather belt around his waist and his food was locusts and wild honey\tEtiya, John to uth janwar laga chuli pora bona kapra lagai aru komor te chamra laga peti pora bandhe. Aru tai pango aru jongol laga mou pani he khai.\" (offset in bytes: 12395) is not in the correct format. Skipping...\n",
            "Line \"Then were going out to him Jerusalem and all Judea and all the region around the Jordan\tTitia Jerusalem, aru pura Judea desh, aru Jordan Nodi laga sob jaga pora manukhan taike lok kori bole ahe.\" (offset in bytes: 12590) is not in the correct format. Skipping...\n",
            "Line \"and being baptized by him in the Jordan River confessing their sins\tTaikhan laga paap shikar kora pichete, John pora Jordan Nodi te taikhan ke baptizma di thakise.\" (offset in bytes: 12754) is not in the correct format. Skipping...\n",
            "Line \"Now having seen many of the Pharisees and Sadducees coming for his baptism he said to them Offspring of vipers Who warned you to flee from the coming wrath\tKintu tai logot bisi Pharisee aru Sadducee khan baptizma lobole ahi thaka dikhise, aru tai taikhan ke koise, “Tumikhan saph laga bacha khan, kun koise tumikhan Isor laga khong pora polabo karone?\" (offset in bytes: 13108) is not in the correct format. Skipping...\n",
            "Line \"Therefore produce fruit worthy of repentance\tItu nimite, mon ghura laga bijon apnikhan laga kaam pora dikhai dibi.\" (offset in bytes: 13223) is not in the correct format. Skipping...\n",
            "Line \"And you should not think to say in yourselves We have Abraham as } father For I say to you that God is able to raise up children for Abraham from these stones\tApnikhan laga mon te ‘Amikhan laga baba Abraham ase’ eneka nabhabi bi. Moi tumikhan ke koi ase Isor to itu pathor khan loikena Abraham laga khandan bonabo pare.\" (offset in bytes: 13547) is not in the correct format. Skipping...\n",
            "Line \"But already the ax is set against the root of the trees So every tree not producing good fruit is chopped down and thrown into a fire\tKintu sob gash khan kati bole kuthar to age pora taiyar hoikena ase. Juntu gash bhal phol nadhure itu gash ke katikena jui te phelai dibo.\" (offset in bytes: 13820) is not in the correct format. Skipping...\n",
            "Line \"I indeed baptize you with water for repentance But the one coming after me is more powerful than I of whom I am not worthy to carry away his sandals He will baptize you with the Holy Spirit and with fire\tMoi to tumikhan ke pani pora he baptizma dibo. Kintu Jun ami laga pichete ahi ase, Tai to ami pora bi hokti thaka ase, Tai laga chapal moi dhori bole layak nai. Tai tumikhan ke Pobitro Atma aru jui pora baptizma dibo.\" (offset in bytes: 14242) is not in the correct format. Skipping...\n",
            "Line \"And the news of him went out into all Syria and they brought to him all the ones having sickness being afflicted by various diseases and pains and being possessed by demons and the epileptic and the paralytic and he healed them\tTailaga khobor sob Syria desh te punchi jaise, aru manukhan Jisu usorte kisim-kisim bemar pora dukh pai thaka khan, gaw bhitor te bhoot huma khan, murgi-bemar thaka khan, aru jothor-rugi bemar thaka khan sob ke loikena anise. Aru taikhan ke Jisu pora changai kori dise.\" (offset in bytes: 21218) is not in the correct format. Skipping...\n",
            "Line \"And large crowds followed him from Galilee and Decapolis and Jerusalem and Judea and beyond the Jordan\tAru Galilee, Decapolis, Jerusalem, Judea, aru Jordan pora bi dur thaka sob jaga pora manukhan Jisu logot ahise.\" (offset in bytes: 21433) is not in the correct format. Skipping...\n",
            "\n",
            "Line \"Now it has been said Whoever divorces his wife let him give her a certificate of divorce\tEneka bi koikena ase, ‘Jun mota tai laga maiki ke chari bole mon ase, tai pora chari diya laga chithi likhikena tailaga maiki ke dibole lage.’\" (offset in bytes: 28440) is not in the correct format. Skipping...\n",
            "\n",
            "\n",
            "Line \"Then Herod having secretly called the learned men inquired from them the time of the appearing of the star\tItu pichete, Herod pora lukaikena gyaani manukhan ke kotha korise aru tara ki homoi te ulaise itu bhal pora jani loise.\" (offset in bytes: 6782) is not in the correct format. Skipping...\n",
            "Line \"And having sent them to Bethlehem he said Having gone search carefully for the young child and after you have found him report to me so that I also having come might worship him they having heard the king went and behold the star that they saw in the east was going before them until having come it stood over where the young child was\tItu pichete tai gyaani manukhan ke Bethlehem te eneka koikena pathaise, “Jabi aru bhal pora itu bacha ke bisari bi. Kitia apnikhan taike pabo, amike kobi, kilekoile Moi bi tate jaikena Tai ke aradhana koribo.”\" (offset in bytes: 7332) is not in the correct format. Skipping...\n",
            "\n",
            "--- Alignment Complete. Results saved to 'alignments_sample.txt' ---\n",
            "\n",
            "--- Sample of Generated Alignments ---\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# file: main.py\n",
        "\n",
        "import argparse\n",
        "from transformer_tagger import train_transformer_tagger, NagamesePosTagger\n",
        "from nltk_tagger import train_and_save_nltk_tagger, NltkPosTagger\n",
        "from nmt_translator import Translator as NMT_Translator\n",
        "# You can add imports for the subword tokenizer and word aligner here if you want to control them from the CLI\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description=\"Nagamese NLP Toolkit Command-Line Interface\")\n",
        "    subparsers = parser.add_subparsers(dest='command', required=True, help='Available commands')\n",
        "\n",
        "    # --- Command to train the Transformer POS Tagger ---\n",
        "    parser_train_tagger = subparsers.add_parser('train-tagger', help='Train the Transformer POS Tagger.')\n",
        "    parser_train_tagger.add_argument('--conll_file', type=str, default='nagamese_manual_enriched.conll', help='Path to the CoNLL training file.')\n",
        "    parser_train_tagger.add_argument('--model_dir', type=str, default='nagamese_pos_model', help='Directory to save the trained model.')\n",
        "\n",
        "    # --- Command to tag text using the Transformer model ---\n",
        "    parser_tag = subparsers.add_parser('tag', help='Tag a sentence using the trained Transformer POS model.')\n",
        "    parser_tag.add_argument('text', type=str, help='The Nagamese sentence to tag.')\n",
        "    parser_tag.add_argument('--model_dir', type=str, default='nagamese_pos_model', help='Directory of the trained model.')\n",
        "\n",
        "    # --- Command to train the NLTK Tagger ---\n",
        "    parser_train_nltk = subparsers.add_parser('train-nltk-tagger', help='Train the NLTK POS Tagger.')\n",
        "    parser_train_nltk.add_argument('--conll_file', type=str, default='nagamese_manual_enriched.conll', help='Path to the CoNLL training file.')\n",
        "    parser_train_nltk.add_argument('--model_path', type=str, default='nagamese_nltk_tagger.pkl', help='Path to save the pickled NLTK tagger.')\n",
        "\n",
        "    # --- Command to translate text using the NMT model ---\n",
        "    parser_translate = subparsers.add_parser('translate', help='Translate a Nagamese sentence to English.')\n",
        "    parser_translate.add_argument('text', type=str, help='The Nagamese sentence to translate.')\n",
        "    parser_translate.add_argument('--model_path', type=str, default='nmt-nagamese-english.pt', help='Path to the trained NMT model state dictionary.')\n",
        "    parser_translate.add_argument('--vocabs_path', type=str, default='nmt-vocabs.pkl', help='Path to the pickled vocab file.')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # --- Execute the chosen command ---\n",
        "    if args.command == 'train-tagger':\n",
        "        print(f\"Starting Transformer POS tagger training...\")\n",
        "        train_transformer_tagger(args.conll_file, args.model_dir)\n",
        "        print(\"Training complete.\")\n",
        "\n",
        "    elif args.command == 'tag':\n",
        "        try:\n",
        "            tagger = NagamesePosTagger(args.model_dir)\n",
        "            tags = tagger.predict(args.text)\n",
        "            print(tags)\n",
        "        except OSError as e:\n",
        "            print(f\"Error: {e}. Have you trained the model first with the 'train-tagger' command?\")\n",
        "\n",
        "    elif args.command == 'train-nltk-tagger':\n",
        "        print(\"Starting NLTK POS tagger training...\")\n",
        "        train_and_save_nltk_tagger(args.conll_file, args.model_path)\n",
        "        print(\"Training complete.\")\n",
        "\n",
        "    elif args.command == 'translate':\n",
        "        import torch\n",
        "        try:\n",
        "            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "            translator = NMT_Translator(args.model_path, args.vocabs_path, device)\n",
        "            translation = translator.translate(args.text)\n",
        "            print(f\"Translation: {translation}\")\n",
        "        except (OSError, FileNotFoundError) as e:\n",
        "            print(f\"Error: {e}. Ensure the NMT model and vocabs exist by running the `nmt_translator.py` script first.\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "PcCfdEIp96vY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}