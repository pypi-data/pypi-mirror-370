{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !uv pip install h3pandas duckdb h3 pyarrow pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking\n",
    "\n",
    "We install a couple libraries. \n",
    "- I ran this locally on my 16 core M3 Max Macbook Pro. \n",
    "- If you know how to make any of these other libraries more performant, please open a PR. I want to be as fair as possible.  \n",
    "- I'm not an expert in DuckDB, but copying the data should be 0 cost due to Apache Arrow?\n",
    "- I used `h3==4.1.2`, `polars==1.8.2` and `duckdb==1.1.3`. \n",
    "\n",
    "To run the benchmarks you can either run it with your cli by running `python -m benchmarks.main` or you can run it in this notebook with the below cell.\n",
    "\n",
    "**The benchmarks aims to cover most of the common operations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "import statistics\n",
    "\n",
    "from .. import benchmarks \n",
    "\n",
    "param_config = benchmarks.ParamConfig(\n",
    "    resolution=9,\n",
    "    grid_ring_distance=3,\n",
    "    num_iterations=3,\n",
    "    libraries=[\"plh3\", \"duckdb\", \"h3_py\"],\n",
    "    difficulty_to_num_rows={\n",
    "        \"basic\": 10_000_000,\n",
    "        \"medium\": 10_000_000,\n",
    "        \"complex\": 100_000,\n",
    "    },\n",
    "    # functions=[\"grid_path\"],\n",
    "    # verbose=True,\n",
    ")\n",
    "benchmark_engine = benchmarks.Benchmark(config=param_config)\n",
    "raw_results = benchmark_engine.run_all()\n",
    "prev_func = None\n",
    "for result in raw_results:\n",
    "    if prev_func != result.name:\n",
    "        print(f\"\\n{result.name}\")\n",
    "        prev_func = result.name\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# of if you used the cli\n",
    "# import json\n",
    "\n",
    "# with open(\"../benchmarks/benchmarks-results.json\", \"r\") as f:\n",
    "#     raw_results = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "| Function            | polars-h3 (Time)   | duckdb (Time) | h3_py (Time)   |\n",
    "|---------------------|---------------|---------------|---------------|\n",
    "| latlng_to_cell (10M)      | 0.19s   | 3.62s   | 6.89s   |\n",
    "| cell_to_latlng (10M)      | 1.42s   | 2.96s   | 36.30s  |\n",
    "| get_resolution (10M)      | 0.28s   | 0.18s   | 2.10s   |\n",
    "| int_hex_to_str (10M)      | 0.36s   | 1.66s   | 2.35s   |\n",
    "| str_hex_to_int (10M)      | 0.13s   | 1.26s   | 2.05s   |\n",
    "| is_valid_cell (10M)       | 0.03s   | 0.25s   | 2.09s   |\n",
    "| are_neighbor_cells (10M)  | 0.16s   | 0.77s   | 4.20s   |\n",
    "| cell_to_parent (10M)      | 0.05s   | 0.18s   | 3.66s   |\n",
    "| cell_to_children (10M)    | 2.23s   | 3.20s   | 62.38s  |\n",
    "| grid_disk (10M)           | 3.86s   | 13.50s  | 140.69s |\n",
    "| grid_ring (10M)           | 3.18s   | 7.67s   | 90.83s  |\n",
    "| grid_distance (10M)      | 0.16s   | 1.61s   | 5.23s   |\n",
    "| cell_to_boundary (10M)    | 3.96s   | 39.13s  | 186.57s |\n",
    "| grid_path (100K)           | 0.74s   | 13.14s  | 28.31s  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Multipliers\n",
    "\n",
    "In this section, we take the raw benchmark results and group them by the function name.\n",
    "Then, for each function, we identify the fastest execution time across all libraries\n",
    "and compute how many times slower the other libraries are in comparison to that fastest time.\n",
    "\n",
    "After we compute these \"multiples\" (speed factors) for each library across all functions,\n",
    "we then summarize the data by calculating the median and average multiples per library.\n",
    "This helps us understand, on average, how much slower each library is compared to the fastest one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median:\n",
      "{'plh3': 1.0, 'duckdb': 4.69, 'h3_py': 30.93}\n",
      "Average:\n",
      "{'plh3': 1.04, 'duckdb': 6.97, 'h3_py': 33.55}\n"
     ]
    }
   ],
   "source": [
    "by_name = defaultdict(list)\n",
    "for d in raw_results:\n",
    "    by_name[d[\"name\"]].append(d)\n",
    "\n",
    "multiples = []\n",
    "for speeds in by_name.values():\n",
    "    fastest = min(v[\"seconds\"] for v in speeds)\n",
    "    for v in speeds:\n",
    "        multiples.append((v[\"library\"], v[\"seconds\"] / fastest))\n",
    "\n",
    "by_lib = defaultdict(list)\n",
    "for lib, mult in multiples:\n",
    "    by_lib[lib].append(mult)\n",
    "\n",
    "median_by_lib = {lib: round(statistics.median(ms), 2) for lib, ms in by_lib.items()}\n",
    "avg_by_lib = {lib: round(sum(ms) / len(ms), 2) for lib, ms in by_lib.items()}\n",
    "print(\"Median:\")\n",
    "print(median_by_lib)\n",
    "print(\"Average:\")\n",
    "print(avg_by_lib)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
