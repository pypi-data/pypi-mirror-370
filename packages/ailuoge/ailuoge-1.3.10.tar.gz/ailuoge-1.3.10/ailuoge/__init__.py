import hashlib
import json
import math
import openpyxl
import os
import pymssql
import pymysql
import re
import requests
import sqlalchemy
import time
import zipfile

import pandas as pd
import numpy as np

from openpyxl.utils import get_column_letter
from tqdm import tqdm


class GetEcData:

    def __init__(self, user_name, user_token):
        self.wsdl = 'http://openapi-web.eccang.com/openApi/api/unity'
        self.headers = {
            'ContentType': 'application/json'
        }

        self.user_name = user_name
        self.user_token = user_token

    def __concat_params(self, biz_content: dict, interface_method: str):
        """
        å¤„ç†post data, ç”Ÿæˆkey
        :param biz_content:  è¯¦ç»†è°ƒç”¨å‚æ•°
        :param interface_method:  æ˜“ä»“çš„ä¿¡æ¯ getStockOrderList
        :return:
        """
        post_data = {
            "app_key": self.user_name,
            "biz_content": json.dumps(biz_content),
            "charset": "UTF-8",
            "interface_method": interface_method,
            "nonce_str": "113456",
            "service_id": "E7HPYV",
            "sign_type": "MD5",
            "timestamp": int(time.time() * 1000),
            "version": "v1.0.0"
        }

        # å°†å­—å…¸è½¬åŒ–ä¸ºæ˜“ä»“éœ€è¦çš„åŠ å¯†å½¢å¼
        post_data_str = ''
        for one_key, one_value in zip(post_data.keys(), post_data.values()):
            if type(one_value) == dict:
                one_value = json.dumps(one_value).replace(': ', ':')
            post_data_str += one_key
            post_data_str += '='
            post_data_str += str(one_value)
            post_data_str += '&'

        post_data_str = post_data_str[:-1]
        post_data_str += self.user_token

        # å¯¹ç»„åˆåçš„ä¿¡æ¯è¿›è¡ŒåŠ å¯†md5
        post_data['sign'] = hashlib.md5(bytes(post_data_str, encoding='utf-8')).hexdigest()

        return post_data

    def __get_data(self, biz_content: dict, interface_method: str, key_word: str = 'data'):
        """
        è·å–å•é¡µæ•°æ®ï¼ŒæŠŠä¼ å…¥çš„å‚æ•°è½¬æˆjsonæ ¼å¼ï¼Œå‘apiè¯·æ±‚ï¼Œæå–response.texté‡Œçš„dataæ•°æ®
        :param biz_content:
        :param interface_method:
        :param key_word: è¦è¿”å›çš„å…³é”®è¯
        :return:
        """
        concated_params = self.__concat_params(biz_content, interface_method)

        # è·å–response
        res = requests.post(self.wsdl, json=concated_params, headers=self.headers)
        # æŠŠresponseçš„textçš„jsonæ ¼å¼è½¬æ¢æˆå­—å…¸æ ¼å¼
        try:
            page_info = json.loads(res.text)
            # æ‰“å°å¼‚å¸¸ä¿¡æ¯
            if page_info['message'] not in ['Success', 'ok']:
                print(page_info['message'])  
                print(f'å½“å‰è¯·æ±‚çš„biz_contentï¼š{biz_content}')
                print(f'å½“å‰è¯·æ±‚çš„interface_methodï¼š{interface_method}')
        except:
            page_info = {'message': 'ç³»ç»Ÿå¼‚å¸¸'}
            print(res.text)

        # åˆ¤æ–­æ˜¯å¦è¶…æ—¶
        try:
            # æ ¹æ®ä¼ å…¥çš„é”®è¿”å›å€¼
            page_data = json.loads(page_info.get('biz_content')).get(key_word)
        except:
            print('ç³»ç»Ÿå¼‚å¸¸ï¼Œæ˜“ä»“å¯èƒ½è¶…æ—¶')
            print(f'å½“å‰è¯·æ±‚çš„biz_contentï¼š{biz_content}')
            print(f'å½“å‰è¯·æ±‚çš„interface_methodï¼š{interface_method}')
            print(page_info)
            page_data = ''

        return page_data

    def get_data(self, biz_content: dict, interface_method: str, special_param: str = None):
        """
        https://open.eccang.com/#/documentCenter?docId=1287&catId=0-225-225,0-177
        è·å–è¯·æ±‚çš„æ•°æ®
        :param biz_content:
        :param interface_method:
        :param special_param: ç‰¹æ®Šå‚æ•°ï¼Œä¼ å…¥è¯¥å‚æ•°åä¸ä¼šå°è¯•è·å–æ•°æ®çš„æœ€å¤§è¡Œæ•°ï¼Œè€Œæ˜¯éå†biz_contentä¸­çš„è¯¥å‚æ•°åˆ—è¡¨
        :return:
        """
        # 0 å‚æ•°è®¾ç½®
        # é»˜è®¤é¡µæ•°
        if not biz_content.get('page_size'):
            biz_content['page_size'] = 20

        list_df = []
        if not special_param:
            # 1 è·å–æœ€å¤§é¡µæ•°
            record_rows = self.__get_data(biz_content, interface_method, 'total_count')
            if not record_rows:
                record_rows = self.__get_data(biz_content, interface_method, 'total')
            # å‘ä¸Šå–æ•´
            max_page = math.ceil(int(record_rows) / biz_content.get('page_size'))

            # 2 æŒ‰é¡µè·å–æ•°æ®
            print('æŒ‰é¡µè·å–æ•°æ®')
            for i in tqdm(range(1, max_page + 1)):
                time.sleep(5)  # æ˜“ä»“é™åˆ¶äº†è¯·æ±‚é¢‘ç‡ï¼Œåªèƒ½è‡ªå·±å‡å°‘äº†
                # 2.1 è°ƒæ•´é”®å€¼å¯¹
                biz_content['page'] = i
                # 2.2 è·å–å¯¹åº”é¡µæ•°çš„æ•°æ®
                pg_data = self.__get_data(biz_content, interface_method)
                if pg_data:
                    list_df.append(pd.DataFrame(pg_data))
        else:
            list_param = biz_content[special_param]  # å‚æ•°åˆ—è¡¨ï¼Œæ¯”å¦‚è®¢å•å·
            lens = len(list_param)
            # 1 éå†special_paramï¼Œæ¯æ¬¡1ä¸ª
            print(f'æ ¹æ®{special_param}ï¼Œæ¯æ¬¡è·å–1ä¸ªæ•°æ®')
            for i in tqdm(range(0, lens, 1)):
                # 2.2 è·å–å¯¹åº”é¡µæ•°çš„æ•°æ®
                biz_content[special_param] = list_param[i: i + 1]
                pg_data = self.__get_data(biz_content, interface_method)
                if pg_data:
                    list_df.append(pd.DataFrame(pg_data))

        return list_df


class Mysql:

    def __init__(self, host, user, password, database):
        self.conn = pymysql.connect(host=host, user=user, password=password, database=database, charset='utf8')
        self.cur = self.conn.cursor()

    def exec_query(self, sql):
        self.cur.execute(sql)
        self.conn.commit()
        result = self.cur.fetchall()
        for row in result:
            print(row)

    def close(self):
        self.conn.close()


def any_files(folder_path: str) -> bool:
    """
    ç»™æ–‡ä»¶åœ°å€ï¼Œåˆ¤æ–­é‡Œé¢æœ‰æ²¡æœ‰æ–‡ä»¶ï¼Œæœ‰çš„è¯è¿”å›Trueï¼Œåä¹‹False
    :param folder_path:
    :return:
    """
    for root, dirs, files in os.walk(folder_path):
        if files:
            return True
    return False


def excel_process(file_path):
    """
    è°ƒæ•´åˆ—å®½ï¼Œå†»ç»“é¦–è¡Œï¼Œæ·»åŠ ç­›é€‰
    freeze title, adjust width of columns, open filter
    :param file_path: path of file
    :return:
    """
    print('è°ƒæ•´åˆ—å®½ï¼Œå†»ç»“é¦–è¡Œï¼Œæ·»åŠ ç­›é€‰')
    # ä¿®æ”¹ä¸‹è¿°å‚æ•°å³å¯ä½¿ç”¨ï¼ŒExcelåç§°åŠSheetåç§°å³å¯
    work_book = openpyxl.load_workbook(file_path)
    for sheet in work_book.sheetnames:
        work_book[sheet].freeze_panes = 'A2'
        work_sheet = work_book[sheet]
        # è®¾ç½®ä¸€ä¸ªå­—å…¸ç”¨äºä¿å­˜åˆ—å®½æ•°æ®
        dim_cols = {}
        # éå†è¡¨æ ¼æ•°æ®ï¼Œè·å–è‡ªé€‚åº”åˆ—å®½æ•°æ®
        for row in work_sheet.rows:
            for cell in row:
                if cell.value:
                    # éå†æ•´ä¸ªè¡¨æ ¼ï¼ŒæŠŠè¯¥åˆ—æ‰€æœ‰çš„å•å…ƒæ ¼æ–‡æœ¬è¿›è¡Œé•¿åº¦å¯¹æ¯”ï¼Œæ‰¾å‡ºæœ€é•¿çš„å•å…ƒæ ¼
                    # åœ¨å¯¹æ¯”å•å…ƒæ ¼æ–‡æœ¬æ—¶éœ€è¦å°†ä¸­æ–‡å­—ç¬¦è¯†åˆ«ä¸º1.7ä¸ªé•¿åº¦ï¼Œè‹±æ–‡å­—ç¬¦è¯†åˆ«ä¸º1ä¸ªï¼Œè¿™é‡Œåªéœ€è¦å°†æ–‡æœ¬é•¿åº¦ç›´æ¥åŠ ä¸Šä¸­æ–‡å­—ç¬¦æ•°é‡å³å¯
                    # re.findall('([\u4e00-\u9fa5])', cell.value)èƒ½å¤Ÿè¯†åˆ«å¤§éƒ¨åˆ†ä¸­æ–‡å­—ç¬¦
                    cell_len = 0.5 * len(re.findall('([\u4e00-\u9fa5])', str(cell.value))) + len(str(cell.value))
                    dim_cols[cell.column] = max((dim_cols.get(cell.column, 0), cell_len))
        for col, value in dim_cols.items():
            # è®¾ç½®åˆ—å®½ï¼Œget_column_letterç”¨äºè·å–æ•°å­—åˆ—å·å¯¹åº”çš„å­—æ¯åˆ—å·ï¼Œæœ€åå€¼+2æ˜¯ç”¨æ¥è°ƒæ•´æœ€ç»ˆæ•ˆæœçš„ï¼Œé™åˆ¶æœ€å°å®½åº¦10ï¼Œ æœ€å¤§å®½åº¦ä¸º30
            if value > 28:
                work_sheet.column_dim_colensions[get_column_letter(col)].width = 30
            elif value < 8:
                work_sheet.column_dim_colensions[get_column_letter(col)].width = 10
            else:
                work_sheet.column_dim_colensions[get_column_letter(col)].width = value + 2
        dict_num_to_alphabet = {
            1: 'A', 2: 'B', 3: 'C', 4: 'D', 5: 'E', 6: 'F', 7: 'G', 8: 'H', 9: 'I', 10: 'J',
            11: 'K', 12: 'L', 13: 'M', 14: 'N', 15: 'O', 16: 'P', 17: 'Q', 18: 'R', 19: 'S', 20: 'T',
            21: 'U', 22: 'V', 23: 'W', 24: 'X', 25: 'Y', 26: 'Z',
        }
        # è·å–ç¬¬ä¸€è¡Œ
        rows = work_sheet.iter_rows(max_row=1, values_only=True)
        max_col = 0
        for cell in rows:
            max_col = len(cell)
        # å¯¹ç¬¬ä¸€è¡Œæ·»åŠ è¿‡æ»¤åŠŸèƒ½
        filters = work_sheet.auto_filter
        filters.ref = 'A:' + dict_num_to_alphabet.get(max_col)
    work_book.save(file_path)
    print('end')
    print('$' * 20)


def incremental_update(
        df: pd.DataFrame,
        table_name: str,
        merge_cols: list,
        conn: sqlalchemy.engine,
        server,
        user,
        password,
        database,
        dtype: dict = None,
        select: str = '',
        need_print: bool = True,
):
    """
    å¢é‡æ›´æ–°dfè‡³æ•°æ®åº“çš„table_nameä¸­
    :param df: éœ€è¦è¢«ä¼ å…¥çš„DataFrame
    :param table_name: éœ€è¦æ›´æ–°çš„æ•°æ®åº“è¡¨å
    :param merge_cols: åŒ¹é…çš„å­—æ®µåˆ—è¡¨
    :param conn: æ•°æ®åº“è¿æ¥
    :param server: æœåŠ¡å™¨åœ°å€
    :param user: ç”¨æˆ·å
    :param password: å¯†ç 
    :param database: dbå
    :param dtype: ç‰¹æ®Šå­—æ®µç±»å‹ï¼Œé»˜è®¤ç©º
    :param select: é€‰æ‹©çš„æ¨¡å¼
    :param need_print: æ˜¯å¦éœ€è¦æ‰“å°ä¿¡æ¯ï¼Œé»˜è®¤æ˜¯
    """
    # dtypeè®¾ç½®ä¸ºç©ºç™½å­—å…¸
    if dtype is None:
        dtype = {}

    # è·å–å½“å‰ç³»ç»Ÿç”¨æˆ·å
    user_program = os.getlogin()
    # é‡ç½®ç´¢å¼•ï¼Œä¸ç„¶åŒ¹é…æ—¶
    df = df.reset_index(drop=True)
    # æ£€æµ‹æ•°æ®åº“ä¸­é‡å¤è®°å½•ï¼Œæœ‰è®¡ç®—åˆ—çš„è¯ä¼šå–è®¡ç®—åˆ—
    sql = f"select {', '.join(merge_cols)}" + ", id"
    sql += f' from {table_name}'
    print('ä»æ•°æ®åº“è¯»å–å·²æœ‰æ•°æ®') if need_print else None
    df_db = pd.read_sql(sql, conn, dtype=dtype)

    rows_inner = df.drop_duplicates(merge_cols)[merge_cols]  # éœ€è¦æ£€æµ‹çš„ç»´åº¦çš„è®°å½•æ•°
    df_db_inner = df_db.merge(rows_inner, 'inner')  # æ•°æ®åº“ä¸­é‡å¤ç»´åº¦çš„è®°å½•

    nums_db = df_db.shape[0]  # æ•°æ®åº“çš„æ€»æ•°é‡
    nums_df = df.shape[0]  # æœ¬åœ°çš„æ€»æ•°é‡
    nums_inner = df_db_inner.shape[0]  # æ•°æ®åº“ä¸­ç­‰äºæœ¬åœ°ç»´åº¦çš„æ•°é‡

    if need_print:
        print('-' * 50)
        print(f'''æœ¬åœ°æ•°æ®ï¼š{nums_df}æ¡''')
        print(f'''æ•°æ®åº“ä¸­â€œ{table_name}â€æ•°æ®ï¼š{nums_db}æ¡''')
        print(f'''æ•°æ®åº“ä¸­ç›¸åŒç»´åº¦çš„ï¼š{nums_inner}æ¡''')
        print(rows_inner.reset_index(drop=True))

    # æ²¡æœ‰è¾“å…¥é»˜è®¤çš„é€‰é¡¹èœè‚´inputè¾“å…¥
    if not select:
        info = ('è¯·è¾“å…¥æŒ‡ä»¤ä»¥ç»§ç»­ï¼ˆç›´æ¥å›è½¦å¯è·³è¿‡ï¼‰ï¼š'
                '\n1ï¼šâš ï¸ æ¸…ç©ºæ•°æ®åº“ï¼Œå†ä¸Šä¼ æœ¬åœ°è®°å½•ï¼ˆæ­¤æ“ä½œä¼šæ¸…ç©ºå†å²æ•°æ®ï¼Œè¯·æ…é‡é€‰æ‹©ï¼ï¼‰'
                '\n2: ğŸ”ƒ åˆ é™¤é‡å¤çš„ï¼Œå†ä¸Šä¼ æœ¬åœ°è®°å½•'
                '\n\n')
        select = input(info)

    mysql = Mysql(server, user, password, database)

    # å…ˆåˆ é™¤
    if select == '1':
        print(f'æ¸…ç©º{table_name}') if need_print else None
        mysql.exec_query(f'truncate table {table_name}')
    elif select == '2':
        ids_to_be_deleted = df_db_inner['id'].to_list()
        ids_len = len(ids_to_be_deleted)
        if ids_len > 0:
            # åˆ æ‰æ•°æ®åº“ä¸­è¿™äº›æ•°æ®
            print(f'åˆ é™¤{table_name}ä¸­{ids_len}æ¡è®°å½•') if need_print else None
            for i in range(0, ids_len, 1000):
                mysql.exec_query(f'delete from {table_name} where id in {tuple(ids_to_be_deleted[i: i + 1000])}')
            print(f'\n{user_program}åˆ é™¤äº†{table_name}çš„{ids_len}æ¡è®°å½•') if need_print else None

    if select != '':
        # å¯¼å…¥æ•°æ®åº“
        upload_records = df.shape[0]
        # å†é‡ç½®ä¸€æ¬¡ç´¢å¼•
        df = df.reset_index(drop=True)

        # å°è¯•æ·»åŠ update_time
        columns_db = pd.read_sql(f'select * from {table_name} limit 1', conn).columns.to_list()
        if 'update_time' in columns_db:
            print('æ·»åŠ update_time') if need_print else None
            df['update_time'] = pd.Timestamp.now()

        print(f'{upload_records}æ¡è®°å½•ç­‰å¾…è¢«å¯¼å…¥è‡³{table_name}') if need_print else None
        for i in range(0, upload_records, 1000):
            df.loc[i: i + 999].to_sql(f'{table_name}', conn, index=False, if_exists='append')
        print(f'{user_program}å¯¼å…¥äº†{upload_records}æ¡è®°å½•è‡³{table_name}') if need_print else None
    else:
        print('è·³è¿‡') if need_print else None


def lambda_f(n: str, sep: str = ','):
    """
    æ ¹æ®è¾“å…¥çš„å…³é”®è¯è¿”å›å¯¹åº”çš„åŒ¿åå‡½æ•°
    :param n: lam_multi_to_unique_single, s: lam_multi_to_single
    :param sep: åˆ†éš”ç¬¦ï¼Œé»˜è®¤é€—å·
    :return lambda function
    """
    lam_multi_to_unique_single = lambda x: re.sub('^,|,$', '', sep.join(set(x)))  # å¤šè¡Œè½¬å”¯ä¸€ä¸€è¡Œ
    lam_multi_to_single = lambda x: re.sub('^,|,$', '', sep.join(list(x)))  # å¤šè¡Œè½¬ä¸€è¡Œ
    return {'us': lam_multi_to_unique_single, 's': lam_multi_to_single}.get(n)


def print_date_ranges(series):
    # ç¡®ä¿Seriesä¸ºdatetimeç±»å‹ï¼Œå¹¶æŒ‰æ—¥æœŸæ’åº
    s = series.sort_values().reset_index(drop=True)

    # è®¡ç®—æ—¥æœŸå·®æ˜¯å¦è¶…è¿‡1å¤©ï¼Œæ ‡è®°æ–°åˆ†ç»„çš„èµ·ç‚¹
    mask = (s.diff() > pd.Timedelta(days=1)).fillna(False)
    group_ids = mask.cumsum()

    # æŒ‰åˆ†ç»„èšåˆï¼Œè·å–æ¯ç»„çš„æœ€å°å’Œæœ€å¤§æ—¥æœŸ
    groups = s.groupby(group_ids).agg([('start', 'min'), ('end', 'max')])

    # ç”Ÿæˆç»“æœå­—ç¬¦ä¸²
    result = []
    for _, row in groups.iterrows():
        start_str = row['start'].strftime('%Y-%m-%d')
        end_str = row['end'].strftime('%Y-%m-%d')
        if start_str == end_str:
            result.append(f"'{start_str}'")
        else:
            result.append(f"'{start_str}'è‡³'{end_str}'")

    # æ‹¼æ¥æœ€ç»ˆè¾“å‡º
    print('æ•°æ®åº“ç°æœ‰çš„æ—¥æœŸèŒƒå›´ï¼š')
    print(', '.join(result))


def input_date_period():
    """åˆ›å»ºæ—¥æœŸèŒƒå›´ï¼Œé»˜è®¤æ˜¨å¤©ï¼Œè¿”å›å¼€å§‹æ—¥æœŸã€ç»“æŸæ—¥æœŸã€è´¦å•æ—¥æœŸï¼Œè´¦å•æ—¥æœŸä¸ºå¼€å§‹æ—¥æœŸ/ç»“æŸæ—¥æœŸ"""
    # 1 è¾“å…¥æ—¥æœŸ
    default_month = (pd.Timestamp.now().to_period('D') - 1).strftime('%Y-%m-%d')
    start_date = input(f'è¯·è¾“å…¥å¼€å§‹æ—¥æœŸï¼Œæ ¼å¼ï¼šyyyy-mm-ddï¼Œå¦‚æœåªè¾“å…¥yyyy-mmåˆ™ä¼šä¸‹è½½æ•´æœˆæ•°æ®ï¼Œå›è½¦åˆ™è¾“å…¥è¯¥æ—¥ï¼š\n{default_month}\n')
    if not start_date:
        start_date = default_month
    # æ£€æµ‹æ˜¯æ•´æœˆè¿˜æ˜¯æ—¥æœŸ
    if re.search('^\d{4}-\d{1,2}$', start_date):
        start_date = pd.to_datetime(start_date, format='%Y-%m').date()
        end_date = (start_date + pd.offsets.MonthEnd(0)).date()
        print(f'æ‚¨è¾“å…¥çš„ä¸ºæ•´æœˆï¼Œæ—¥æœŸèŒƒå›´ä¸ºï¼š{start_date} è‡³ {end_date}')
    else:
        start_date = pd.to_datetime(start_date, format='%Y-%m-%d').date()
        end_date = input(f'è¯·è¾“å…¥ç»“æŸæ—¥æœŸï¼Œæ ¼å¼ï¼šyyyy-mm-ddï¼Œç›´æ¥å›è½¦çš„è¯ç»“æŸæ—¥æœŸä¸ºï¼š{str(start_date)}\n')
        if end_date == '':
            end_date = start_date
        else:
            end_date = pd.to_datetime(end_date, format='%Y-%m-%d').date()
    start_date = str(start_date)
    end_date = str(end_date)
    bill_date = start_date + '/' + end_date
    print(f'æ‚¨é€‰æ‹©çš„æ—¥æœŸèŒƒå›´ä¸ºï¼š{start_date}è‡³{end_date}')

    return start_date, end_date, bill_date


def rename_dict(conn, db_table_name, table_name, key_col, value_col):
    """
    ä»æ•°æ®åº“è¯»å–é‡å‘½åçš„è¡¨ï¼Œå¹¶è½¬åŒ–æˆå­—å…¸
    :param conn: æ•°æ®åº“è¿æ¥
    :param db_table_name: æ•°æ®åº“çš„è¡¨å
    :param table_name: è¦é‡å‘½åçš„è¡¨å
    :param key_col: å­—å…¸é”®
    :param value_col: å­—å…¸å€¼
    """
    dict_rename = pd.read_sql(
        f"select eng_name, chn_name from {db_table_name} where table_name = '{table_name}'", conn).set_index(key_col).to_dict()[value_col]
    return dict_rename