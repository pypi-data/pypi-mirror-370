defaults:
  - base_config_schema
  - /model: chem_mrl # chem_mrl, chem_2d_mrl, classifier or dice_loss_classifier
  - _self_

datasets:
  # Provide train_dataset and/or val_dataset to train and evaluate on different datasets.
  # Datasets should be compatible with datasets.DatasetDict or datasets.Dataset (individual local files).
  - key: pubchem_10m_genmol_similarity # Should be unique
    train_dataset: # may be set to null if only using a val_dataset
      name: Derify/pubchem_10m_genmol_similarity # HuggingFace dataset name or path should load as a datasets.DatasetDict
      split_key: train
      label_cast_type: float32 # float64, float32, or float16 (used by chem_mrl model)
      sample_size: 100000 # Number of training samples to load. Uses seeded sampling if a seed is set.
    val_dataset: # May be set to null if only using a train_dataset
      name: Derify/pubchem_10m_genmol_similarity
      split_key: validation # If loading from different local files, this should be set to train
      label_cast_type: float16
      sample_size: 100000
    test_dataset: # may be set to null to disable evaluation on a test dataset
      name: Derify/pubchem_10m_genmol_similarity
      split_key: test # If loading from different local files, this should be set to train
      label_cast_type: float16
      sample_size: 100000
    smiles_a_column_name: smiles_a
    smiles_b_column_name: smiles_b # Can be set to `null` when training a classifier model
    label_column_name: similarity

early_stopping_patience: 3 # Number of epochs to wait before early stopping
scale_learning_rate: false # Scale learning rate by sqrt(batch_size)
use_normalized_weight_decay:
  false # overrides the weight decay specified in training_args
  # Normalized weight decay for adamw optimizer - https://arxiv.org/pdf/1711.05101.pdf
  # optimized hyperparameter lambda_norm = 0.05 for AdamW optimizer
  # Hyperparameter search indicates a normalized weight decay outperforms
  # the default adamw weight decay

# Note: SentenceTransformerTrainingArguments extends transformers.TrainingArguments
# https://sbert.net/docs/package_reference/sentence_transformer/training_args.html
# https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments
training_args:
  _target_: sentence_transformers.SentenceTransformerTrainingArguments
  run_name: chem_mrl
  resume_from_checkpoint: null # null or the path to a valid checkpoint to resume training from.
  output_dir: training_output/${training_args.run_name}_${now:%Y-%m-%d_%H-%M-%S} # Path to save model, checkpoints and evaluation results
  overwrite_output_dir: false
  num_train_epochs: 3
  learning_rate: 5.0e-05
  lr_scheduler_type: linear
  warmup_ratio: 0.05
  optim: adamw_torch # transformer's compatible optimizer name
  weight_decay: 0
  seed: 42
  data_seed: 42
  do_train: true
  eval_on_start: false
  max_steps: -1
  eval_strategy: epoch # 'no', 'steps', 'epoch'
  eval_steps: 0
  load_best_model_at_end: true
  metric_for_best_model: eval_${datasets[0].key}_${model.eval_metric} # Change index for your dataset of choice but keep the format as is
  save_strategy: epoch # 'no', 'steps', 'epoch'
  save_steps: 0
  save_total_limit: 10
  logging_strategy: epoch # 'no', 'steps', 'epoch'
  logging_steps: 0
  logging_nan_inf_filter: true
  logging_dir: ${training_args.output_dir}/logs
  report_to: all # codecarbon and wandb installed by sentence-transformers
  per_device_train_batch_size: 32
  per_device_eval_batch_size: 32
  tf32: false
  fp16: false
  gradient_accumulation_steps: 1
  gradient_checkpointing: false
  disable_tqdm: false
  dataloader_num_workers: 0
  dataloader_pin_memory: false
  dataloader_prefetch_factor: null # should be null if dataloader_num_workers is 0
  dataloader_persistent_workers: false
