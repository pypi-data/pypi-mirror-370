# MultiMind RAG System Features

## Core Features

### 1. Document Processing
- **Text Chunking**
  - Configurable chunk size and overlap
  - Smart text splitting with token awareness
  - Support for various document formats
- **Metadata Management**
  - Custom metadata per document
  - Metadata filtering during queries
  - Batch metadata assignment
- **File Processing**
  - Direct file upload support
  - Automatic format detection
  - Temporary file handling

### 2. Embedding System
- **Multiple Embedding Models**
  - OpenAI Embeddings (text-embedding-ada-002)
  - HuggingFace Transformers
  - Sentence-T5 Models
- **Embedding Management**
  - Batch processing
  - Caching support
  - Dimension management
- **Model Switching**
  - Runtime model switching
  - Model-specific configurations
  - Fallback mechanisms

### 3. Vector Storage
- **Multiple Vector Stores**
  - FAISS (CPU/GPU support)
  - ChromaDB
  - Extensible store interface
- **Storage Features**
  - Efficient similarity search
  - Metadata indexing
  - Document management
  - Store clearing and maintenance

### 4. Query System
- **Semantic Search**
  - Top-k retrieval
  - Similarity scoring
  - Metadata filtering
- **Response Generation**
  - Context-aware generation
  - Temperature control
  - Token limit management
  - Model-specific parameters

### 5. API System
- **RESTful API**
  - Document management endpoints
  - Query and generation endpoints
  - Model management endpoints
  - Health monitoring
- **Authentication**
  - API key support
  - JWT authentication
  - Role-based access control
  - Scope-based permissions
- **Client Library**
  - Async Python client
  - Comprehensive error handling
  - Type hints and validation
  - Easy integration

## Advanced Features

### 1. Security
- **Authentication**
  - JWT token management
  - API key validation
  - Token expiration
  - Scope-based access control
- **Data Protection**
  - Secure file handling
  - Input validation
  - Error message sanitization
  - Rate limiting support

### 2. Performance
- **Optimization**
  - Batch processing
  - Embedding caching
  - Async operations
  - Connection pooling
- **Resource Management**
  - Memory efficient processing
  - Temporary file cleanup
  - Connection management
  - Resource limits

### 3. Integration
- **Model Integration**
  - OpenAI models
  - Anthropic models
  - Custom model support
  - Model switching
- **Tool Integration**
  - Agent system integration
  - Custom tool support
  - Tool chaining
  - Memory integration

### 4. Monitoring
- **Health Checks**
  - System status
  - Document counts
  - Model status
  - Resource usage
- **Error Handling**
  - Comprehensive error types
  - Detailed error messages
  - Error recovery
  - Logging support

## Implementation Status

### Completed Features
- âœ… Document processing and chunking
- âœ… Basic embedding system
- âœ… FAISS and Chroma vector stores
- âœ… Query and generation system
- âœ… RESTful API with authentication
- âœ… Python client library
- âœ… Basic security features
- âœ… Health monitoring
- âœ… Model switching
- âœ… File processing

### In Progress
- ðŸ”„ Advanced caching system
- ðŸ”„ Rate limiting
- ðŸ”„ Advanced monitoring
- ðŸ”„ Custom model integration
- ðŸ”„ Performance optimization

### Planned Features
- ðŸ“… GPU acceleration
- ðŸ“… Distributed processing
- ðŸ“… Advanced analytics
- ðŸ“… Web interface
- ðŸ“… Additional vector stores
- ðŸ“… Streaming responses
- ðŸ“… Advanced security features
- ðŸ“… Multi-language support

## Usage Examples

### Basic Usage
```python
from multimind.client.rag_client import RAGClient, Document

# Initialize client
client = RAGClient(api_key="your-api-key")

# Add documents
docs = [Document(text="Example document", metadata={"type": "example"})]
await client.add_documents(docs)

# Query
results = await client.query("Example query")
```

### Advanced Usage
```python
# Model switching
await client.switch_model("openai", "gpt-4")

# Generate with custom parameters
response = await client.generate(
    query="Complex query",
    temperature=0.8,
    max_tokens=1000,
    filter_metadata={"type": "specific"}
)
```

### API Usage
```python
# JWT Authentication
token = await client.login("username", "password")

# File processing
await client.add_file(
    "document.pdf",
    metadata={"source": "pdf", "category": "documentation"}
)
```

## System Requirements

### Software Requirements
- Python 3.8+
- FastAPI
- aiohttp
- PyJWT
- FAISS/ChromaDB
- OpenAI/Anthropic API access

### Environment Variables
```bash
# Authentication
export API_KEYS="key1,key2,key3"
export JWT_SECRET="your-secret-key"

# Model APIs
export OPENAI_API_KEY="your-openai-key"
export ANTHROPIC_API_KEY="your-anthropic-key"
```

### Dependencies
- `fastapi`: Web framework
- `aiohttp`: Async HTTP client
- `pydantic`: Data validation
- `pyjwt`: JWT handling
- `faiss-cpu`/`faiss-gpu`: Vector store
- `chromadb`: Vector store
- `sentence-transformers`: Embeddings
- `openai`: OpenAI integration
- `anthropic`: Anthropic integration

## Model Client System and Routing

### Extensible Model Clients
- **ModelClient**: Base class for all model clients (transformer and non-transformer). Subclass to implement custom models.
- **LSTMModelClient, RNNModelClient, GRUModelClient**: Ready-to-use clients for classic sequence models.
- **MoEModelClient**: Mixture-of-Experts client that routes requests to the best expert model based on a routing function.
- **DynamicMoEModelClient**: Advanced MoE client that routes based on runtime metrics (latency, input length, etc.).
- **MultiModalClient**: Unified client for text, image, audio, video, and code models. Routes requests based on input type.
- **Other Clients**: Includes MambaClient, RWKVClient, SpaCyClient, S4Client, HyenaClient, DiffusionTextClient, and stubs for image/audio/video/code models.

### Routing Logic
- **FederatedRouter**: Routes between local and cloud model clients based on context (input size, latency, privacy, etc.). Supports custom routing logic and metrics tracking.
- **Custom Routing**: Easily extendable to support new routing strategies and model types.

### Example Use Cases
- Combine multiple model types in a single workflow.
- Route requests dynamically for cost, latency, or quality optimization.
- Build multimodal applications with unified API.

See the Usage Guide for code examples. 