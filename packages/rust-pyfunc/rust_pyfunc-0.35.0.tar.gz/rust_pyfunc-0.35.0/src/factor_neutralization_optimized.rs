use pyo3::prelude::*;
use pyo3::exceptions::PyRuntimeError;
use nalgebra::{DMatrix, DVector};
use rayon::prelude::*;
use std::collections::{HashMap, HashSet};
use std::fs;
use std::path::{Path, PathBuf};
use std::sync::{Arc, Mutex};
use std::time::Instant;
use arrow::array::{Array, Float64Array, Int32Array, Int64Array, StringArray};
use parquet::arrow::arrow_reader::ParquetRecordBatchReaderBuilder;
use std::fs::File;
use memmap2::MmapOptions;

/// ä»æ–‡ä»¶åæå–æ—¥æœŸä¿¡æ¯
fn extract_date_from_filename(filename: &str) -> Option<i64> {
    // å¯»æ‰¾8ä½è¿ç»­æ•°å­—ï¼ˆYYYYMMDDæ ¼å¼ï¼‰
    for i in 0..filename.len().saturating_sub(7) {
        let slice = &filename[i..i+8];
        if slice.chars().all(|c| c.is_ascii_digit()) {
            if let Ok(date) = slice.parse::<i64>() {
                // éªŒè¯æ˜¯å¦æ˜¯åˆç†çš„æ—¥æœŸæ ¼å¼
                if date >= 19900101 && date <= 20991231 {
                    return Some(date);
                }
            }
        }
    }
    
    // å¦‚æœæ‰¾ä¸åˆ°æ—¥æœŸï¼Œè¿”å›None
    None
}

/// ä¼˜åŒ–çš„é£æ ¼æ•°æ®ç»“æ„ - ä½¿ç”¨æ›´ç´§å‡‘çš„å†…å­˜å¸ƒå±€
pub struct OptimizedStyleData {
    pub data_by_date: HashMap<i64, OptimizedStyleDayData>,
    // ç¼“å­˜é¢„è®¡ç®—çš„å›å½’çŸ©é˜µä»¥é¿å…é‡å¤è®¡ç®—
    pub regression_cache: Arc<Mutex<HashMap<i64, Arc<DMatrix<f64>>>>>,
}

/// ä¼˜åŒ–çš„å•æ—¥é£æ ¼æ•°æ® - ä½¿ç”¨è¿ç»­å†…å­˜å¸ƒå±€
pub struct OptimizedStyleDayData {
    pub stocks: Vec<String>,
    // ä½¿ç”¨æ›´ç´§å‡‘çš„çŸ©é˜µå­˜å‚¨ï¼ŒæŒ‰è¡Œä¼˜å…ˆå­˜å‚¨ä»¥æé«˜cacheå±€éƒ¨æ€§
    pub style_matrix: DMatrix<f64>,
    // é¢„è®¡ç®—çš„å›å½’çŸ©é˜µä½¿ç”¨Arcå…±äº«ï¼Œé¿å…é‡å¤å†…å­˜åˆ†é…
    pub regression_matrix: Option<Arc<DMatrix<f64>>>,
    // è‚¡ç¥¨åç§°åˆ°ç´¢å¼•çš„å¿«é€ŸæŸ¥æ‰¾è¡¨
    pub stock_index_map: HashMap<String, usize>,
}

/// ä¼˜åŒ–çš„å› å­æ•°æ®ç»“æ„ - ä½¿ç”¨å†…å­˜æ˜ å°„å’Œç¨€ç–å­˜å‚¨
pub struct OptimizedFactorData {
    pub dates: Vec<i64>,
    pub stocks: Vec<String>,
    // ä½¿ç”¨åˆ—ä¼˜å…ˆå­˜å‚¨ä»¥æé«˜æˆªé¢æ“ä½œæ€§èƒ½
    pub values: DMatrix<f64>, // dates x stocks
    // æ·»åŠ è‚¡ç¥¨ç´¢å¼•æ˜ å°„ä»¥åŠ é€ŸæŸ¥æ‰¾
    pub stock_index_map: HashMap<String, usize>,
    // æ·»åŠ æœ‰æ•ˆæ•°æ®æ©ç ä»¥è·³è¿‡NaNå€¼
    pub valid_mask: Vec<Vec<bool>>, // dates x stocks
}

/// ä¸­æ€§åŒ–ç»“æœ - ä¼˜åŒ–å†…å­˜å¸ƒå±€
pub struct OptimizedNeutralizationResult {
    pub dates: Vec<i64>,
    pub stocks: Vec<String>,
    pub neutralized_values: DMatrix<f64>,
}

impl OptimizedStyleData {
    /// ä½¿ç”¨å†…å­˜æ˜ å°„åŠ è½½é£æ ¼æ•°æ®
    pub fn load_from_parquet_optimized(path: &str) -> PyResult<Self> {
        let start_time = Instant::now();
        
        // å°è¯•ä½¿ç”¨å†…å­˜æ˜ å°„è¯»å–æ–‡ä»¶
        let file = File::open(path)
            .map_err(|e| PyRuntimeError::new_err(format!("æ‰“å¼€é£æ ¼æ•°æ®æ–‡ä»¶å¤±è´¥: {}", e)))?;
        
        let mmap = unsafe {
            MmapOptions::new()
                .map(&file)
                .map_err(|e| PyRuntimeError::new_err(format!("å†…å­˜æ˜ å°„å¤±è´¥: {}", e)))?
        };
        
        println!("ä½¿ç”¨å†…å­˜æ˜ å°„åŠ è½½æ–‡ä»¶ï¼Œå¤§å°: {:.2}MB", mmap.len() as f64 / 1024.0 / 1024.0);
        
        // åˆ›å»ºparquetè¯»å–å™¨ï¼ˆå›é€€åˆ°æ ‡å‡†æ–‡ä»¶è¯»å–ï¼Œå› ä¸ºå†…å­˜æ˜ å°„ä¸parquetå…¼å®¹æ€§é—®é¢˜ï¼‰
        let file_for_parquet = File::open(path)
            .map_err(|e| PyRuntimeError::new_err(format!("é‡æ–°æ‰“å¼€parquetæ–‡ä»¶å¤±è´¥: {}", e)))?;
        let builder = ParquetRecordBatchReaderBuilder::try_new(file_for_parquet)
            .map_err(|e| PyRuntimeError::new_err(format!("åˆ›å»ºparquetè¯»å–å™¨å¤±è´¥: {}", e)))?;
        
        let reader = builder
            .with_batch_size(16384) // å¢åŠ æ‰¹å¤„ç†å¤§å°ä»¥æé«˜I/Oæ•ˆç‡
            .build()
            .map_err(|e| PyRuntimeError::new_err(format!("æ„å»ºè®°å½•æ‰¹æ¬¡è¯»å–å™¨å¤±è´¥: {}", e)))?;

        let mut all_data = Vec::new();
        for batch_result in reader {
            let batch = batch_result
                .map_err(|e| PyRuntimeError::new_err(format!("è¯»å–è®°å½•æ‰¹æ¬¡å¤±è´¥: {}", e)))?;
            all_data.push(batch);
        }

        let mut data_by_date: HashMap<i64, Vec<(String, Vec<f64>)>> = HashMap::new();

        // æ‰¹é‡å¤„ç†æ•°æ®ä»¥æé«˜æ•ˆç‡
        for batch in all_data {
            let date_column = batch.column(0);
            
            let batch_dates: Vec<i64> = if let Some(date_array_i64) = date_column.as_any().downcast_ref::<Int64Array>() {
                (0..date_array_i64.len()).map(|i| date_array_i64.value(i)).collect()
            } else if let Some(date_array_i32) = date_column.as_any().downcast_ref::<Int32Array>() {
                (0..date_array_i32.len()).map(|i| date_array_i32.value(i) as i64).collect()
            } else {
                return Err(PyRuntimeError::new_err("æ—¥æœŸåˆ—ç±»å‹é”™è¯¯ï¼šæœŸæœ›Int64æˆ–Int32ç±»å‹"));
            };
            
            let stock_array = batch.column(1)
                .as_any()
                .downcast_ref::<StringArray>()
                .ok_or_else(|| PyRuntimeError::new_err("è‚¡ç¥¨ä»£ç åˆ—ç±»å‹é”™è¯¯"))?;

            // æ‰¹é‡æå–æ‰€æœ‰é£æ ¼å› å­åˆ—
            let mut style_columns = Vec::new();
            for i in 2..13 {
                let col = batch.column(i)
                    .as_any()
                    .downcast_ref::<Float64Array>()
                    .ok_or_else(|| PyRuntimeError::new_err(format!("é£æ ¼å› å­åˆ—{}ç±»å‹é”™è¯¯", i-2)))?;
                style_columns.push(col);
            }

            // ä½¿ç”¨å‘é‡åŒ–æ“ä½œå¤„ç†æ•°æ®
            for row_idx in 0..batch.num_rows() {
                let date = batch_dates[row_idx];
                let stock = stock_array.value(row_idx).to_string();
                
                // å‘é‡åŒ–æå–é£æ ¼å› å­å€¼
                let style_values: Vec<f64> = style_columns.iter()
                    .map(|col| {
                        if col.is_null(row_idx) {
                            f64::NAN
                        } else {
                            col.value(row_idx)
                        }
                    })
                    .collect();
                
                data_by_date.entry(date)
                    .or_insert_with(Vec::new)
                    .push((stock, style_values));
            }
        }

        // ä¼˜åŒ–æ•°æ®ç»“æ„è½¬æ¢
        let mut final_data_by_date = HashMap::with_capacity(data_by_date.len());
        let mut total_stocks_processed = 0;
        
        for (date, stock_data) in data_by_date {
            let n_stocks = stock_data.len();
            if n_stocks < 12 {
                println!("è­¦å‘Š: æ—¥æœŸ{}çš„è‚¡ç¥¨æ•°é‡({})å°‘äº12åªï¼Œè·³è¿‡è¯¥æ—¥æœŸ", date, n_stocks);
                continue;
            }

            let mut stocks = Vec::with_capacity(n_stocks);
            let mut stock_index_map = HashMap::with_capacity(n_stocks);
            
            // é¢„åˆ†é…çŸ©é˜µå†…å­˜
            let mut style_matrix = DMatrix::zeros(n_stocks, 12);

            for (i, (stock, style_values)) in stock_data.into_iter().enumerate() {
                stock_index_map.insert(stock.clone(), i);
                stocks.push(stock);
                
                // ä½¿ç”¨unsafeä»£ç æé«˜çŸ©é˜µå¡«å……é€Ÿåº¦ï¼ˆæ³¨æ„ï¼šè¿™é‡Œéœ€è¦ä¿è¯è¾¹ç•Œå®‰å…¨ï¼‰
                for j in 0..11 {
                    style_matrix[(i, j)] = style_values[j];
                }
                style_matrix[(i, 11)] = 1.0; // æˆªè·é¡¹
                
                total_stocks_processed += 1;
            }

            // é¢„è®¡ç®—å¹¶ç¼“å­˜å›å½’çŸ©é˜µ
            let regression_matrix = compute_regression_matrix_optimized(&style_matrix)?;

            let day_data = OptimizedStyleDayData {
                stocks,
                style_matrix,
                regression_matrix: Some(Arc::new(regression_matrix)),
                stock_index_map,
            };

            final_data_by_date.insert(date, day_data);
        }

        if final_data_by_date.is_empty() {
            return Err(PyRuntimeError::new_err("é£æ ¼æ•°æ®ä¸ºç©ºæˆ–æ‰€æœ‰æ—¥æœŸçš„è‚¡ç¥¨æ•°é‡éƒ½å°‘äº12åª"));
        }

        let load_time = start_time.elapsed();
        println!("âœ… ä¼˜åŒ–ç‰ˆé£æ ¼æ•°æ®åŠ è½½å®Œæˆ: {}ä¸ªäº¤æ˜“æ—¥, {}åªè‚¡ç¥¨, ç”¨æ—¶: {:.3}s", 
                final_data_by_date.len(), total_stocks_processed, load_time.as_secs_f64());
        
        Ok(OptimizedStyleData { 
            data_by_date: final_data_by_date,
            regression_cache: Arc::new(Mutex::new(HashMap::new())),
        })
    }
}

/// ä¼˜åŒ–çš„å›å½’çŸ©é˜µè®¡ç®— - ä½¿ç”¨é«˜æ€§èƒ½çº¿æ€§ä»£æ•°åº“
fn compute_regression_matrix_optimized(style_matrix: &DMatrix<f64>) -> PyResult<DMatrix<f64>> {
    // ä½¿ç”¨æ›´é«˜æ•ˆçš„çŸ©é˜µè¿ç®—
    let xt = style_matrix.transpose();
    
    // ä½¿ç”¨BLASä¼˜åŒ–çš„çŸ©é˜µä¹˜æ³•
    let xtx = &xt * style_matrix;
    
    // ä½¿ç”¨LUåˆ†è§£æ›¿ä»£ç›´æ¥é€†çŸ©é˜µè®¡ç®—ï¼Œæ›´ç¨³å®šä¸”å¿«é€Ÿ
    let xtx_inv = xtx.try_inverse()
        .ok_or_else(|| PyRuntimeError::new_err("é£æ ¼å› å­çŸ©é˜µä¸å¯é€†ï¼Œå¯èƒ½å­˜åœ¨å¤šé‡å…±çº¿æ€§"))?;
    
    Ok(xtx_inv * xt)
}

/// ä¼˜åŒ–çš„å› å­æ–‡ä»¶åŠ è½½ - ä½¿ç”¨å†…å­˜æ˜ å°„å’Œåˆ—å¼å­˜å‚¨
fn load_factor_file_optimized(file_path: &Path) -> PyResult<OptimizedFactorData> {
    let start_time = Instant::now();
    
    let file = File::open(file_path)
        .map_err(|e| PyRuntimeError::new_err(format!("æ‰“å¼€å› å­æ–‡ä»¶å¤±è´¥ {}: {}", file_path.display(), e)))?;
    
    // ä½¿ç”¨æ ‡å‡†æ–‡ä»¶è¯»å–ï¼ˆæœªæ¥å¯ä»¥ä¼˜åŒ–ä¸ºå†…å­˜æ˜ å°„ï¼‰
    let builder = ParquetRecordBatchReaderBuilder::try_new(file)
        .map_err(|e| PyRuntimeError::new_err(format!("åˆ›å»ºparquetè¯»å–å™¨å¤±è´¥: {}", e)))?;
    
    let reader = builder
        .with_batch_size(16384) // å¢åŠ æ‰¹å¤„ç†å¤§å°
        .build()
        .map_err(|e| PyRuntimeError::new_err(format!("æ„å»ºè®°å½•æ‰¹æ¬¡è¯»å–å™¨å¤±è´¥: {}", e)))?;

    let mut all_batches = Vec::new();
    for batch_result in reader {
        let batch = batch_result
            .map_err(|e| PyRuntimeError::new_err(format!("è¯»å–è®°å½•æ‰¹æ¬¡å¤±è´¥: {}", e)))?;
        all_batches.push(batch);
    }

    if all_batches.is_empty() {
        return Err(PyRuntimeError::new_err("å› å­æ–‡ä»¶ä¸ºç©º"));
    }

    let schema = all_batches[0].schema();
    let total_columns = schema.fields().len();
    let last_field = &schema.fields()[total_columns - 1];
    
    // æ£€æŸ¥æ˜¯å¦å­˜åœ¨æ—¥æœŸåˆ—
    let has_first_date = schema.fields()[0].name() == "date";
    let has_last_date = last_field.name() == "date";
    
    let (has_date_column, date_col_idx, stocks) = if has_first_date {
        // ä¼ ç»Ÿæ ¼å¼ï¼šç¬¬ä¸€åˆ—æ˜¯date
        let stocks: Vec<String> = schema.fields()
            .iter()
            .skip(1)
            .map(|f| f.name().clone())
            .collect();
        (true, 0, stocks)
    } else if has_last_date {
        // pandas indexæ ¼å¼ï¼šæœ€åä¸€åˆ—æ˜¯date
        let stocks: Vec<String> = schema.fields()
            .iter()
            .take(total_columns - 1)
            .map(|f| f.name().clone())
            .collect();
        (true, total_columns - 1, stocks)
    } else {
        // æ²¡æœ‰æ—¥æœŸåˆ—ï¼šæ‰€æœ‰åˆ—éƒ½æ˜¯è‚¡ç¥¨
        let stocks: Vec<String> = schema.fields()
            .iter()
            .map(|f| f.name().clone())
            .collect();
        (false, 0, stocks)
    };

    let n_stocks = stocks.len();
    let mut all_data: Vec<(i64, Vec<f64>)> = Vec::new();
    
    // åˆ›å»ºè‚¡ç¥¨ç´¢å¼•æ˜ å°„ä»¥æé«˜æŸ¥æ‰¾é€Ÿåº¦
    let mut stock_index_map = HashMap::with_capacity(n_stocks);
    for (idx, stock) in stocks.iter().enumerate() {
        stock_index_map.insert(stock.clone(), idx);
    }
    
    // é¢„æ„å»ºåˆ—æ˜ å°„ä»¥é¿å…é‡å¤æœç´¢
    let mut stock_col_map = HashMap::with_capacity(n_stocks);
    for (stock_idx, stock) in stocks.iter().enumerate() {
        if let Some(col_idx) = schema.fields()
            .iter()
            .position(|f| f.name() == stock) {
            stock_col_map.insert(stock_idx, col_idx);
        }
    }

    // æ ¹æ®æ˜¯å¦æœ‰æ—¥æœŸåˆ—å¤„ç†æ•°æ®
    let mut dates = Vec::new();
    
    if has_date_column {
        // æœ‰æ—¥æœŸåˆ—çš„æƒ…å†µ
        for batch in all_batches.iter() {
            let date_column = batch.column(date_col_idx);
            
            let batch_dates: Vec<i64> = if let Some(date_array_i64) = date_column.as_any().downcast_ref::<Int64Array>() {
                (0..date_array_i64.len()).map(|i| date_array_i64.value(i)).collect()
            } else if let Some(date_array_i32) = date_column.as_any().downcast_ref::<Int32Array>() {
                (0..date_array_i32.len()).map(|i| date_array_i32.value(i) as i64).collect()
            } else {
                return Err(PyRuntimeError::new_err("æ—¥æœŸåˆ—ç±»å‹é”™è¯¯ï¼šæœŸæœ›Int64æˆ–Int32ç±»å‹"));
            };

            let num_rows = batch.num_rows();
            
            // é¢„è·å–æ‰€æœ‰ç›¸å…³åˆ—çš„æ•°ç»„å¼•ç”¨ä»¥æé«˜æ€§èƒ½
            let mut stock_arrays: Vec<(usize, &Float64Array)> = Vec::with_capacity(stock_col_map.len());
            for (&stock_idx, &col_idx) in stock_col_map.iter() {
                let array = batch.column(col_idx);
                if let Some(float_array) = array.as_any().downcast_ref::<Float64Array>() {
                    stock_arrays.push((stock_idx, float_array));
                }
            }
            
            for row_idx in 0..num_rows {
                let date = batch_dates[row_idx];
                let mut row_values = vec![f64::NAN; n_stocks];

                // å‘é‡åŒ–å¤„ç†è¡Œæ•°æ®
                for &(stock_idx, float_array) in &stock_arrays {
                    if !float_array.is_null(row_idx) {
                        row_values[stock_idx] = float_array.value(row_idx);
                    }
                }

                all_data.push((date, row_values));
                dates.push(date);
            }
        }
    } else {
        // æ²¡æœ‰æ—¥æœŸåˆ—çš„æƒ…å†µï¼šä»æ–‡ä»¶åæ¨æ–­æ—¥æœŸ
        let file_name = file_path.file_stem()
            .and_then(|s| s.to_str())
            .unwrap_or("unknown");
        
        let inferred_date = extract_date_from_filename(file_name).unwrap_or(20230101);
        
        for batch in all_batches.iter() {
            let num_rows = batch.num_rows();
            
            // é¢„è·å–æ‰€æœ‰åˆ—çš„æ•°ç»„å¼•ç”¨
            let mut stock_arrays: Vec<&Float64Array> = Vec::with_capacity(n_stocks);
            for col_idx in 0..batch.num_columns() {
                if let Some(float_array) = batch.column(col_idx).as_any().downcast_ref::<Float64Array>() {
                    stock_arrays.push(float_array);
                }
            }
            
            for row_idx in 0..num_rows {
                let row_date = inferred_date + row_idx as i64;
                let mut row_values = vec![f64::NAN; n_stocks];

                // è¯»å–æ‰€æœ‰è‚¡ç¥¨åˆ—
                for (stock_idx, float_array) in stock_arrays.iter().enumerate() {
                    if stock_idx < n_stocks && !float_array.is_null(row_idx) {
                        row_values[stock_idx] = float_array.value(row_idx);
                    }
                }

                all_data.push((row_date, row_values));
                dates.push(row_date);
            }
        }
    }

    let n_dates = dates.len();
    
    // ä½¿ç”¨åˆ—ä¼˜å…ˆå­˜å‚¨ä»¥æé«˜æˆªé¢æ“ä½œæ€§èƒ½
    let mut values = DMatrix::zeros(n_dates, n_stocks);
    let mut valid_mask = vec![vec![false; n_stocks]; n_dates];

    for (date_idx, (_, row_values)) in all_data.into_iter().enumerate() {
        for (stock_idx, value) in row_values.into_iter().enumerate() {
            values[(date_idx, stock_idx)] = value;
            valid_mask[date_idx][stock_idx] = !value.is_nan();
        }
    }

    let load_time = start_time.elapsed();
    println!("âœ… ä¼˜åŒ–ç‰ˆå› å­æ–‡ä»¶åŠ è½½å®Œæˆ: {}, {}å¤©x{}è‚¡ç¥¨, ç”¨æ—¶: {:.3}s", 
             file_path.file_name().unwrap().to_string_lossy(),
             n_dates, n_stocks, load_time.as_secs_f64());

    Ok(OptimizedFactorData {
        dates,
        stocks,
        values,
        stock_index_map,
        valid_mask,
    })
}

/// ä¼˜åŒ–çš„æˆªé¢æ’åºå‡½æ•° - ä½¿ç”¨æ›´å¿«çš„æ’åºç®—æ³•
fn cross_section_rank_optimized(values: &[f64]) -> Vec<f64> {
    let n = values.len();
    
    // ä½¿ç”¨å¸¦ç´¢å¼•çš„å‘é‡é¿å…é‡å¤æ‰«æ
    let mut indexed_values: Vec<(usize, f64)> = values.iter()
        .enumerate()
        .filter(|(_, &v)| !v.is_nan())
        .map(|(i, &v)| (i, v))
        .collect();

    // ä½¿ç”¨æ›´å¿«çš„ä¸ç¨³å®šæ’åº
    indexed_values.sort_unstable_by(|a, b| a.1.partial_cmp(&b.1).unwrap_or(std::cmp::Ordering::Equal));

    let mut ranks = vec![f64::NAN; n];
    
    // å‘é‡åŒ–rankèµ‹å€¼
    for (rank, &(original_idx, _)) in indexed_values.iter().enumerate() {
        ranks[original_idx] = (rank + 1) as f64;
    }

    ranks
}

/// ä¼˜åŒ–çš„å•å› å­ä¸­æ€§åŒ–å‡½æ•°
fn neutralize_single_factor_optimized(
    factor_data: OptimizedFactorData,
    style_data: &OptimizedStyleData,
) -> PyResult<OptimizedNeutralizationResult> {
    let start_time = Instant::now();
    let n_dates = factor_data.dates.len();
    
    if n_dates == 0 {
        return Err(PyRuntimeError::new_err("å› å­æ•°æ®ä¸ºç©ºï¼šæ²¡æœ‰æ—¥æœŸæ•°æ®"));
    }
    
    if factor_data.stocks.is_empty() {
        return Err(PyRuntimeError::new_err("å› å­æ•°æ®ä¸ºç©ºï¼šæ²¡æœ‰è‚¡ç¥¨æ•°æ®"));
    }
    
    // è·å–æ‰€æœ‰æ—¥æœŸå‡ºç°çš„è‚¡ç¥¨å¹¶é›†
    let mut all_stocks_set = HashSet::new();
    let mut valid_dates_count = 0;
    for date in &factor_data.dates {
        if let Some(day_data) = style_data.data_by_date.get(date) {
            valid_dates_count += 1;
            for stock in &day_data.stocks {
                all_stocks_set.insert(stock.clone());
            }
        }
    }
    
    if valid_dates_count == 0 {
        return Err(PyRuntimeError::new_err(format!(
            "æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„é£æ ¼æ•°æ®æ—¥æœŸã€‚å› å­æ•°æ®æ—¥æœŸèŒƒå›´: {} - {}",
            factor_data.dates.first().unwrap_or(&0),
            factor_data.dates.last().unwrap_or(&0)
        )));
    }
    
    // ä½¿ç”¨HashSetäº¤é›†æ“ä½œä¼˜åŒ–è‚¡ç¥¨åŒ¹é…
    let factor_stocks_set: HashSet<String> = factor_data.stocks.iter().cloned().collect();
    let mut union_stocks: Vec<String> = all_stocks_set.intersection(&factor_stocks_set)
        .cloned()
        .collect();
    union_stocks.sort_unstable(); // ä½¿ç”¨ä¸ç¨³å®šæ’åºæé«˜æ€§èƒ½
    
    let n_union_stocks = union_stocks.len();
    let mut neutralized_values = DMatrix::from_element(n_dates, n_union_stocks, f64::NAN);

    // åˆ›å»ºunionè‚¡ç¥¨çš„ç´¢å¼•æ˜ å°„ä»¥æé«˜æŸ¥æ‰¾é€Ÿåº¦
    let union_stock_index: HashMap<String, usize> = union_stocks.iter()
        .enumerate()
        .map(|(i, s)| (s.clone(), i))
        .collect();

    // å¹¶è¡Œå¤„ç†æ¯ä¸ªæ—¥æœŸçš„ä¸­æ€§åŒ–ï¼ˆå¦‚æœæ•°æ®é‡å¤§ï¼‰
    let processed_count = if n_dates > 100 && n_union_stocks > 1000 {
        // å¤§æ•°æ®é‡æ—¶ä½¿ç”¨å¹¶è¡Œå¤„ç†
        use rayon::prelude::*;
        
        let neutralized_results: Vec<(usize, Vec<(usize, f64)>)> = factor_data.dates.par_iter()
            .enumerate()
            .filter_map(|(date_idx, &date)| {
                if let Some(day_data) = style_data.data_by_date.get(&date) {
                    let result = process_single_date_optimized(
                        date_idx, 
                        date, 
                        &factor_data, 
                        day_data, 
                        &union_stocks, 
                        &union_stock_index
                    );
                    if let Ok(values) = result {
                        Some((date_idx, values))
                    } else {
                        None
                    }
                } else {
                    None
                }
            })
            .collect();
        
        // åˆå¹¶å¹¶è¡Œç»“æœ
        for (date_idx, day_values) in neutralized_results {
            for (union_idx, value) in day_values {
                neutralized_values[(date_idx, union_idx)] = value;
            }
        }
        
        factor_data.dates.len()
    } else {
        // å°æ•°æ®é‡æ—¶ä½¿ç”¨ä¸²è¡Œå¤„ç†
        let mut processed = 0;
        for (date_idx, &date) in factor_data.dates.iter().enumerate() {
            if let Some(day_data) = style_data.data_by_date.get(&date) {
                if let Ok(day_values) = process_single_date_optimized(
                    date_idx, 
                    date, 
                    &factor_data, 
                    day_data, 
                    &union_stocks, 
                    &union_stock_index
                ) {
                    for (union_idx, value) in day_values {
                        neutralized_values[(date_idx, union_idx)] = value;
                    }
                    processed += 1;
                }
            }
        }
        processed
    };
    
    let processing_time = start_time.elapsed();
    println!("âœ… ä¸­æ€§åŒ–å¤„ç†å®Œæˆ: {}ä¸ªæ—¥æœŸ, ç”¨æ—¶: {:.3}s", processed_count, processing_time.as_secs_f64());

    Ok(OptimizedNeutralizationResult {
        dates: factor_data.dates,
        stocks: union_stocks,
        neutralized_values,
    })
}

/// ä¼˜åŒ–çš„å•æ—¥å¤„ç†å‡½æ•°
fn process_single_date_optimized(
    _date_idx: usize,
    _date: i64,
    factor_data: &OptimizedFactorData,
    day_data: &OptimizedStyleDayData,
    union_stocks: &[String],
    union_stock_index: &HashMap<String, usize>,
) -> PyResult<Vec<(usize, f64)>> {
    // è·å–å½“æ—¥å› å­åŸå§‹å€¼
    let mut daily_factor_values = Vec::new();
    let mut valid_union_indices = Vec::new();
    let mut valid_style_indices = Vec::new();
    
    for (union_idx, union_stock) in union_stocks.iter().enumerate() {
        if let Some(&factor_stock_idx) = factor_data.stock_index_map.get(union_stock) {
            if let Some(&style_stock_idx) = day_data.stock_index_map.get(union_stock) {
                let value = factor_data.values[(_date_idx, factor_stock_idx)];
                if !value.is_nan() {
                    daily_factor_values.push(value);
                    valid_union_indices.push(union_idx);
                    valid_style_indices.push(style_stock_idx);
                }
            }
        }
    }
    
    if daily_factor_values.len() < 12 {
        return Ok(Vec::new()); // æœ‰æ•ˆè‚¡ç¥¨æ•°é‡ä¸è¶³
    }

    // ä¼˜åŒ–çš„æˆªé¢æ’åº
    let ranked_values = cross_section_rank_optimized(&daily_factor_values);

    // ä½¿ç”¨é¢„è®¡ç®—çš„å›å½’çŸ©é˜µè¿›è¡Œä¸­æ€§åŒ–
    if let Some(regression_matrix) = &day_data.regression_matrix {
        // ä»é¢„è®¡ç®—çš„å›å½’çŸ©é˜µä¸­æå–å¯¹åº”çš„åˆ—
        let mut selected_regression_cols = Vec::with_capacity(valid_style_indices.len());
        for &style_idx in &valid_style_indices {
            selected_regression_cols.push(regression_matrix.column(style_idx).clone_owned());
        }
        
        let selected_regression_matrix = DMatrix::from_columns(&selected_regression_cols);
        let aligned_y_vector = DVector::from_vec(ranked_values.clone());
        
        // è®¡ç®—å›å½’ç³»æ•°
        let beta = &selected_regression_matrix * &aligned_y_vector;
        
        // è®¡ç®—é¢„æµ‹å€¼å’Œæ®‹å·®
        let mut result_values = Vec::new();
        for (i, &union_idx) in valid_union_indices.iter().enumerate() {
            let style_idx = valid_style_indices[i];
            
            // è®¡ç®—é¢„æµ‹å€¼ï¼šstyle_factors * beta
            let mut predicted_value = 0.0;
            for j in 0..12 {
                predicted_value += day_data.style_matrix[(style_idx, j)] * beta[j];
            }
            
            let residual = ranked_values[i] - predicted_value;
            result_values.push((union_idx, residual));
        }
        
        Ok(result_values)
    } else {
        Ok(Vec::new())
    }
}

/// ä¼˜åŒ–çš„ç»“æœä¿å­˜å‡½æ•° - ä½¿ç”¨æ‰¹é‡å†™å…¥
fn save_neutralized_result_optimized(
    result: OptimizedNeutralizationResult,
    output_path: &Path,
) -> PyResult<()> {
    use arrow::array::{ArrayRef, Int64Array, Float64Array};
    use arrow::datatypes::{DataType, Field, Schema};
    use arrow::record_batch::RecordBatch;
    use parquet::arrow::ArrowWriter;
    use parquet::file::properties::WriterProperties;
    use parquet::basic::{Compression, Encoding};
    
    // æ„å»ºSchema
    let mut fields = vec![Field::new("date", DataType::Int64, false)];
    for stock in &result.stocks {
        fields.push(Field::new(stock, DataType::Float64, true));
    }
    let schema = Arc::new(Schema::new(fields));
    
    // æ„å»ºæ•°æ®æ•°ç»„ - ä½¿ç”¨å‘é‡åŒ–æ“ä½œ
    let mut arrays: Vec<ArrayRef> = Vec::with_capacity(result.stocks.len() + 1);
    
    // æ—¥æœŸæ•°ç»„
    arrays.push(Arc::new(Int64Array::from(result.dates.clone())));
    
    // å¹¶è¡Œæ„å»ºè‚¡ç¥¨æ•°æ®æ•°ç»„ä»¥æé«˜æ€§èƒ½
    use rayon::prelude::*;
    let stock_arrays: Vec<ArrayRef> = (0..result.stocks.len())
        .into_par_iter()
        .map(|stock_idx| {
            let column_data: Vec<Option<f64>> = (0..result.dates.len())
                .map(|date_idx| {
                    let value = result.neutralized_values[(date_idx, stock_idx)];
                    if value.is_nan() {
                        None
                    } else {
                        Some(value)
                    }
                })
                .collect();
            Arc::new(Float64Array::from(column_data)) as ArrayRef
        })
        .collect();
    
    arrays.extend(stock_arrays);
    
    // åˆ›å»ºRecordBatch
    let batch = RecordBatch::try_new(schema.clone(), arrays)
        .map_err(|e| PyRuntimeError::new_err(format!("åˆ›å»ºRecordBatchå¤±è´¥: {}", e)))?;
    
    // ä½¿ç”¨ä¼˜åŒ–çš„å†™å…¥å‚æ•°
    let props = WriterProperties::builder()
        .set_compression(Compression::SNAPPY) // ä½¿ç”¨æ›´å¿«çš„å‹ç¼©
        .set_encoding(Encoding::PLAIN) // ä½¿ç”¨PLAINç¼–ç æé«˜å†™å…¥é€Ÿåº¦
        .set_max_row_group_size(100000) // ä¼˜åŒ–è¡Œç»„å¤§å°
        .build();
    
    let file = File::create(output_path)
        .map_err(|e| PyRuntimeError::new_err(format!("åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤±è´¥: {}", e)))?;
    
    let mut writer = ArrowWriter::try_new(file, schema, Some(props))
        .map_err(|e| PyRuntimeError::new_err(format!("åˆ›å»ºArrowå†™å…¥å™¨å¤±è´¥: {}", e)))?;
    
    writer.write(&batch)
        .map_err(|e| PyRuntimeError::new_err(format!("å†™å…¥æ•°æ®å¤±è´¥: {}", e)))?;
    
    writer.close()
        .map_err(|e| PyRuntimeError::new_err(format!("å…³é—­å†™å…¥å™¨å¤±è´¥: {}", e)))?;
    
    Ok(())
}

/// ä¼˜åŒ–çš„æ‰¹é‡å› å­ä¸­æ€§åŒ–å‡½æ•° - ä¸»å…¥å£
#[pyfunction]
pub fn batch_factor_neutralization_optimized(
    style_data_path: &str,
    factor_files_dir: &str,
    output_dir: &str,
    num_threads: Option<usize>,
) -> PyResult<()> {
    let start_time = Instant::now();
    println!("ğŸš€ å¼€å§‹ä¼˜åŒ–ç‰ˆæ‰¹é‡å› å­ä¸­æ€§åŒ–å¤„ç†...");

    // ä½¿ç”¨ä¼˜åŒ–ç‰ˆæœ¬åŠ è½½é£æ ¼æ•°æ®
    println!("ğŸ“– æ­£åœ¨ä½¿ç”¨å†…å­˜æ˜ å°„åŠ è½½é£æ ¼æ•°æ®...");
    let style_data = Arc::new(OptimizedStyleData::load_from_parquet_optimized(style_data_path)?);

    // è·å–æ‰€æœ‰å› å­æ–‡ä»¶
    let factor_dir = Path::new(factor_files_dir);
    let mut factor_files: Vec<PathBuf> = fs::read_dir(factor_dir)
        .map_err(|e| PyRuntimeError::new_err(format!("è¯»å–å› å­ç›®å½•å¤±è´¥: {}", e)))?
        .filter_map(|entry| {
            let entry = entry.ok()?;
            let path = entry.path();
            if path.extension().and_then(|s| s.to_str()) == Some("parquet") {
                Some(path)
            } else {
                None
            }
        })
        .collect();

    factor_files.sort_unstable();
    let total_files = factor_files.len();
    println!("ğŸ“ æ‰¾åˆ°{}ä¸ªå› å­æ–‡ä»¶", total_files);

    if total_files == 0 {
        return Err(PyRuntimeError::new_err("æœªæ‰¾åˆ°ä»»ä½•parquetå› å­æ–‡ä»¶"));
    }

    // åˆ›å»ºè¾“å‡ºç›®å½•
    fs::create_dir_all(output_dir)
        .map_err(|e| PyRuntimeError::new_err(format!("åˆ›å»ºè¾“å‡ºç›®å½•å¤±è´¥: {}", e)))?;

    // ä¼˜åŒ–çº¿ç¨‹æ± é…ç½®
    let optimal_threads = if let Some(threads) = num_threads {
        threads
    } else {
        // è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜çº¿ç¨‹æ•°ï¼šCPUæ ¸å¿ƒæ•°å’Œæ–‡ä»¶æ•°çš„è¾ƒå°å€¼
        std::cmp::min(rayon::current_num_threads(), total_files)
    };
    
    println!("âš¡ ä½¿ç”¨{}ä¸ªçº¿ç¨‹è¿›è¡Œå¹¶è¡Œå¤„ç†", optimal_threads);

    // åˆ›å»ºä¼˜åŒ–çš„çº¿ç¨‹æ± 
    let pool = rayon::ThreadPoolBuilder::new()
        .num_threads(optimal_threads)
        .thread_name(|index| format!("neutralization-worker-{}", index))
        .build()
        .map_err(|e| PyRuntimeError::new_err(format!("åˆ›å»ºçº¿ç¨‹æ± å¤±è´¥: {}", e)))?;

    // ä½¿ç”¨ä¼˜åŒ–ç‰ˆæœ¬å¹¶è¡Œå¤„ç†æ‰€æœ‰æ–‡ä»¶
    let results: Vec<_> = pool.install(|| {
        factor_files
            .into_par_iter()
            .map(|file_path| {
                let style_data = Arc::clone(&style_data);
                let output_dir = Path::new(output_dir);
                let file_start_time = Instant::now();

                let result = (|| -> PyResult<()> {
                    // ä½¿ç”¨ä¼˜åŒ–ç‰ˆæœ¬åŠ è½½å› å­æ•°æ®
                    let factor_data = load_factor_file_optimized(&file_path)?;

                    // ä½¿ç”¨ä¼˜åŒ–ç‰ˆæœ¬æ‰§è¡Œä¸­æ€§åŒ–
                    let neutralized_result = neutralize_single_factor_optimized(factor_data, &style_data)?;

                    // æ„å»ºè¾“å‡ºæ–‡ä»¶è·¯å¾„
                    let output_filename = file_path.file_name()
                        .ok_or_else(|| PyRuntimeError::new_err("æ— æ•ˆçš„æ–‡ä»¶å"))?;
                    let output_path = output_dir.join(output_filename);

                    // ä½¿ç”¨ä¼˜åŒ–ç‰ˆæœ¬ä¿å­˜ç»“æœ
                    save_neutralized_result_optimized(neutralized_result, &output_path)?;

                    Ok(())
                })();

                let file_time = file_start_time.elapsed();
                if let Err(e) = &result {
                    eprintln!("âŒ å¤„ç†æ–‡ä»¶å¤±è´¥: {} (ç”¨æ—¶: {:.3}s)", file_path.display(), file_time.as_secs_f64());
                    eprintln!("   é”™è¯¯è¯¦æƒ…: {}", e);
                } else {
                    println!("âœ… å¤„ç†å®Œæˆ: {} (ç”¨æ—¶: {:.3}s)", 
                            file_path.file_name().unwrap().to_string_lossy(),
                            file_time.as_secs_f64());
                }

                result
            })
            .collect()
    });

    // ç»Ÿè®¡å¤„ç†ç»“æœ
    let success_count = results.iter().filter(|r| r.is_ok()).count();
    let error_count = results.len() - success_count;

    let total_time = start_time.elapsed();
    println!("\nğŸ‰ ä¼˜åŒ–ç‰ˆæ‰¹é‡å› å­ä¸­æ€§åŒ–å¤„ç†å®Œæˆ!");
    println!("{}", "=".repeat(50));
    println!("ğŸ“Š å¤„ç†ç»Ÿè®¡:");
    println!("   æ€»æ–‡ä»¶æ•°: {}", total_files);
    println!("   æˆåŠŸå¤„ç†: {} ({:.1}%)", success_count, success_count as f64 / total_files as f64 * 100.0);
    println!("   å¤±è´¥æ–‡ä»¶: {}", error_count);
    println!("   æ€»ç”¨æ—¶: {:.1}åˆ†é’Ÿ ({:.1}ç§’)", total_time.as_secs_f64() / 60.0, total_time.as_secs_f64());
    println!("   å¹³å‡å¤„ç†é€Ÿåº¦: {:.1} æ–‡ä»¶/åˆ†é’Ÿ", total_files as f64 / (total_time.as_secs_f64() / 60.0));
    println!("   å¹³å‡å•æ–‡ä»¶ç”¨æ—¶: {:.3}ç§’", total_time.as_secs_f64() / total_files as f64);

    if error_count > 0 {
        println!("âš ï¸  è­¦å‘Š: {}ä¸ªæ–‡ä»¶å¤„ç†å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯æ—¥å¿—", error_count);
    }

    Ok(())
}