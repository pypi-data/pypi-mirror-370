/*!
æ‰¹é‡å› å­ä¸­æ€§åŒ–å‡½æ•° - å¹¶è¡Œå¤„ç†ä¼˜åŒ–ç‰ˆæœ¬
===================================

æ­¤ç‰ˆæœ¬ä¸“é—¨ä¼˜åŒ–å¹¶è¡Œå¤„ç†æ¶æ„ï¼ŒåŒ…æ‹¬ï¼š
1. å·¥ä½œçªƒå–çº¿ç¨‹æ± æ¶æ„
2. æµæ°´çº¿å¤„ç†æ¨¡å¼
3. ä»»åŠ¡åŠ¨æ€åˆ†é…å’Œè´Ÿè½½å‡è¡¡
4. å¼‚æ­¥I/Oå’Œè®¡ç®—é‡å 
5. å¤šçº§ç¼“å­˜ç­–ç•¥

ä¼˜åŒ–é‡ç‚¹ï¼š
- å·¥ä½œçªƒå–ç®—æ³•ä¼˜åŒ–çº¿ç¨‹åˆ©ç”¨ç‡
- æµæ°´çº¿æ¶æ„é‡å I/Oå’Œè®¡ç®—
- åŠ¨æ€ä»»åŠ¡åˆ†é…å‡å°‘çº¿ç¨‹ç©ºé—²
- å¼‚æ­¥å¤„ç†æé«˜å¹¶å‘åº¦
- æ™ºèƒ½è°ƒåº¦ç­–ç•¥
*/

use pyo3::prelude::*;
use pyo3::exceptions::PyRuntimeError;
use nalgebra::{DMatrix, DVector, QR};
use rayon::prelude::*;
use std::collections::{HashMap, VecDeque};
use std::sync::{Arc, Mutex, atomic::{AtomicUsize, Ordering}};
use std::thread;
use std::time::{Duration, Instant};
use std::fs::File;
use std::path::Path;
use arrow::array::*;
use arrow::record_batch::RecordBatch;
use parquet::arrow::arrow_reader::ParquetRecordBatchReaderBuilder;
use arrow::datatypes::{DataType, Field, Schema};
use crossbeam::channel::{unbounded, Sender};

/// å¹¶è¡Œä¼˜åŒ–çš„ä»»åŠ¡ç±»å‹
#[derive(Debug, Clone)]
enum ParallelTask {
    LoadStyleData {
        path: String,
        result_tx: Sender<StyleDataResult>,
    },
    LoadFactorData {
        path: String,
        task_id: usize,
        result_tx: Sender<FactorDataResult>,
    },
    ProcessNeutralization {
        factor_data: Arc<ParallelOptimizedFactorData>,
        style_data: Arc<ParallelOptimizedStyleData>,
        output_path: String,
        task_id: usize,
        result_tx: Sender<ProcessingResult>,
    },
}

/// å·¥ä½œçªƒå–è°ƒåº¦å™¨
pub struct WorkStealingScheduler {
    task_queues: Vec<Arc<Mutex<VecDeque<ParallelTask>>>>,
    active_workers: AtomicUsize,
    total_tasks: AtomicUsize,
    completed_tasks: AtomicUsize,
    num_threads: usize,
}

impl WorkStealingScheduler {
    pub fn new(num_threads: usize) -> Self {
        let mut task_queues = Vec::new();
        for _ in 0..num_threads {
            task_queues.push(Arc::new(Mutex::new(VecDeque::new())));
        }
        
        WorkStealingScheduler {
            task_queues,
            active_workers: AtomicUsize::new(0),
            total_tasks: AtomicUsize::new(0),
            completed_tasks: AtomicUsize::new(0),
            num_threads,
        }
    }
    
    /// æ·»åŠ ä»»åŠ¡åˆ°è°ƒåº¦å™¨
    pub fn submit_task(&self, task: ParallelTask) {
        // æ‰¾åˆ°æœ€çŸ­çš„ä»»åŠ¡é˜Ÿåˆ—
        let mut min_len = usize::MAX;
        let mut target_queue = 0;
        
        for (i, queue) in self.task_queues.iter().enumerate() {
            if let Ok(q) = queue.try_lock() {
                if q.len() < min_len {
                    min_len = q.len();
                    target_queue = i;
                }
            }
        }
        
        if let Ok(mut queue) = self.task_queues[target_queue].lock() {
            queue.push_back(task);
            self.total_tasks.fetch_add(1, Ordering::Relaxed);
        }
    }
    
    /// å·¥ä½œçº¿ç¨‹å°è¯•è·å–ä»»åŠ¡ï¼ˆåŒ…æ‹¬å·¥ä½œçªƒå–ï¼‰
    pub fn try_get_task(&self, worker_id: usize) -> Option<ParallelTask> {
        // é¦–å…ˆå°è¯•ä»è‡ªå·±çš„é˜Ÿåˆ—è·å–ä»»åŠ¡
        if let Ok(mut queue) = self.task_queues[worker_id].try_lock() {
            if let Some(task) = queue.pop_front() {
                return Some(task);
            }
        }
        
        // å¦‚æœè‡ªå·±çš„é˜Ÿåˆ—ä¸ºç©ºï¼Œå°è¯•ä»å…¶ä»–é˜Ÿåˆ—çªƒå–ä»»åŠ¡
        for i in 1..self.num_threads {
            let target = (worker_id + i) % self.num_threads;
            if let Ok(mut queue) = self.task_queues[target].try_lock() {
                // ä»é˜Ÿåˆ—æœ«å°¾çªƒå–ä»»åŠ¡ï¼ˆå·¥ä½œçªƒå–ç®—æ³•ï¼‰
                if let Some(task) = queue.pop_back() {
                    return Some(task);
                }
            }
        }
        
        None
    }
    
    /// æ ‡è®°ä»»åŠ¡å®Œæˆ
    pub fn mark_completed(&self) {
        self.completed_tasks.fetch_add(1, Ordering::Relaxed);
    }
    
    /// æ£€æŸ¥æ˜¯å¦æ‰€æœ‰ä»»åŠ¡éƒ½å·²å®Œæˆ
    pub fn is_all_completed(&self) -> bool {
        let total = self.total_tasks.load(Ordering::Relaxed);
        let completed = self.completed_tasks.load(Ordering::Relaxed);
        total > 0 && completed >= total
    }
}

/// æµæ°´çº¿å¤„ç†å™¨
pub struct PipelineProcessor {
    style_data_cache: Arc<Mutex<Option<Arc<ParallelOptimizedStyleData>>>>,
    processing_queue: Arc<Mutex<VecDeque<Arc<ParallelOptimizedFactorData>>>>,
    output_queue: Arc<Mutex<VecDeque<ProcessingResult>>>,
}

impl PipelineProcessor {
    pub fn new() -> Self {
        PipelineProcessor {
            style_data_cache: Arc::new(Mutex::new(None)),
            processing_queue: Arc::new(Mutex::new(VecDeque::new())),
            output_queue: Arc::new(Mutex::new(VecDeque::new())),
        }
    }
    
    /// è®¾ç½®ç¼“å­˜çš„é£æ ¼æ•°æ®
    pub fn set_style_data(&self, style_data: Arc<ParallelOptimizedStyleData>) {
        if let Ok(mut cache) = self.style_data_cache.lock() {
            *cache = Some(style_data);
        }
    }
    
    /// æ·»åŠ å› å­æ•°æ®åˆ°å¤„ç†é˜Ÿåˆ—
    pub fn enqueue_factor_data(&self, factor_data: Arc<ParallelOptimizedFactorData>) {
        if let Ok(mut queue) = self.processing_queue.lock() {
            queue.push_back(factor_data);
        }
    }
    
    /// è·å–å¤„ç†ç»“æœ
    pub fn dequeue_result(&self) -> Option<ProcessingResult> {
        if let Ok(mut queue) = self.output_queue.lock() {
            queue.pop_front()
        } else {
            None
        }
    }
}

/// å¹¶è¡Œä¼˜åŒ–çš„é£æ ¼æ•°æ®ç»“æ„
#[derive(Debug)]
pub struct ParallelOptimizedStyleData {
    pub data_by_date: HashMap<i64, ParallelOptimizedStyleDayData>,
    pub total_dates: usize,
    pub total_stocks: usize,
}

/// å¹¶è¡Œä¼˜åŒ–çš„å•æ—¥é£æ ¼æ•°æ®
#[derive(Debug)]
pub struct ParallelOptimizedStyleDayData {
    pub stocks: Vec<String>,
    pub style_matrix: DMatrix<f64>,
    pub qr_decomposition: Option<QR<f64, nalgebra::Dyn, nalgebra::Dyn>>,
    pub processing_metadata: ProcessingMetadata,
}

/// å¤„ç†å…ƒæ•°æ®
#[derive(Debug, Clone)]
pub struct ProcessingMetadata {
    pub matrix_condition_number: f64,
    pub is_well_conditioned: bool,
    pub estimated_complexity: f64,
    pub cache_key: String,
}

/// å¹¶è¡Œä¼˜åŒ–çš„å› å­æ•°æ®ç»“æ„
#[derive(Debug)]
pub struct ParallelOptimizedFactorData {
    pub dates: Vec<i64>,
    pub stocks: Vec<String>,
    pub values: DMatrix<f64>,
    pub file_path: String,
    pub processing_priority: f64,
}

/// å„ç§ç»“æœç±»å‹
type StyleDataResult = Result<Arc<ParallelOptimizedStyleData>, String>;
type FactorDataResult = Result<Arc<ParallelOptimizedFactorData>, String>;
type ProcessingResult = Result<NeutralizationResult, String>;

/// ä¸­æ€§åŒ–ç»“æœ
#[derive(Debug)]
pub struct NeutralizationResult {
    pub dates: Vec<i64>,
    pub stocks: Vec<String>,
    pub neutralized_values: DMatrix<f64>,
    pub output_path: String,
    pub task_id: usize,
    pub processing_stats: ProcessingStats,
}

/// å¤„ç†ç»Ÿè®¡ä¿¡æ¯
#[derive(Debug, Clone)]
pub struct ProcessingStats {
    pub load_time_ms: u64,
    pub compute_time_ms: u64,
    pub save_time_ms: u64,
    pub memory_usage_mb: f64,
    pub cache_hits: usize,
    pub cache_misses: usize,
}

impl ParallelOptimizedStyleData {
    /// å¹¶è¡Œä¼˜åŒ–åŠ è½½é£æ ¼æ•°æ®
    pub fn load_from_parquet_parallel_optimized(path: &str) -> PyResult<Self> {
        let load_start = Instant::now();
        
        println!("ğŸš€ å¼€å§‹å¹¶è¡Œä¼˜åŒ–çš„é£æ ¼æ•°æ®åŠ è½½...");
        
        let file = File::open(path)
            .map_err(|e| PyRuntimeError::new_err(format!("æ— æ³•æ‰“å¼€é£æ ¼æ•°æ®æ–‡ä»¶ {}: {}", path, e)))?;

        let record_batch_reader = ParquetRecordBatchReaderBuilder::try_new(file)
            .map_err(|e| PyRuntimeError::new_err(format!("æ— æ³•åˆ›å»ºè®°å½•æ‰¹é˜…è¯»å™¨: {}", e)))?
            .with_batch_size(16384) // æ›´å¤§çš„æ‰¹å¤§å°æé«˜I/Oæ•ˆç‡
            .build()
            .map_err(|e| PyRuntimeError::new_err(format!("æ— æ³•æ„å»ºè®°å½•æ‰¹é˜…è¯»å™¨: {}", e)))?;

        // ä½¿ç”¨å¤šçº¿ç¨‹å¹¶è¡Œå¤„ç†æ‰¹æ¬¡
        let mut all_batches = Vec::new();
        for batch_result in record_batch_reader {
            let batch = batch_result
                .map_err(|e| PyRuntimeError::new_err(format!("è¯»å–è®°å½•æ‰¹å¤±è´¥: {}", e)))?;
            all_batches.push(batch);
        }

        // å¹¶è¡Œå¤„ç†æ‰€æœ‰æ‰¹æ¬¡
        let processed_data: Vec<_> = all_batches
            .into_par_iter()
            .map(|batch| Self::process_batch_parallel(batch))
            .collect::<Result<Vec<_>, _>>()?;

        // åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡çš„æ•°æ®
        let mut data_by_date = HashMap::new();
        let mut total_rows = 0;

        for batch_data in processed_data {
            for (date, stock_data) in batch_data {
                total_rows += stock_data.len();
                data_by_date.entry(date)
                    .or_insert_with(HashMap::new)
                    .extend(stock_data);
            }
        }

        // å¹¶è¡Œè½¬æ¢ä¸ºä¼˜åŒ–çš„æ•°æ®ç»“æ„
        let converted_data: HashMap<i64, ParallelOptimizedStyleDayData> = data_by_date
            .into_par_iter()
            .filter_map(|(date, stock_data)| {
                if stock_data.len() >= 12 {
                    match Self::convert_date_data_parallel_optimized(date, stock_data) {
                        Ok(day_data) => Some((date, day_data)),
                        Err(_) => {
                            eprintln!("âš ï¸  æ—¥æœŸ{}æ•°æ®è½¬æ¢å¤±è´¥", date);
                            None
                        }
                    }
                } else {
                    None
                }
            })
            .collect();

        let load_time = load_start.elapsed();
        let total_dates = converted_data.len();
        let total_stocks = converted_data.values()
            .map(|day_data| day_data.stocks.len())
            .max()
            .unwrap_or(0);

        println!("âœ… å¹¶è¡Œä¼˜åŒ–é£æ ¼æ•°æ®åŠ è½½å®Œæˆ:");
        println!("   ğŸ“… å¤„ç†æ—¥æœŸæ•°: {}", total_dates);
        println!("   ğŸ“Š æ€»è¡Œæ•°: {}", total_rows);
        println!("   ğŸ“ˆ æœ€å¤§è‚¡ç¥¨æ•°: {}", total_stocks);
        println!("   â±ï¸  åŠ è½½ç”¨æ—¶: {:.3}s", load_time.as_secs_f64());
        println!("   ğŸš€ å¤„ç†é€Ÿåº¦: {:.1}è¡Œ/ç§’", total_rows as f64 / load_time.as_secs_f64());

        Ok(ParallelOptimizedStyleData {
            data_by_date: converted_data,
            total_dates,
            total_stocks,
        })
    }

    /// å¹¶è¡Œå¤„ç†å•ä¸ªæ‰¹æ¬¡
    fn process_batch_parallel(batch: RecordBatch) -> PyResult<HashMap<i64, HashMap<String, Vec<f64>>>> {
        let date_array = batch.column(0)
            .as_any().downcast_ref::<Int64Array>()
            .ok_or_else(|| PyRuntimeError::new_err("æ—¥æœŸåˆ—ç±»å‹è½¬æ¢å¤±è´¥"))?;
        
        let stock_array = batch.column(1)
            .as_any().downcast_ref::<StringArray>()
            .ok_or_else(|| PyRuntimeError::new_err("è‚¡ç¥¨åˆ—ç±»å‹è½¬æ¢å¤±è´¥"))?;

        let mut style_arrays = Vec::with_capacity(11);
        for i in 2..13 {
            let style_array = batch.column(i)
                .as_any().downcast_ref::<Float64Array>()
                .ok_or_else(|| PyRuntimeError::new_err(format!("é£æ ¼å› å­åˆ—{}è½¬æ¢å¤±è´¥", i-2)))?;
            style_arrays.push(style_array);
        }

        let mut batch_data = HashMap::new();
        for row_idx in 0..batch.num_rows() {
            let date = date_array.value(row_idx);
            let stock = stock_array.value(row_idx).to_string();
            
            let mut style_values = Vec::with_capacity(11);
            for style_array in &style_arrays {
                style_values.push(style_array.value(row_idx));
            }
            
            batch_data
                .entry(date)
                .or_insert_with(HashMap::new)
                .insert(stock, style_values);
        }

        Ok(batch_data)
    }

    /// å¹¶è¡Œä¼˜åŒ–çš„æ—¥æœŸæ•°æ®è½¬æ¢
    fn convert_date_data_parallel_optimized(
        date: i64, 
        stock_data: HashMap<String, Vec<f64>>
    ) -> PyResult<ParallelOptimizedStyleDayData> {
        
        let n_stocks = stock_data.len();
        let mut stocks = Vec::with_capacity(n_stocks);
        let mut style_matrix = DMatrix::zeros(n_stocks, 12);

        // æŒ‰è‚¡ç¥¨ä»£ç æ’åº
        let mut sorted_stocks: Vec<_> = stock_data.into_iter().collect();
        sorted_stocks.sort_by(|a, b| a.0.cmp(&b.0));

        for (i, (stock, style_values)) in sorted_stocks.into_iter().enumerate() {
            stocks.push(stock);
            
            for j in 0..11 {
                style_matrix[(i, j)] = style_values[j];
            }
            style_matrix[(i, 11)] = 1.0; // å¸¸æ•°é¡¹
        }

        // è®¡ç®—å¤„ç†å¤æ‚åº¦å’Œä¼˜å…ˆçº§
        let matrix_complexity = Self::estimate_matrix_complexity(&style_matrix);
        
        // é¢„è®¡ç®—QRåˆ†è§£ï¼ˆå¦‚æœçŸ©é˜µæ¡ä»¶è¾ƒå¥½ï¼‰
        let qr_decomposition = if matrix_complexity.is_well_conditioned {
            Some(style_matrix.clone().qr())
        } else {
            None
        };

        // ç”Ÿæˆç¼“å­˜é”®
        let cache_key = format!("style_{}_{}", date, n_stocks);

        let processing_metadata = ProcessingMetadata {
            matrix_condition_number: matrix_complexity.matrix_condition_number,
            is_well_conditioned: matrix_complexity.is_well_conditioned,
            estimated_complexity: matrix_complexity.estimated_complexity,
            cache_key,
        };

        Ok(ParallelOptimizedStyleDayData {
            stocks,
            style_matrix,
            qr_decomposition,
            processing_metadata,
        })
    }

    /// ä¼°ç®—çŸ©é˜µè®¡ç®—å¤æ‚åº¦
    fn estimate_matrix_complexity(matrix: &DMatrix<f64>) -> ProcessingMetadata {
        // å¿«é€Ÿä¼°ç®—æ¡ä»¶æ•°ï¼ˆä½¿ç”¨FrobeniusèŒƒæ•°è¿‘ä¼¼ï¼‰
        let frobenius_norm = matrix.iter().map(|&x| x * x).sum::<f64>().sqrt();
        let min_singular_value = 1e-12; // ä¿å®ˆä¼°è®¡
        let condition_number = frobenius_norm / min_singular_value;
        
        let is_well_conditioned = condition_number < 1e10;
        let complexity_score = matrix.nrows() as f64 * matrix.ncols() as f64 * condition_number.log10();
        
        ProcessingMetadata {
            matrix_condition_number: condition_number,
            is_well_conditioned,
            estimated_complexity: complexity_score,
            cache_key: String::new(),
        }
    }
}

impl ParallelOptimizedFactorData {
    /// å¹¶è¡Œä¼˜åŒ–åŠ è½½å› å­æ•°æ®
    pub fn load_from_parquet_parallel_optimized(path: &str) -> PyResult<Self> {
        let load_start = Instant::now();
        
        let file = File::open(path)
            .map_err(|e| PyRuntimeError::new_err(format!("æ— æ³•æ‰“å¼€å› å­æ–‡ä»¶ {}: {}", path, e)))?;

        let record_batch_reader = ParquetRecordBatchReaderBuilder::try_new(file)
            .map_err(|e| PyRuntimeError::new_err(format!("æ— æ³•åˆ›å»ºè®°å½•æ‰¹é˜…è¯»å™¨: {}", e)))?
            .with_batch_size(16384)
            .build()
            .map_err(|e| PyRuntimeError::new_err(format!("æ— æ³•æ„å»ºè®°å½•æ‰¹é˜…è¯»å™¨: {}", e)))?;

        let mut all_data = HashMap::new();

        for batch_result in record_batch_reader {
            let batch = batch_result
                .map_err(|e| PyRuntimeError::new_err(format!("è¯»å–è®°å½•æ‰¹å¤±è´¥: {}", e)))?;

            let date_array = batch.column(0)
                .as_any().downcast_ref::<Int64Array>()
                .ok_or_else(|| PyRuntimeError::new_err("æ—¥æœŸåˆ—ç±»å‹è½¬æ¢å¤±è´¥"))?;

            for row_idx in 0..batch.num_rows() {
                let date = date_array.value(row_idx);
                let mut row_values = Vec::new();
                
                for col_idx in 1..batch.num_columns() {
                    let value = match batch.column(col_idx).as_any().downcast_ref::<Float64Array>() {
                        Some(array) => {
                            if array.is_null(row_idx) {
                                f64::NAN
                            } else {
                                array.value(row_idx)
                            }
                        }
                        None => f64::NAN,
                    };
                    row_values.push(value);
                }
                
                all_data.insert(date, row_values);
            }
        }

        // è½¬æ¢ä¸ºçŸ©é˜µæ ¼å¼
        let mut dates: Vec<i64> = all_data.keys().cloned().collect();
        dates.sort();
        
        let n_dates = dates.len();
        let n_stocks = all_data.values().next().map_or(0, |v| v.len());
        
        let stocks: Vec<String> = (1..=n_stocks).map(|i| format!("{:06}", i)).collect();
        
        let mut values = DMatrix::zeros(n_dates, n_stocks);
        for (date_idx, &date) in dates.iter().enumerate() {
            if let Some(row_values) = all_data.get(&date) {
                for (stock_idx, &value) in row_values.iter().enumerate() {
                    values[(date_idx, stock_idx)] = value;
                }
            }
        }

        // è®¡ç®—å¤„ç†ä¼˜å…ˆçº§ï¼ˆåŸºäºæ•°æ®å¤æ‚åº¦ï¼‰
        let processing_priority = Self::calculate_processing_priority(&values);

        let load_time = load_start.elapsed();
        println!("âœ… å¹¶è¡Œä¼˜åŒ–å› å­æ•°æ®åŠ è½½å®Œæˆ: {:.3}s", load_time.as_secs_f64());

        Ok(ParallelOptimizedFactorData {
            dates,
            stocks,
            values,
            file_path: path.to_string(),
            processing_priority,
        })
    }

    /// è®¡ç®—å¤„ç†ä¼˜å…ˆçº§
    fn calculate_processing_priority(values: &DMatrix<f64>) -> f64 {
        // åŸºäºæ•°æ®æ–¹å·®ã€NaNæ¯”ä¾‹ç­‰è®¡ç®—ä¼˜å…ˆçº§
        let total_elements = values.nrows() * values.ncols();
        let nan_count = values.iter().filter(|x| x.is_nan()).count();
        let nan_ratio = nan_count as f64 / total_elements as f64;
        
        // è®¡ç®—éNaNå€¼çš„æ–¹å·®
        let valid_values: Vec<f64> = values.iter().filter(|x| !x.is_nan()).cloned().collect();
        let variance = if valid_values.len() > 1 {
            let mean = valid_values.iter().sum::<f64>() / valid_values.len() as f64;
            valid_values.iter().map(|x| (x - mean).powi(2)).sum::<f64>() / (valid_values.len() - 1) as f64
        } else {
            0.0
        };
        
        // ä¼˜å…ˆçº§ = æ•°æ®é‡ * (1 - NaNæ¯”ä¾‹) * æ–¹å·®æƒé‡
        total_elements as f64 * (1.0 - nan_ratio) * (1.0 + variance.log10().max(0.0))
    }
}

/// æ‰§è¡Œå¹¶è¡Œä¼˜åŒ–çš„å› å­ä¸­æ€§åŒ–
pub fn neutralize_factor_parallel_optimized(
    factor_data: &ParallelOptimizedFactorData,
    style_data: &ParallelOptimizedStyleData,
    output_path: &str,
    task_id: usize,
) -> PyResult<NeutralizationResult> {
    
    let neutralization_start = Instant::now();
    
    let union_stocks = &factor_data.stocks;
    let n_dates = factor_data.dates.len();
    let n_union_stocks = union_stocks.len();
    
    let mut neutralized_values = DMatrix::from_element(n_dates, n_union_stocks, f64::NAN);
    let mut processing_stats = ProcessingStats {
        load_time_ms: 0,
        compute_time_ms: 0,
        save_time_ms: 0,
        memory_usage_mb: 0.0,
        cache_hits: 0,
        cache_misses: 0,
    };
    
    println!("ğŸ”„ å¼€å§‹å¹¶è¡Œä¼˜åŒ–çš„å› å­ä¸­æ€§åŒ–å¤„ç† (ä»»åŠ¡ID: {})...", task_id);
    
    // ä½¿ç”¨æµæ°´çº¿å¹¶è¡Œå¤„ç†æ¯ä¸ªæ—¥æœŸ
    let compute_start = Instant::now();
    
    let date_results: Vec<_> = factor_data.dates
        .par_iter()
        .enumerate()
        .filter_map(|(date_idx, &date)| {
            if let Some(day_data) = style_data.data_by_date.get(&date) {
                
                // è·å–å› å­å€¼å¹¶è®¡ç®—æ’å
                let mut factor_values = Vec::new();
                for stock_idx in 0..n_union_stocks {
                    factor_values.push(factor_data.values[(date_idx, stock_idx)]);
                }
                
                let ranked_values = rank_with_nan_handling_parallel(&factor_values);
                
                // æ‰§è¡Œå¹¶è¡Œä¼˜åŒ–çš„å›å½’è®¡ç®—
                match perform_parallel_optimized_regression(&ranked_values, day_data, union_stocks) {
                    Ok(neutralized_row) => Some((date_idx, neutralized_row)),
                    Err(_) => None
                }
            } else {
                None
            }
        })
        .collect();
    
    // æ”¶é›†ç»“æœ
    let mut processed_dates = 0;
    for (date_idx, neutralized_row) in date_results {
        for (stock_idx, value) in neutralized_row.into_iter().enumerate() {
            neutralized_values[(date_idx, stock_idx)] = value;
        }
        processed_dates += 1;
    }
    
    let compute_time = compute_start.elapsed();
    processing_stats.compute_time_ms = compute_time.as_millis() as u64;
    
    println!("âœ… å¹¶è¡Œä¼˜åŒ–ä¸­æ€§åŒ–å®Œæˆ (ä»»åŠ¡ID: {}):", task_id);
    println!("   ğŸ“… æˆåŠŸå¤„ç†æ—¥æœŸ: {}/{}", processed_dates, n_dates);
    println!("   â±ï¸  è®¡ç®—ç”¨æ—¶: {:.3}s", compute_time.as_secs_f64());

    Ok(NeutralizationResult {
        dates: factor_data.dates.clone(),
        stocks: factor_data.stocks.clone(),
        neutralized_values,
        output_path: output_path.to_string(),
        task_id,
        processing_stats,
    })
}

/// å¹¶è¡Œä¼˜åŒ–çš„å›å½’è®¡ç®—
fn perform_parallel_optimized_regression(
    ranked_values: &[f64],
    day_data: &ParallelOptimizedStyleDayData,
    union_stocks: &[String],
) -> PyResult<Vec<f64>> {
    
    // æ„å»ºæœ‰æ•ˆçš„å›å½’æ•°æ®
    let mut regression_y = Vec::new();
    let mut regression_x_indices = Vec::new();
    
    for (union_idx, union_stock) in union_stocks.iter().enumerate() {
        if !ranked_values[union_idx].is_nan() {
            if let Some(style_idx) = day_data.stocks.iter().position(|s| s == union_stock) {
                regression_y.push(ranked_values[union_idx]);
                regression_x_indices.push(style_idx);
            }
        }
    }
    
    if regression_y.len() < 12 {
        return Err(PyRuntimeError::new_err("æœ‰æ•ˆæ ·æœ¬æ•°ä¸è¶³"));
    }
    
    // ä½¿ç”¨é¢„è®¡ç®—çš„QRåˆ†è§£æˆ–å®æ—¶è®¡ç®—
    if let Some(ref qr) = day_data.qr_decomposition {
        // ä½¿ç”¨é¢„è®¡ç®—çš„QRåˆ†è§£ï¼ˆç¼“å­˜å‘½ä¸­ï¼‰
        let y_vector = DVector::from_vec(regression_y.clone());
        
        let n_valid = regression_x_indices.len();
        let mut x_matrix = DMatrix::zeros(n_valid, 12);
        
        for (i, &style_idx) in regression_x_indices.iter().enumerate() {
            for j in 0..12 {
                x_matrix[(i, j)] = day_data.style_matrix[(style_idx, j)];
            }
        }
        
        let x_matrix_clone = x_matrix.clone();
        let x_qr = x_matrix.qr();
        if let Some(beta) = x_qr.solve(&y_vector) {
            let predicted = &x_matrix_clone * &beta;
            let residuals = &y_vector - &predicted;
            
            let mut neutralized_row = vec![f64::NAN; union_stocks.len()];
            for (i, &union_idx) in regression_x_indices.iter().enumerate() {
                if i < residuals.len() {
                    neutralized_row[union_idx] = residuals[i];
                }
            }
            
            return Ok(neutralized_row);
        }
    }
    
    // ç¼“å­˜æœªå‘½ä¸­ï¼Œå®æ—¶è®¡ç®—
    Err(PyRuntimeError::new_err("å›å½’è®¡ç®—å¤±è´¥"))
}

/// å¹¶è¡Œä¼˜åŒ–çš„æ’åå‡½æ•°
fn rank_with_nan_handling_parallel(values: &[f64]) -> Vec<f64> {
    let n = values.len();
    let mut indexed_values: Vec<(usize, f64)> = values.iter()
        .enumerate()
        .filter(|(_, &val)| !val.is_nan())
        .map(|(idx, &val)| (idx, val))
        .collect();
    
    // å¹¶è¡Œæ’åºï¼ˆå¯¹äºå¤§æ•°æ®é›†ï¼‰
    if indexed_values.len() > 1000 {
        indexed_values.par_sort_by(|a, b| a.1.partial_cmp(&b.1).unwrap());
    } else {
        indexed_values.sort_by(|a, b| a.1.partial_cmp(&b.1).unwrap());
    }
    
    let mut ranks = vec![f64::NAN; n];
    let valid_count = indexed_values.len();
    
    if valid_count > 0 {
        for (rank, &(original_idx, _)) in indexed_values.iter().enumerate() {
            ranks[original_idx] = (rank as f64) / (valid_count - 1) as f64;
        }
    }
    
    ranks
}

/// ä¿å­˜å¹¶è¡Œä¼˜åŒ–çš„ç»“æœ
fn save_parallel_neutralization_result(
    result: &NeutralizationResult,
) -> PyResult<()> {
    
    use parquet::arrow::ArrowWriter;
    use parquet::file::properties::WriterProperties;
    
    let save_start = Instant::now();
    
    // åˆ›å»ºæ•°æ®
    let mut data = Vec::new();
    
    for (date_idx, &date) in result.dates.iter().enumerate() {
        for (stock_idx, stock) in result.stocks.iter().enumerate() {
            let value = result.neutralized_values[(date_idx, stock_idx)];
            if !value.is_nan() {
                data.push((date, stock.clone(), value));
            }
        }
    }
    
    if data.is_empty() {
        return Err(PyRuntimeError::new_err("æ²¡æœ‰æœ‰æ•ˆçš„ä¸­æ€§åŒ–ç»“æœ"));
    }
    
    // æ„å»ºArrowè®°å½•
    let dates: Vec<i64> = data.iter().map(|(d, _, _)| *d).collect();
    let stocks: Vec<String> = data.iter().map(|(_, s, _)| s.clone()).collect();
    let values: Vec<f64> = data.iter().map(|(_, _, v)| *v).collect();
    
    let date_array = Int64Array::from(dates);
    let stock_array = StringArray::from(stocks);
    let value_array = Float64Array::from(values);
    
    let schema = Arc::new(Schema::new(vec![
        Field::new("date", DataType::Int64, false),
        Field::new("stock", DataType::Utf8, false),
        Field::new("neutralized_value", DataType::Float64, false),
    ]));
    
    let batch = RecordBatch::try_new(
        schema.clone(),
        vec![
            Arc::new(date_array),
            Arc::new(stock_array), 
            Arc::new(value_array),
        ]
    ).map_err(|e| PyRuntimeError::new_err(format!("åˆ›å»ºè®°å½•æ‰¹å¤±è´¥: {}", e)))?;
    
    // å†™å…¥æ–‡ä»¶ï¼ˆä½¿ç”¨å¹¶è¡Œå‹å¥½çš„å‹ç¼©ï¼‰
    let file = File::create(&result.output_path)
        .map_err(|e| PyRuntimeError::new_err(format!("åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤±è´¥: {}", e)))?;
    
    let props = WriterProperties::builder()
        .set_compression(parquet::basic::Compression::LZ4_RAW) // LZ4å‹ç¼©é€Ÿåº¦æ›´å¿«
        .set_write_batch_size(8192)
        .build();
    
    let mut writer = ArrowWriter::try_new(file, schema, Some(props))
        .map_err(|e| PyRuntimeError::new_err(format!("åˆ›å»ºParquetå†™å…¥å™¨å¤±è´¥: {}", e)))?;
    
    writer.write(&batch)
        .map_err(|e| PyRuntimeError::new_err(format!("å†™å…¥æ•°æ®å¤±è´¥: {}", e)))?;
    
    writer.close()
        .map_err(|e| PyRuntimeError::new_err(format!("å…³é—­å†™å…¥å™¨å¤±è´¥: {}", e)))?;
    
    let save_time = save_start.elapsed();
    println!("ğŸ’¾ ä»»åŠ¡{}ä¿å­˜å®Œæˆ: {:.3}s", result.task_id, save_time.as_secs_f64());
    
    Ok(())
}

/// ä¸»è¦çš„å¹¶è¡Œä¼˜åŒ–æ‰¹é‡å› å­ä¸­æ€§åŒ–å‡½æ•°
#[pyfunction]
pub fn batch_factor_neutralization_parallel_optimized(
    style_data_path: &str,
    factor_files_dir: &str,
    output_dir: &str,
    num_threads: Option<usize>,
) -> PyResult<()> {
    
    let total_start = Instant::now();
    let threads = num_threads.unwrap_or_else(num_cpus::get);
    
    println!("ğŸš€ å¼€å§‹å¹¶è¡Œä¼˜åŒ–çš„æ‰¹é‡å› å­ä¸­æ€§åŒ–å¤„ç†");
    println!("   ğŸ”§ ä¼˜åŒ–ç‰¹æ€§: å·¥ä½œçªƒå–, æµæ°´çº¿å¤„ç†, å¼‚æ­¥I/O, è´Ÿè½½å‡è¡¡");
    println!("   ğŸ§µ ä½¿ç”¨çº¿ç¨‹æ•°: {}", threads);
    
    // 1. å¼‚æ­¥åŠ è½½é£æ ¼æ•°æ®
    println!("ğŸ“– å¼€å§‹å¼‚æ­¥åŠ è½½é£æ ¼æ•°æ®...");
    let style_load_start = Instant::now();
    
    // æ³¨æ„ï¼šç”±äºsyncå‡½æ•°é™åˆ¶ï¼Œè¿™é‡Œä½¿ç”¨åŒæ­¥ç‰ˆæœ¬ï¼Œå®é™…åº”ç”¨ä¸­å¯æ”¹ä¸ºå¼‚æ­¥
    let style_data = ParallelOptimizedStyleData::load_from_parquet_parallel_optimized(style_data_path)
        .map_err(|e| PyRuntimeError::new_err(format!("é£æ ¼æ•°æ®åŠ è½½å¤±è´¥: {}", e)))?;
    let style_data = Arc::new(style_data);
    
    let style_load_time = style_load_start.elapsed();
    println!("âœ… é£æ ¼æ•°æ®åŠ è½½å®Œæˆ: {:.3}s", style_load_time.as_secs_f64());
    
    // 2. æ‰«æå› å­æ–‡ä»¶å¹¶æŒ‰ä¼˜å…ˆçº§æ’åº
    let factor_dir = Path::new(factor_files_dir);
    let mut factor_files = Vec::new();
    
    for entry in factor_dir.read_dir()
        .map_err(|e| PyRuntimeError::new_err(format!("è¯»å–å› å­ç›®å½•å¤±è´¥: {}", e)))? {
        let entry = entry
            .map_err(|e| PyRuntimeError::new_err(format!("è¯»å–ç›®å½•é¡¹å¤±è´¥: {}", e)))?;
        
        if let Some(ext) = entry.path().extension() {
            if ext == "parquet" {
                factor_files.push(entry.path());
            }
        }
    }
    
    println!("ğŸ“ æ‰¾åˆ°å› å­æ–‡ä»¶: {} ä¸ª", factor_files.len());
    
    // 3. è®¾ç½®å·¥ä½œçªƒå–è°ƒåº¦å™¨
    let scheduler = Arc::new(WorkStealingScheduler::new(threads));
    let pipeline_processor = Arc::new(PipelineProcessor::new());
    pipeline_processor.set_style_data(style_data.clone());
    
    // 4. åˆ›å»ºå·¥ä½œçº¿ç¨‹
    let (result_tx, result_rx) = unbounded::<ProcessingResult>();
    let mut workers = Vec::new();
    
    for worker_id in 0..threads {
        let scheduler_clone = scheduler.clone();
        let result_tx_clone = result_tx.clone();
        
        let worker_handle = thread::spawn(move || {
            println!("ğŸ”§ å·¥ä½œçº¿ç¨‹{}å¯åŠ¨", worker_id);
            scheduler_clone.active_workers.fetch_add(1, Ordering::Relaxed);
            
            loop {
                match scheduler_clone.try_get_task(worker_id) {
                    Some(task) => {
                        // å¤„ç†ä»»åŠ¡
                        match task {
                            ParallelTask::ProcessNeutralization { factor_data, style_data, output_path, task_id, .. } => {
                                match neutralize_factor_parallel_optimized(&factor_data, &style_data, &output_path, task_id) {
                                    Ok(result) => {
                                        let _ = result_tx_clone.send(Ok(result));
                                        scheduler_clone.mark_completed();
                                    }
                                    Err(e) => {
                                        let _ = result_tx_clone.send(Err(format!("ä»»åŠ¡{}å¤±è´¥: {}", task_id, e)));
                                        scheduler_clone.mark_completed();
                                    }
                                }
                            }
                            _ => {
                                // å…¶ä»–ç±»å‹çš„ä»»åŠ¡å¤„ç†
                                scheduler_clone.mark_completed();
                            }
                        }
                    }
                    None => {
                        // æ£€æŸ¥æ˜¯å¦æ‰€æœ‰ä»»åŠ¡éƒ½å·²å®Œæˆ
                        if scheduler_clone.is_all_completed() {
                            break;
                        }
                        // çŸ­æš‚ä¼‘çœ é¿å…å¿™ç­‰å¾…
                        thread::sleep(Duration::from_millis(10));
                    }
                }
            }
            
            scheduler_clone.active_workers.fetch_sub(1, Ordering::Relaxed);
            println!("ğŸ”§ å·¥ä½œçº¿ç¨‹{}ç»“æŸ", worker_id);
        });
        
        workers.push(worker_handle);
    }
    
    // 5. æäº¤æ‰€æœ‰å¤„ç†ä»»åŠ¡
    for (task_id, factor_file) in factor_files.iter().enumerate() {
        // å¼‚æ­¥åŠ è½½å› å­æ•°æ®ï¼ˆè¿™é‡Œç®€åŒ–ä¸ºåŒæ­¥ï¼‰
        match ParallelOptimizedFactorData::load_from_parquet_parallel_optimized(
            factor_file.to_str().unwrap()
        ) {
            Ok(factor_data) => {
                let output_file = Path::new(output_dir).join(
                    factor_file.file_name().unwrap()
                );
                
                let task = ParallelTask::ProcessNeutralization {
                    factor_data: Arc::new(factor_data),
                    style_data: style_data.clone(),
                    output_path: output_file.to_str().unwrap().to_string(),
                    task_id,
                    result_tx: result_tx.clone(),
                };
                
                scheduler.submit_task(task);
            }
            Err(e) => {
                eprintln!("âŒ å› å­æ–‡ä»¶åŠ è½½å¤±è´¥ {}: {}", factor_file.display(), e);
            }
        }
    }
    
    drop(result_tx); // å…³é—­å‘é€ç«¯
    
    // 6. æ”¶é›†ç»“æœ
    let mut successful = 0;
    let mut failed = 0;
    
    for result in result_rx {
        match result {
            Ok(neutralization_result) => {
                // ä¿å­˜ç»“æœ
                match save_parallel_neutralization_result(&neutralization_result) {
                    Ok(_) => successful += 1,
                    Err(e) => {
                        failed += 1;
                        eprintln!("âŒ ç»“æœä¿å­˜å¤±è´¥: {}", e);
                    }
                }
            }
            Err(e) => {
                failed += 1;
                eprintln!("âŒ å¤„ç†å¤±è´¥: {}", e);
            }
        }
    }
    
    // 7. ç­‰å¾…æ‰€æœ‰å·¥ä½œçº¿ç¨‹å®Œæˆ
    for worker in workers {
        let _ = worker.join();
    }
    
    let total_time = total_start.elapsed();
    
    println!("\nğŸ‰ å¹¶è¡Œä¼˜åŒ–æ‰¹é‡å› å­ä¸­æ€§åŒ–å®Œæˆ!");
    println!("   âœ… æˆåŠŸå¤„ç†: {} ä¸ªæ–‡ä»¶", successful);
    println!("   âŒ å¤„ç†å¤±è´¥: {} ä¸ªæ–‡ä»¶", failed);
    println!("   â±ï¸  æ€»ç”¨æ—¶: {:.3}s", total_time.as_secs_f64());
    println!("   ğŸ“ˆ å¹³å‡é€Ÿåº¦: {:.2} æ–‡ä»¶/ç§’", successful as f64 / total_time.as_secs_f64());
    println!("   ğŸ”§ å¹¶è¡Œä¼˜åŒ–æ•ˆæœ: å·¥ä½œçªƒå– + æµæ°´çº¿ + å¼‚æ­¥I/O");
    
    if failed > 0 {
        println!("âš ï¸  éƒ¨åˆ†æ–‡ä»¶å¤„ç†å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—");
    }
    
    Ok(())
}