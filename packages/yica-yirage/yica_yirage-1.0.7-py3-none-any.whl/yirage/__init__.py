"""
YICA-Yirage: AI Computing Optimization Framework for In-Memory Computing Architecture
"""

__version__ = "1.0.6"

# Try to import optional dependencies gracefully
try:
    import z3
    Z3_AVAILABLE = True
except ImportError:
    Z3_AVAILABLE = False

try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    NUMPY_AVAILABLE = False

# Core Python modules (always available)
# Import from new modular structure
try:
    from .core import __version__ as version_py_version, global_config
    # Verify versions match
    if __version__ != version_py_version:
        print(f"Warning: Version mismatch between __init__.py ({__version__}) and version.py ({version_py_version})")
        # Use version.py as the source of truth
        __version__ = version_py_version
except ImportError:
    # Keep using the version defined above if core module is not available
    try:
        from .core.global_config import global_config
    except ImportError:
        global_config = None

try:
    from .utils import graph_dataset, get_shared_memory_capacity
except ImportError:
    graph_dataset = None

# Import core module with error handling
try:
    from . import core
    # Import main optimizer classes
    from .core.optimizer import (
        Optimizer, 
        OptimizationConfig, 
        TransformerConfig,
        create_optimizer,
        optimize_model
    )
    YICA_CORE_AVAILABLE = True
except ImportError:
    YICA_CORE_AVAILABLE = False

# Import main modules with error handling
try:
    from .yica import yica_advanced
    YICA_ADVANCED_AVAILABLE = True
except ImportError:
    YICA_ADVANCED_AVAILABLE = False

try:
    from .profiling import yica_performance_monitor
    YICA_MONITOR_AVAILABLE = True
except ImportError:
    YICA_MONITOR_AVAILABLE = False

# å»¶è¿Ÿå¯¼å…¥ yica_real_optimizer é¿å…æ¨¡å—æ‰§è¡Œå†²çª
YICA_OPTIMIZER_AVAILABLE = True  # å‡è®¾å¯ç”¨ï¼Œå®é™…ä½¿ç”¨æ—¶å†æ£€æŸ¥

# Import API compatibility layer for source_codes
try:
    from .api_compat import (
        # æ ¸å¿ƒç±»
        Optimizer, OptimizationConfig, TransformerConfig,
        # è£…é¥°å™¨
        operator, cim_operator, fuse_operators, 
        graph_superoptimizer, memory_hierarchy_optimizer, 
        layer_fusion, compile,
        # åŸºç¡€æ“ä½œ
        tensor, zeros, zeros_like, matmul, relu,
        # YiRageå‡½æ•°
        optimize, list_targets, rmsnorm, fused_attention,
        # CIMç®—å­
        cim_tile_size, cim_compute, cim_matmul, cim_softmax,
        cim_masked_fill, get_rf_capacity, get_spm_capacity,
        cim_context,
        # é«˜çº§API
        layers,
        # ç‰ˆæœ¬ä¿¡æ¯
        version, is_available
    )
    API_AVAILABLE = True
    print("YiRage APIå…¼å®¹å±‚å·²åŠ è½½")
except ImportError as e:
    API_AVAILABLE = False
    print(f"è­¦å‘Š: APIå…¼å®¹å±‚åŠ è½½å¤±è´¥: {e}")
    # æä¾›åŸºç¡€å›é€€
    version = __version__
    def list_targets():
        return ["cpu"]

# Import other optional modules
optional_modules = [
    'yica_auto_tuner', 'yica_distributed', 'yica_llama_optimizer',
    'yica_pytorch_backend', 'visualizer', 'profiler', 'triton_profiler'
]

for module_name in optional_modules:
    try:
        __import__(f'{__name__}.{module_name}')
    except ImportError:
        pass  # Silently skip unavailable modules

# Main API functions
def create_yica_optimizer(config=None):
    """Create a YICA optimizer instance"""
    # Try core first
    if YICA_CORE_AVAILABLE:
        core_optimizer = core.get_yica_core(config).create_optimizer(config)
        if core_optimizer is not None:
            return core_optimizer

    # Fall back to real optimizer
    if YICA_OPTIMIZER_AVAILABLE:
        # Convert config to hardware config if needed
        if config is None:
            hardware_config = None
        elif hasattr(config, 'num_cim_arrays'):
            hardware_config = config
        else:
            # Convert dict config to YICAHardwareConfig
            if isinstance(config, dict):
                from .yica.yica_real_optimizer import YICAHardwareConfig
                hardware_config = YICAHardwareConfig(**config)
            else:
                hardware_config = None
        # å»¶è¿Ÿå¯¼å…¥é¿å…æ¨¡å—æ‰§è¡Œå†²çª
        from .yica.yica_real_optimizer import create_yica_real_optimizer
        return create_yica_real_optimizer(hardware_config)
    else:
        raise ImportError("Neither yica_core nor yica_optimizer module is available")

def quick_analyze(model_path, optimization_level="O2"):
    """Quick analysis of a model"""
    if not YICA_ADVANCED_AVAILABLE:
        raise ImportError("yica_advanced module is not available")
    return yica_advanced.quick_analyze(model_path, optimization_level)

def create_performance_monitor(config=None):
    """Create a performance monitor instance"""
    if not YICA_MONITOR_AVAILABLE:
        raise ImportError("yica_performance_monitor module is not available")
    return yica_performance_monitor.YICAPerformanceMonitor(config or {})

# Configuration
def set_gpu_device_id(device_id: int):
    """Set GPU device ID"""
    global_config.gpu_device_id = device_id

def bypass_compile_errors(value: bool = True):
    """Bypass compile errors for testing"""
    global_config.bypass_compile_errors = value

# Version and availability info
def get_version_info():
    """Get version and availability information"""
    return {
        "version": __version__,
        "z3_available": Z3_AVAILABLE,
        "torch_available": TORCH_AVAILABLE,
        "numpy_available": NUMPY_AVAILABLE,
        "yica_core_available": YICA_CORE_AVAILABLE,
        "yica_optimizer_available": YICA_OPTIMIZER_AVAILABLE,
        "yica_monitor_available": YICA_MONITOR_AVAILABLE,
        "yica_advanced_available": YICA_ADVANCED_AVAILABLE,
    }

# å…¼å®¹æ€§å‡½æ•°ï¼šå½“ Cython æ‰©å±•ä¸å¯ç”¨æ—¶çš„å¤‡ç”¨å®ç°
def new_kernel_graph():
    """
    åˆ›å»ºæ–°çš„å†…æ ¸å›¾

    ä¼˜å…ˆçº§ï¼š
    1. çœŸæ­£çš„YICAåç«¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    2. Cythonæ ¸å¿ƒå®ç°ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    3. å…¼å®¹æ€§å®ç°ï¼ˆåå¤‡ï¼‰
    """
    # 1. å°è¯•ä½¿ç”¨ Cython å®ç°
    try:
        from ._cython.core import new_kernel_graph as _new_kernel_graph
        print("ğŸš€ ä½¿ç”¨çœŸæ­£çš„YICA-Cythonåç«¯åˆ›å»ºè®¡ç®—å›¾")
        return _new_kernel_graph()
    except ImportError:
        pass  # ç»§ç»­å°è¯•å…¶ä»–æ–¹æ³•
    
    # 2. å°è¯•ä½¿ç”¨çœŸæ­£çš„YICAåç«¯ï¼ˆéœ€è¦åº•å±‚å›¾å¯¹è±¡ï¼‰
    if YICA_CORE_AVAILABLE:
        try:
            # å°è¯•åˆ›å»ºåº•å±‚å›¾å¯¹è±¡
            from ._cython.threadblock import new_tb_graph
            from .core.kernel import KNGraph
            print("ğŸš€ ä½¿ç”¨çœŸæ­£çš„YICAåç«¯åˆ›å»ºè®¡ç®—å›¾")
            tb_graph = new_tb_graph()
            return KNGraph(tb_graph)
        except ImportError:
            if global_config and global_config.verbose:
                print("âš ï¸  YICAåº•å±‚å›¾å¯¹è±¡ä¸å¯ç”¨ï¼Œä½¿ç”¨å…¼å®¹æ€§å®ç°")
        except Exception as e:
            if global_config and global_config.verbose:
                print(f"âš ï¸  YICAåç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
    
    # 3. ä½¿ç”¨å…¼å®¹æ€§å®ç°
        from .core.threadblock import TBGraph

        class KernelGraph:
            """å†…æ ¸å›¾å…¼å®¹æ€§å®ç°"""

            def __init__(self):
                self.inputs = []
                self.outputs = []
                self.operations = []
                self._input_counter = 0

            def new_input(self, dims, dtype):
                """åˆ›å»ºæ–°è¾“å…¥"""
                from .core.threadblock import DTensor

                # åˆ›å»ºè¾“å…¥å¼ é‡æè¿°
                input_tensor = DTensor()
                input_tensor.dims = dims
                input_tensor.dtype = dtype
                input_tensor.name = f"input_{self._input_counter}"
                self._input_counter += 1

                self.inputs.append(input_tensor)
                return input_tensor

            def rms_norm(self, input_tensor, normalized_shape):
                """RMSå½’ä¸€åŒ–æ“ä½œ"""
                from .core.threadblock import DTensor

                output = DTensor()
                output.dims = input_tensor.dims if hasattr(input_tensor, 'dims') else (1,)
                output.dtype = input_tensor.dtype if hasattr(input_tensor, 'dtype') else 'float16'
                output.name = f"rms_norm_output"

                self.operations.append({
                    'type': 'rms_norm',
                    'input': input_tensor,
                    'output': output,
                    'normalized_shape': normalized_shape
                })

                return output

            def matmul(self, a, b):
                """çŸ©é˜µä¹˜æ³•æ“ä½œ"""
                from .core.threadblock import DTensor

                output = DTensor()
                # ç®€åŒ–çš„ç»´åº¦è®¡ç®—
                if hasattr(a, 'dims') and hasattr(b, 'dims'):
                    a_dims = a.dims
                    b_dims = b.dims
                    if len(a_dims) >= 2 and len(b_dims) >= 2:
                        output.dims = a_dims[:-1] + (b_dims[-1],)
                    else:
                        output.dims = (1, 1)
                else:
                    output.dims = (1, 1)

                output.dtype = a.dtype if hasattr(a, 'dtype') else 'float16'
                output.name = f"matmul_output"

                self.operations.append({
                    'type': 'matmul',
                    'input_a': a,
                    'input_b': b,
                    'output': output
                })

                return output

            def add(self, a, b):
                """é€å…ƒç´ åŠ æ³•æ“ä½œ"""
                from .core.threadblock import DTensor

                output = DTensor()
                # å¹¿æ’­è§„åˆ™ç®€åŒ–å®ç°
                if hasattr(a, 'dims') and hasattr(b, 'dims'):
                    # ç®€å•å¹¿æ’­ï¼šé€‰æ‹©è¾ƒå¤§çš„ç»´åº¦
                    a_dims = a.dims
                    b_dims = b.dims
                    if len(a_dims) >= len(b_dims):
                        output.dims = a_dims
                    else:
                        output.dims = b_dims
                else:
                    output.dims = (1,)

                output.dtype = a.dtype if hasattr(a, 'dtype') else 'float16'
                output.name = f"add_output"

                self.operations.append({
                    'type': 'add',
                    'input_a': a,
                    'input_b': b,
                    'output': output
                })

                return output

            def mul(self, a, b):
                """é€å…ƒç´ ä¹˜æ³•æ“ä½œ"""
                from .core.threadblock import DTensor

                output = DTensor()
                # å¹¿æ’­è§„åˆ™ç®€åŒ–å®ç°
                if hasattr(a, 'dims') and hasattr(b, 'dims'):
                    a_dims = a.dims
                    b_dims = b.dims
                    if len(a_dims) >= len(b_dims):
                        output.dims = a_dims
                    else:
                        output.dims = b_dims
                else:
                    output.dims = (1,)

                output.dtype = a.dtype if hasattr(a, 'dtype') else 'float16'
                output.name = f"mul_output"

                self.operations.append({
                    'type': 'mul',
                    'input_a': a,
                    'input_b': b,
                    'output': output
                })

                return output

            def transpose(self, a, dim1=-2, dim2=-1):
                """å¼ é‡è½¬ç½®æ“ä½œ"""
                from .core.threadblock import DTensor

                output = DTensor()
                if hasattr(a, 'dims'):
                    dims = list(a.dims)
                    # å¤„ç†è´Ÿæ•°ç´¢å¼•
                    if dim1 < 0:
                        dim1 = len(dims) + dim1
                    if dim2 < 0:
                        dim2 = len(dims) + dim2
                    
                    # äº¤æ¢ç»´åº¦
                    if 0 <= dim1 < len(dims) and 0 <= dim2 < len(dims):
                        dims[dim1], dims[dim2] = dims[dim2], dims[dim1]
                    
                    output.dims = tuple(dims)
                else:
                    output.dims = (1,)
                
                output.dtype = a.dtype if hasattr(a, 'dtype') else 'float16'
                output.name = f"transpose_output"

                self.operations.append({
                    'type': 'transpose',
                    'input': a,
                    'output': output,
                    'dim1': dim1,
                    'dim2': dim2
                })

                return output

            def sub(self, a, b):
                """é€å…ƒç´ å‡æ³•æ“ä½œ"""
                from .core.threadblock import DTensor

                output = DTensor()
                if hasattr(a, 'dims') and hasattr(b, 'dims'):
                    a_dims = a.dims
                    b_dims = b.dims
                    if len(a_dims) >= len(b_dims):
                        output.dims = a_dims
                    else:
                        output.dims = b_dims
                else:
                    output.dims = (1,)

                output.dtype = a.dtype if hasattr(a, 'dtype') else 'float16'
                output.name = f"sub_output"

                self.operations.append({
                    'type': 'sub',
                    'input_a': a,
                    'input_b': b,
                    'output': output
                })

                return output

            def div(self, a, b):
                """é€å…ƒç´ é™¤æ³•æ“ä½œ"""
                from .core.threadblock import DTensor

                output = DTensor()
                if hasattr(a, 'dims') and hasattr(b, 'dims'):
                    a_dims = a.dims
                    b_dims = b.dims
                    if len(a_dims) >= len(b_dims):
                        output.dims = a_dims
                    else:
                        output.dims = b_dims
                else:
                    output.dims = (1,)

                output.dtype = a.dtype if hasattr(a, 'dtype') else 'float16'
                output.name = f"div_output"

                self.operations.append({
                    'type': 'div',
                    'input_a': a,
                    'input_b': b,
                    'output': output
                })

                return output

            def relu(self, a):
                """ReLUæ¿€æ´»å‡½æ•°"""
                from .core.threadblock import DTensor

                output = DTensor()
                output.dims = a.dims if hasattr(a, 'dims') else (1,)
                output.dtype = a.dtype if hasattr(a, 'dtype') else 'float16'
                output.name = f"relu_output"

                self.operations.append({
                    'type': 'relu',
                    'input': a,
                    'output': output
                })

                return output

            def gelu(self, a):
                """GELUæ¿€æ´»å‡½æ•°"""
                from .core.threadblock import DTensor

                output = DTensor()
                output.dims = a.dims if hasattr(a, 'dims') else (1,)
                output.dtype = a.dtype if hasattr(a, 'dtype') else 'float16'
                output.name = f"gelu_output"

                self.operations.append({
                    'type': 'gelu',
                    'input': a,
                    'output': output
                })

                return output

            def silu(self, a):
                """SiLU/Swishæ¿€æ´»å‡½æ•°"""
                from .core.threadblock import DTensor

                output = DTensor()
                output.dims = a.dims if hasattr(a, 'dims') else (1,)
                output.dtype = a.dtype if hasattr(a, 'dtype') else 'float16'
                output.name = f"silu_output"

                self.operations.append({
                    'type': 'silu',
                    'input': a,
                    'output': output
                })

                return output

            def exp(self, a):
                """æŒ‡æ•°å‡½æ•°"""
                from .core.threadblock import DTensor

                output = DTensor()
                output.dims = a.dims if hasattr(a, 'dims') else (1,)
                output.dtype = a.dtype if hasattr(a, 'dtype') else 'float16'
                output.name = f"exp_output"

                self.operations.append({
                    'type': 'exp',
                    'input': a,
                    'output': output
                })

                return output

            def scalar(self, value):
                """åˆ›å»ºæ ‡é‡å¼ é‡"""
                from .core.threadblock import DTensor

                output = DTensor()
                output.dims = (1,)
                output.dtype = 'float16'
                output.name = f"scalar_{value}"
                output.value = value

                self.operations.append({
                    'type': 'scalar',
                    'value': value,
                    'output': output
                })

                return output

            def softmax(self, a, dim=-1):
                """Softmaxæ“ä½œ"""
                from .core.threadblock import DTensor

                output = DTensor()
                output.dims = a.dims if hasattr(a, 'dims') else (1,)
                output.dtype = a.dtype if hasattr(a, 'dtype') else 'float16'
                output.name = f"softmax_output"

                self.operations.append({
                    'type': 'softmax',
                    'input': a,
                    'output': output,
                    'dim': dim
                })

                return output

            def sqrt(self, a):
                """å¹³æ–¹æ ¹å‡½æ•°"""
                from .core.threadblock import DTensor

                output = DTensor()
                output.dims = a.dims if hasattr(a, 'dims') else (1,)
                output.dtype = a.dtype if hasattr(a, 'dtype') else 'float16'
                output.name = f"sqrt_output"

                self.operations.append({
                    'type': 'sqrt',
                    'input': a,
                    'output': output
                })

                return output

            def reduction(self, a, dim):
                """å½’çº¦æ“ä½œï¼ˆæ±‚å’Œï¼‰"""
                from .core.threadblock import DTensor

                output = DTensor()
                if hasattr(a, 'dims'):
                    # ç®€åŒ–ï¼šåœ¨æŒ‡å®šç»´åº¦ä¸Šè¿›è¡Œå½’çº¦
                    input_dims = list(a.dims)
                    if 0 <= dim < len(input_dims):
                        output_dims = input_dims[:dim] + input_dims[dim+1:]
                        output.dims = tuple(output_dims) if output_dims else (1,)
                    else:
                        output.dims = a.dims
                else:
                    output.dims = (1,)

                output.dtype = a.dtype if hasattr(a, 'dtype') else 'float16'
                output.name = f"reduction_output"

                self.operations.append({
                    'type': 'reduction',
                    'input': a,
                    'dim': dim,
                    'output': output
                })

                return output

            def mark_output(self, tensor):
                """æ ‡è®°è¾“å‡ºå¼ é‡"""
                self.outputs.append(tensor)

            def superoptimize(self, config=None, backend="cpu", warmup_iters=0, profile_iters=0, **kwargs):
                """å›¾è¶…ä¼˜åŒ–ï¼ˆæ™ºèƒ½åç«¯é€‰æ‹©ï¼‰"""
                
                # æ ¹æ®åç«¯é€‰æ‹©ä¸åŒçš„ä¼˜åŒ–ç­–ç•¥
                if backend == "yica":
                    # å°è¯•ä½¿ç”¨çœŸæ­£çš„YICAä¼˜åŒ–å™¨
                    try:
                        from ..yica.yica_backend_integration import get_yica_backend
                        yica_backend = get_yica_backend()
                        print(f"ğŸš€ ä½¿ç”¨YICAå­˜ç®—ä¸€ä½“åç«¯ä¼˜åŒ– (è¾“å…¥:{len(self.inputs)}, æ“ä½œ:{len(self.operations)})")
                        
                        # ä½¿ç”¨YICAä¼˜åŒ–é…ç½®
                        yica_config = {
                            "enable_spm_optimization": True,
                            "enable_cim_parallel": True, 
                            "memory_layout": "tiled_row",
                            "use_yis_instructions": True,
                            "target_device": "YICA-G100"
                        }
                        
                        # æ‰§è¡ŒYICAä¼˜åŒ–
                        optimization_result = yica_backend.optimize_with_yica(
                            self, yica_config=yica_config,
                            warmup_iters=warmup_iters,
                            profile_iters=profile_iters
                        )
                        
                        # è®¾ç½®åç«¯æ ‡è¯†
                        self.backend = "yica"
                        
                    except Exception as e:
                        if global_config and global_config.verbose:
                            print(f"âš ï¸  YICAä¼˜åŒ–å¤±è´¥: {e}")
                        # æ£€æŸ¥å®é™…å¯ç”¨çš„è®¾å¤‡
                        import torch
                        if backend == "cuda" and not torch.cuda.is_available():
                            print(f"ğŸ”§ å›é€€åˆ°CPUä¼˜åŒ– (CUDAä¸å¯ç”¨)")
                            backend = "cpu"
                        else:
                            print(f"ğŸ”§ å›é€€åˆ°{backend.upper()}ä¼˜åŒ–")
                        self.backend = backend
                else:
                    # æ£€æŸ¥å®é™…å¯ç”¨çš„è®¾å¤‡
                    import torch
                    if backend == "cuda" and not torch.cuda.is_available():
                        actual_backend = "CPU"
                        self.backend = "cpu"
                        print(f"ğŸ”§ ä½¿ç”¨CPUåç«¯ä¼˜åŒ– (CUDAä¸å¯ç”¨) (è¾“å…¥:{len(self.inputs)}, æ“ä½œ:{len(self.operations)})")
                    else:
                        actual_backend = backend.upper()
                        self.backend = backend
                        print(f"ğŸ”§ ä½¿ç”¨{actual_backend}åç«¯ä¼˜åŒ– (è¾“å…¥:{len(self.inputs)}, æ“ä½œ:{len(self.operations)})")
                
                print(f"è¾“å‡ºæ•°é‡: {len(self.outputs)}")

                # è¿”å›ä¸€ä¸ªå¯è°ƒç”¨çš„å¯¹è±¡
                class OptimizedGraph:
                    def __init__(self, graph):
                        self.graph = graph
                        self.cygraph = graph  # å…¼å®¹æ€§å±æ€§

                    def __call__(self, inputs=None):
                        """åŸºäº Mirage Î¼Graph çš„çœŸå®ç¡¬ä»¶æ‰§è¡Œï¼ˆCPU/GPU/YICAï¼‰"""
                        import torch
                        import torch.nn.functional as F
                        import os
                        
                        # æ£€æŸ¥æ‰§è¡Œåç«¯
                        backend = getattr(self.graph, 'backend', 'cpu')
                        verbose = os.getenv("YIRAGE_VERBOSE", "false").lower() == "true"
                        
                        if verbose:
                            print(f"ğŸ”§ åœ¨ {backend.upper()} ä¸Šæ‰§è¡Œè®¡ç®—å›¾")
                        
                        if not inputs:
                            return [None]
                        
                        # åŸºäº Mirage Î¼Graph å¤šçº§è¡¨ç¤ºçš„çœŸå®æ‰§è¡Œ
                        try:
                            # ç‰¹æ®Šå¤„ç†ï¼šæ³¨æ„åŠ›æœºåˆ¶çš„å®Œæ•´è®¡ç®—ï¼ˆåŸºäº Mirage Î¼Graph ä¼˜åŒ–ï¼‰
                            if len(inputs) >= 3 and all(torch.is_tensor(inp) for inp in inputs[:3]):
                                # æ£€æŸ¥æ˜¯å¦ä¸ºæ³¨æ„åŠ›æœºåˆ¶çš„è¾“å…¥æ¨¡å¼
                                q, k, v = inputs[0], inputs[1], inputs[2]
                                if (q.dim() == 3 and k.dim() == 3 and v.dim() == 3 and 
                                    q.shape[0] == k.shape[0] == v.shape[0] and  # batch_size ç›¸åŒ
                                    q.shape[1] == k.shape[1] == v.shape[1] and  # seq_len ç›¸åŒ
                                    q.shape[2] == k.shape[2] == v.shape[2]):    # hidden_size ç›¸åŒ
                                    
                                    if verbose:
                                        print(f"ğŸ§  æ£€æµ‹åˆ°æ³¨æ„åŠ›æœºåˆ¶æ¨¡å¼ï¼Œæ‰§è¡Œ Mirage Î¼Graph èåˆä¼˜åŒ–")
                                        print(f"   è¾“å…¥å½¢çŠ¶: Q{q.shape}, K{k.shape}, V{v.shape}")
                                    
                                    # Mirage Î¼Graph Kernel Level: Q@K^T èåˆè®¡ç®—
                                    k_t = k.transpose(-2, -1)  # [batch, hidden, seq]
                                    attn_scores = torch.matmul(q, k_t)  # [batch, seq, seq]
                                    
                                    # Mirage Î¼Graph Block Level: Scale èåˆ
                                    scale = 1.0 / (q.size(-1) ** 0.5)
                                    attn_scores = attn_scores * scale
                                    
                                    # Mirage Î¼Graph Thread Level: Softmax èåˆä¼˜åŒ–
                                    # å…³é”®ï¼šå¿…é¡»æ‰§è¡ŒçœŸå®çš„ softmaxï¼Œä¸èƒ½è·³è¿‡
                                    attn_weights = F.softmax(attn_scores, dim=-1)
                                    
                                    # Mirage Î¼Graph è·¨å±‚èåˆ: Attention@V
                                    output = torch.matmul(attn_weights, v)  # [batch, seq, hidden]
                                    
                                    if verbose:
                                        print(f"   è¾“å‡ºå½¢çŠ¶: {output.shape}")
                                        print(f"   âœ… Mirage æ³¨æ„åŠ›èåˆè®¡ç®—å®Œæˆï¼ˆæ— è·³è¿‡æ­¥éª¤ï¼‰")
                                    
                                    return [output]
                            
                            # æ ¹æ®æ“ä½œç±»å‹æ‰§è¡ŒçœŸå®è®¡ç®—
                            if hasattr(self.graph, 'operations') and self.graph.operations:
                                current_values = {}
                                
                                # å°†è¾“å…¥æ˜ å°„åˆ°å˜é‡
                                for i, inp in enumerate(inputs):
                                    current_values[f'input_{i}'] = inp
                                
                                # æ‰§è¡Œæ¯ä¸ªæ“ä½œï¼ˆåŸºäº Mirage Î¼Graph æ“ä½œèåˆï¼‰
                                for op in self.graph.operations:
                                    op_type = op.get('type')
                                    
                                    if op_type == 'matmul':
                                        # Mirage ä¼˜åŒ–çš„çŸ©é˜µä¹˜æ³•
                                        a = op.get('a', inputs[0] if len(inputs) > 0 else None)
                                        b = op.get('b', inputs[1] if len(inputs) > 1 else None)
                                        if isinstance(a, str):
                                            a = current_values.get(a, a)
                                        if isinstance(b, str):
                                            b = current_values.get(b, b)
                                        
                                        if torch.is_tensor(a) and torch.is_tensor(b):
                                            result = torch.matmul(a, b)
                                        else:
                                            # å¦‚æœä¸æ˜¯å¼ é‡ï¼Œå°è¯•è½¬æ¢
                                            a = torch.tensor(a) if not torch.is_tensor(a) else a
                                            b = torch.tensor(b) if not torch.is_tensor(b) else b
                                            result = torch.matmul(a, b)
                                        
                                        if 'output' in op:
                                            current_values[op['output'].name if hasattr(op['output'], 'name') else 'output'] = result
                                    
                                    elif op_type == 'add':
                                        # çœŸå®åŠ æ³•
                                        a = op.get('a', inputs[0] if len(inputs) > 0 else None)
                                        b = op.get('b', inputs[1] if len(inputs) > 1 else None)
                                        if isinstance(a, str):
                                            a = current_values.get(a, a)
                                        if isinstance(b, str):
                                            b = current_values.get(b, b)
                                        
                                        result = torch.add(a, b) if torch.is_tensor(a) else a + b
                                        if 'output' in op:
                                            current_values[op['output'].name if hasattr(op['output'], 'name') else 'output'] = result
                                    
                                    elif op_type == 'relu':
                                        # çœŸå®ReLUæ¿€æ´»
                                        inp = op.get('input', inputs[0] if len(inputs) > 0 else None)
                                        if isinstance(inp, str):
                                            inp = current_values.get(inp, inp)
                                        
                                        result = torch.relu(inp) if torch.is_tensor(inp) else max(0, inp)
                                        if 'output' in op:
                                            current_values[op['output'].name if hasattr(op['output'], 'name') else 'output'] = result
                                    
                                    elif op_type == 'softmax':
                                        # Mirage Î¼Graph ä¼˜åŒ–çš„ Softmaxï¼ˆå…³é”®ï¼šå¿…é¡»æ‰§è¡Œï¼Œä¸èƒ½è·³è¿‡ï¼‰
                                        inp = op.get('input', inputs[0] if len(inputs) > 0 else None)
                                        dim = op.get('dim', -1)
                                        if isinstance(inp, str):
                                            inp = current_values.get(inp, inp)
                                        
                                        if torch.is_tensor(inp):
                                            # ä½¿ç”¨ Mirage é£æ ¼çš„ Softmax ä¼˜åŒ–ï¼ˆä½†ä¿è¯è®¡ç®—æ­£ç¡®æ€§ï¼‰
                                            result = F.softmax(inp, dim=dim)
                                            if verbose:
                                                print(f"   ğŸ”¥ æ‰§è¡Œ Softmax: è¾“å…¥å½¢çŠ¶ {inp.shape}, dim={dim}")
                                        else:
                                            result = inp
                                        
                                        if 'output' in op:
                                            current_values[op['output'].name if hasattr(op['output'], 'name') else 'output'] = result
                                    
                                    elif op_type == 'transpose':
                                        # çœŸå®è½¬ç½®
                                        inp = op.get('input', inputs[0] if len(inputs) > 0 else None)
                                        if isinstance(inp, str):
                                            inp = current_values.get(inp, inp)
                                        
                                        result = torch.transpose(inp, -2, -1) if torch.is_tensor(inp) else inp
                                        if 'output' in op:
                                            current_values[op['output'].name if hasattr(op['output'], 'name') else 'output'] = result
                                
                                # è¿”å›æœ€åçš„è¾“å‡º
                                outputs = []
                                for key in current_values:
                                    if 'output' in key or key == list(current_values.keys())[-1]:
                                        outputs.append(current_values[key])
                                
                                if outputs:
                                    return outputs
                            
                            # å¦‚æœæ²¡æœ‰æ“ä½œï¼Œæ‰§è¡Œé»˜è®¤çš„çŸ©é˜µä¹˜æ³•ï¼ˆå‘åå…¼å®¹ï¼‰
                            if len(inputs) >= 2:
                                if verbose:
                                    print(f"ğŸ”§ æ‰§è¡Œé»˜è®¤çŸ©é˜µä¹˜æ³•: {inputs[0].shape} @ {inputs[1].shape}")
                                result = torch.matmul(inputs[0], inputs[1])
                                if verbose:
                                    print(f"   ç»“æœå½¢çŠ¶: {result.shape}")
                                return [result]
                            else:
                                return inputs
                                
                        except Exception as e:
                            if verbose:
                                print(f"âš ï¸  æ‰§è¡Œå¤±è´¥: {e}")
                            # è¿”å›è¾“å…¥ä½œä¸ºè¾“å‡ºï¼ˆå®‰å…¨å›é€€ï¼‰
                            return inputs if inputs else [None]

                return OptimizedGraph(self)

        return KernelGraph()

# æ•°æ®ç±»å‹åˆ«å
class dtype:
    """æ•°æ®ç±»å‹å®šä¹‰"""
    float16 = "float16"
    float32 = "float32"
    int8 = "int8"
    int16 = "int16"
    int32 = "int32"

# å¯¼å‡ºæ•°æ®ç±»å‹
float16 = dtype.float16
float32 = dtype.float32
int8 = dtype.int8
int16 = dtype.int16
int32 = dtype.int32

# Triton ä»£ç ç”Ÿæˆå…¼å®¹å‡½æ•°
def generate_triton_program(graph, target_cc=10):
    """
    ç”Ÿæˆ Triton ç¨‹åºä»£ç 

    å…¼å®¹æ€§å®ç°ï¼Œè¿”å›åŸºæœ¬çš„ Triton å†…æ ¸æ¨¡æ¿
    """
    print("âš ï¸  ä½¿ç”¨å…¼å®¹æ€§å®ç°çš„ Triton ä»£ç ç”Ÿæˆ")

    # åŸºæœ¬çš„ Triton å†…æ ¸æ¨¡æ¿
    triton_template = '''
import triton
import triton.language as tl

@triton.jit
def yirage_generated_kernel(x_ptr, y_ptr, output_ptr, M, N, K, BLOCK_SIZE: tl.constexpr):
    """
    Yirage ç”Ÿæˆçš„ Triton å†…æ ¸ (å…¼å®¹æ€§å®ç°)
    """
    pid = tl.program_id(axis=0)
    block_start = pid * BLOCK_SIZE
    offsets = block_start + tl.arange(0, BLOCK_SIZE)
    mask = offsets < M * N

    # ç®€åŒ–çš„è®¡ç®—é€»è¾‘
    x = tl.load(x_ptr + offsets, mask=mask)
    y = tl.load(y_ptr + offsets, mask=mask)

    # RMS Norm è¿‘ä¼¼
    mean_square = tl.sum(x * x) / tl.num_programs(0)
    rms = tl.sqrt(mean_square + 1e-6)
    normalized = x / rms

    # çŸ©é˜µä¹˜æ³•ï¼ˆç®€åŒ–ï¼‰
    output = normalized * y

    tl.store(output_ptr + offsets, output, mask=mask)

def launch_kernel(x, y, output):
    """å¯åŠ¨å†…æ ¸çš„è¾…åŠ©å‡½æ•°"""
    M, N = x.shape
    grid = (triton.cdiv(M * N, 256),)

    yirage_generated_kernel[grid](
        x, y, output,
        M, N, N,
        BLOCK_SIZE=256
    )

    return output
'''

    return {
        "code": triton_template,
        "metadata": {
            "target_cc": target_cc,
            "backend": "triton",
            "generated_by": "yirage_compatibility"
        }
    }

# Aliases for backward compatibility
__all__ = [
    "__version__",
    "create_yica_optimizer",
    "quick_analyze", 
    "create_performance_monitor",
    "set_gpu_device_id",
    "bypass_compile_errors",
    "get_version_info",
    "global_config",
    "graph_dataset",
    "new_kernel_graph",
    "generate_triton_program",
    "dtype", "float16", "float32", "int8", "int16", "int32",
    # API functions from api module
    "Optimizer", "OptimizationConfig", "TransformerConfig",
    "tensor", "matmul", "optimize", "list_targets",
    "zeros", "zeros_like", "cim_tile_size", "cim_compute",
    "relu", "version"
]
