---
title: 'LLM'
description: 'Learn about the multiple LLM providers supported by NOMOS'
icon: 'brain'
---

NOMOS supports multiple LLM providers, allowing you to choose the best model for your use case.

## Supported Providers

<CardGroup cols={2}>
  <Card
    title="OpenAI"
    href="#openai"
  >
    GPT-4o, GPT-4o-mini, and more
  </Card>
  <Card
    title="Anthropic"
    href="#anthropic"
  >
    Claude 3.5 Sonnet, Haiku, and Opus
  </Card>
  <Card
    title="Google Gemini"
    href="#google-gemini"
  >
    Gemini 2.0 Flash, Pro, and more
  </Card>
  <Card
    title="Mistral AI"
    href="#mistral-ai"
  >
    Mistral Large, Medium, and Small
  </Card>
  <Card
    title="Ollama"
    href="#ollama-local-models"
  >
    Local models including Llama, Qwen, and more
  </Card>
  <Card
    title="HuggingFace"
    href="#huggingface"
  >
    Open source models via HuggingFace
  </Card>
  <Card
    title="Cohere"
    href="#cohere"
  >
    Command R+, Command R, and more
  </Card>
  <Card
    title="Custom"
  >
    Use the `BaseLLM` class to implement your own provider
  </Card>
  <Card
    title="More Coming Soon"
  >
    NOMOS is continuously expanding support for new LLM providers
  </Card>
</CardGroup>

## OpenAI

<CodeGroup>

```python Basic Usage
from nomos.llms import OpenAI

llm = OpenAI(model="gpt-4o-mini")
```

```python Available Models
llm = OpenAI(model="gpt-4o")
llm = OpenAI(model="gpt-4o-mini")
llm = OpenAI(model="gpt-4-turbo")
llm = OpenAI(model="gpt-3.5-turbo")
```

```bash Installation
pip install nomos[openai]
```

```bash Environment Variable
export OPENAI_API_KEY=your-api-key-here
```

</CodeGroup>

## Anthropic

<CodeGroup>

```python Basic Usage
from nomos.llms import Anthropic

llm = Anthropic(model="claude-3-5-sonnet-20241022")
```

```python Available Models
llm = Anthropic(model="claude-3-5-sonnet-20241022")
llm = Anthropic(model="claude-3-5-haiku-20241022")
llm = Anthropic(model="claude-3-opus-20240229")
llm = Anthropic(model="claude-3-sonnet-20240229")
llm = Anthropic(model="claude-3-haiku-20240307")
```

```bash Installation
pip install nomos[anthropic]
```

```bash Environment Variable
export ANTHROPIC_API_KEY=your-api-key-here
```

</CodeGroup>

## Google Gemini

<CodeGroup>

```python Basic Usage
from nomos.llms import Gemini

llm = Gemini(model="gemini-2.0-flash-exp")
```

```python Available Models
llm = Gemini(model="gemini-2.0-flash-exp")
llm = Gemini(model="gemini-1.5-pro")
llm = Gemini(model="gemini-1.5-flash")
llm = Gemini(model="gemini-1.0-pro")
```

```bash Installation
pip install nomos[google]
```

```bash Environment Variable
export GOOGLE_API_KEY=your-api-key-here
```

</CodeGroup>

## Mistral AI

<CodeGroup>

```python Basic Usage
from nomos.llms import Mistral

llm = Mistral(model="ministral-8b-latest")
```

```python Available Models
llm = Mistral(model="ministral-8b-latest")
llm = Mistral(model="mistral-small")
llm = Mistral(model="mistral-medium")
llm = Mistral(model="mistral-large")
```

```bash Installation
pip install nomos[mistralai]
```

```bash Environment Variable
export MISTRAL_API_KEY=your-api-key-here
```

</CodeGroup>

## Ollama (Local Models)

<CodeGroup>

```python Basic Usage
from nomos.llms import Ollama

llm = Ollama(model="llama3.3")
```

```python Popular Models
llm = Ollama(model="llama3.3")
llm = Ollama(model="qwen2.5:14b")
llm = Ollama(model="codestral")
llm = Ollama(model="deepseek-coder-v2")
llm = Ollama(model="phi4")
```

```bash Installation
pip install nomos[ollama]
```

```bash Prerequisites
# Install Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# Pull a model
ollama pull llama3.3
```

</CodeGroup>

## HuggingFace

<CodeGroup>

```python Basic Usage
from nomos.llms import HuggingFace

llm = HuggingFace(model="meta-llama/Meta-Llama-3-8B-Instruct")
```

```python Other Models
llm = HuggingFace(model="microsoft/DialoGPT-large")
```

```bash Installation
pip install nomos[huggingface]
```

```bash Environment Variable
export HUGGINGFACE_API_TOKEN=your-token-here
```

</CodeGroup>

## Cohere

<CodeGroup>

```python Basic Usage
from nomos.llms import Cohere

llm = Cohere(model="command-a-03-2025")
```

```python Available Models
llm = Cohere(model="command-a-03-2025")
llm = Cohere(model="command-r-plus")
llm = Cohere(model="command-r")
llm = Cohere(model="command")
```

```bash Installation
pip install nomos[cohere]
```

```bash Environment Variable
export COHERE_API_KEY=your-api-key-here
```

</CodeGroup>

## YAML Configuration

You can specify LLM configuration in your YAML config file:

<Tabs>
  <Tab title="OpenAI">
    ```yaml
    llm:
      provider: openai
      model: gpt-4o-mini
    ```
  </Tab>
  <Tab title="Anthropic">
    ```yaml
    llm:
      provider: anthropic
      model: claude-3-5-sonnet-20241022
    ```
  </Tab>
  <Tab title="Mistral">
    ```yaml
    llm:
      provider: mistral
      model: mistral-medium
    ```
  </Tab>
  <Tab title="Google Gemini">
    ```yaml
    llm:
      provider: google
      model: gemini-2.0-flash-exp
    ```
  </Tab>
  <Tab title="Ollama">
    ```yaml
    llm:
      provider: ollama
      model: llama3.3
      base_url: http://localhost:11434  # Optional: custom Ollama URL
    ```
  </Tab>
  <Tab title="HuggingFace">
    ```yaml
    llm:
      provider: huggingface
      model: meta-llama/Meta-Llama-3-8B-Instruct
    ```
  </Tab>
  <Tab title="Cohere">
    ```yaml
    llm:
      provider: cohere
      model: command-a-03-2025
    ```
  </Tab>
</Tabs>

## Advanced Configuration

### Custom Parameters

You can pass additional parameters to LLM providers:

<CodeGroup>

```python OpenAI Advanced
llm = OpenAI(
    model="gpt-4o-mini",
    temperature=0.7,
    max_tokens=1000,
    top_p=0.9
)
```

```python Anthropic Advanced
llm = Anthropic(
    model="claude-3-5-sonnet-20241022",
    temperature=0.3,
    max_tokens=2048,
    top_p=0.8
)
```

```python Cohere Advanced
llm = Cohere(
    model="command-a-03-2025",
    temperature=0.5,
    max_tokens=1500
)
```

</CodeGroup>

### YAML Advanced Configuration

<CodeGroup>

```yaml OpenAI
llm:
  provider: openai
  model: gpt-4o-mini
  temperature: 0.7
  max_tokens: 1000
  top_p: 0.9
```

```yaml Anthropic
llm:
  provider: anthropic
  model: claude-3-5-sonnet-20241022
  temperature: 0.3
  max_tokens: 2048
  top_p: 0.8
```

```yaml Cohere
llm:
  provider: cohere
  model: command-a-03-2025
  temperature: 0.5
  max_tokens: 1500
```

</CodeGroup>

### Multiple LLMs

Multiple LLMs can be defined to use different models for specific purposes. For example, you might configure one model optimized for coding tasks and another for general conversation. This flexibility allows you to tailor the behavior of your application to different use cases, improving efficiency and user experience.

```yaml
llm:
  global:
    provider: anthropic
    model: claude-3-5-sonnet-20241022
  coding:
    provider: anthropic
    model: claude-opus-4-20250514
```


## Troubleshooting

<AccordionGroup>
  <Accordion title="API Key Not Found">
    Ensure environment variables are set correctly in your shell profile or `.env.local` file
  </Accordion>
  <Accordion title="Model Not Available">
    Check that the model name is correct and available in your region
  </Accordion>
  <Accordion title="Rate Limits">
    Implement retry logic or use different models with higher rate limits
  </Accordion>
  <Accordion title="Local Models (Ollama)">
    Ensure Ollama is running (`ollama serve`) and the model is pulled (`ollama pull model-name`)
  </Accordion>
</AccordionGroup>

### Error Handling

NOMOS includes built-in error handling and retry mechanisms:

```yaml
name: my-agent
llm:
  provider: openai
  model: gpt-4o-mini
max_errors: 3  # Retry up to 3 times on LLM errors
```

## Performance Tips

<CardGroup cols={2}>
  <Card
    title="Choose the Right Model"
    icon="target"
  >
    Use smaller models for simple tasks to reduce latency and costs
  </Card>
  <Card
    title="Configure Temperature"
    icon="thermometer"
  >
    Lower values (0.1-0.3) for consistent responses
  </Card>
  <Card
    title="Set Max Tokens"
    icon="hash"
  >
    Limit response length to control costs and latency
  </Card>
  <Card
    title="Use Local Models"
    icon="server"
  >
    Ollama for development or when data privacy is important
  </Card>
</CardGroup>

## Model Documentation

For the most up-to-date list of available models, refer to the official documentation:

<CardGroup cols={2}>
  <Card
    title="Anthropic Claude Models"
    icon="external-link"
    href="https://docs.anthropic.com/en/docs/about-claude/models/overview"
  >
    Official Claude models documentation
  </Card>
  <Card
    title="OpenAI Models"
    icon="external-link"
    href="https://platform.openai.com/docs/models"
  >
    Complete OpenAI models reference
  </Card>
  <Card
    title="Google Gemini Models"
    icon="external-link"
    href="https://cloud.google.com/vertex-ai/generative-ai/docs/models"
  >
    Vertex AI Generative AI models
  </Card>
  <Card
    title="Mistral Models"
    icon="external-link"
    href="https://docs.mistral.ai/getting-started/models/models_overview/"
  >
    Mistral AI models overview
  </Card>
  <Card
    title="Ollama Model Library"
    icon="external-link"
    href="https://ollama.com/search"
  >
    Browse available local models
  </Card>
  <Card
    title="HuggingFace Models"
    icon="external-link"
    href="https://huggingface.co/models"
  >
    Explore HuggingFace model hub
  </Card>
  <Card
    title="Cohere Models"
    icon="external-link"
    href="https://docs.cohere.com/docs/models"
  >
    Cohere models documentation
  </Card>
</CardGroup>
