Metadata-Version: 2.4
Name: llm-scratch-lab
Version: 0.0.1
Summary: A toolkit for building an LLM from scratch
Author-email: Frantz Paul <fpaul787@gmail.com>
License-Expression: MIT
Project-URL: Homepage, https://github.com/fpaul787/llm-from-scratch-lab
Project-URL: Issues, https://github.com/fpaul787/llm-from-scratch-lab/issues
Classifier: Programming Language :: Python :: 3
Classifier: Operating System :: OS Independent
Requires-Python: >=3.8
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: tiktoken>=0.9.0
Requires-Dist: torch>=2.7.0
Dynamic: license-file

# llm-from-scratch-lab
Lab and python package for building a small GPT model. This code was adapted from the [Build a Large Language Model (From Scratch)](https://github.com/rasbt/LLMs-from-scratch) repository. This lab is intended to be a learning exercise, not a production-ready implementation.
It is still very much a work in progress, and in some regards, a little incomplete. However, it is a good starting point for understanding how to build a small GPT model from scratch.

## Requirements
* pytorch < 3
* `tiktoken` BPE tokenizer < 1

## Usage
There is a `run_lab.py` script that can be used to run the lab. It will send a piece of text to the lab and the lab will perform the preprocessing, tokenization, and model inference steps. The output will be printed to the console.

## Package Installation
TBD
