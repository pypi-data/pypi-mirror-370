"""
{{ display_name }} plugin for AgentUp{% if template == "ai" %} with AI capabilities{% endif %}.

{{ description }}
"""

import datetime
from typing import Dict, Any
{% if template == "ai" %}import asyncio{% endif %}

from agent.plugins.base import Plugin
from agent.plugins.decorators import capability
from agent.plugins.models import CapabilityContext


class {{ class_name }}(Plugin):
    """{% if template == "ai" %}AI-enabled {% else %}Basic {% endif %}plugin class for {{ display_name }}."""

    def __init__(self):
        """Initialize the plugin."""
        super().__init__()
        self.name = "{{ plugin_name_snake }}"
        self.version = "1.0.0"
        {% if template == "ai" %}
        self.llm_service = None
        {% endif %}

    {% if template == "ai" %}
    async def initialize(self, config: Dict[str, Any], services: Dict[str, Any]):
        """Initialize plugin with configuration and services."""
        self.config = config

        # Store LLM service for AI operations
        self.llm_service = services.get("llm")

        # Setup other services
        if "http_client" in services:
            self.http_client = services["http_client"]

    {% endif %}
    @capability(
        id="{{ capability_id }}",
        name="{{ display_name }}",
        description="{{ description }}",
        scopes=[{% if template == "ai" %}"{{ plugin_name }}:use", "ai:function"{% else %}"{{ plugin_name }}:use"{% endif %}],
        ai_function={% if template == "ai" %}True{% else %}False{% endif %}{% if template == "ai" %},
        ai_parameters={
            "type": "object",
            "properties": {
                "input": {
                    "type": "string",
                    "description": "The input to process with {{ display_name }}"
                },
                {% if template == "ai" %}"mode": {
                    "type": "string",
                    "enum": ["fast", "accurate", "balanced"],
                    "description": "Processing mode",
                    "default": "balanced"
                },
                "format": {
                    "type": "string",
                    "enum": ["text", "json", "markdown"],
                    "description": "Output format",
                    "default": "text"
                }{% endif %}
            },
            "required": ["input"]
        }{% endif %}
    )
    async def {{ capability_method_name }}(self, context: CapabilityContext) -> Dict[str, Any]:
        """Execute the {{ display_name.lower() }} capability."""
        try:
            # Extract input from context using base class method
            input_text = self._extract_task_content(context)
            
            # Log the start of processing (demonstrates structured logging)
            self.logger.info("Starting capability execution", capability_id="{{ capability_id }}", input_length=len(input_text))

            {% if template == "ai" %}
            # Extract parameters for AI functions
            params = context.metadata.get("parameters", {})
            mode = params.get("mode", "balanced")
            format = params.get("format", "text")
            input_text = params.get("input", input_text)

            # AI-powered processing
            if not self.llm_service:
                self.logger.error("LLM service not available for AI processing")
                return {
                    "success": False,
                    "error": "LLM service not available",
                    "content": "AI functionality requires LLM service configuration"
                }

            # Process based on mode
            if mode == "fast":
                result = await self._fast_processing(input_text)
            elif mode == "accurate":
                result = await self._accurate_processing(input_text)
            else:  # balanced
                result = await self._balanced_processing(input_text)

            # Format output
            formatted_result = self._format_output(result, format)
            
            # Log successful completion
            self.logger.info("Capability execution completed", 
                           capability_id="{{ capability_id }}", 
                           mode=mode, 
                           format=format, 
                           result_length=len(formatted_result))

            return {
                "success": True,
                "content": formatted_result,
                "metadata": {
                    "capability": "{{ capability_id }}",
                    "mode": mode,
                    "format": format,
                    "processed_at": datetime.datetime.now().isoformat()
                }
            }
            {% else %}
            # Basic processing
            processed_result = f"{{ display_name }} processed: {input_text}"
            
            # Log successful completion
            self.logger.info("Capability execution completed", 
                           capability_id="{{ capability_id }}", 
                           input_length=len(input_text),
                           result_length=len(processed_result))

            return {
                "success": True,
                "content": processed_result,
                "metadata": {
                    "capability": "{{ capability_id }}",
                    "processed_at": datetime.datetime.now().isoformat(),
                    "input_length": len(input_text)
                }
            }
            {% endif %}

        except Exception as e:
            # Log the error with structured data
            self.logger.error("Error in capability execution", 
                            capability_id="{{ capability_id }}", 
                            error=str(e), 
                            exc_info=True)
            return {
                "success": False,
                "error": str(e),
                "content": f"Error in {{ display_name }}: {str(e)}"
            }

    {% if template == "ai" %}
    async def _fast_processing(self, input: str) -> str:
        """Fast processing mode using LLM."""
        # Quick processing with minimal LLM interaction
        prompt = f"Quickly process this input: {input[:100]}..."
        
        self.logger.debug("Starting fast processing", input_length=len(input), prompt_length=len(prompt))

        try:
            # Adapt to different LLM service interfaces
            if hasattr(self.llm_service, 'generate'):
                response = await self.llm_service.generate(prompt, max_tokens=50)
            elif hasattr(self.llm_service, 'chat'):
                response = await self.llm_service.chat([{"role": "user", "content": prompt}], max_tokens=50)
            else:
                # Fallback for basic processing
                return f"Fast processing of: {input[:50]}..."

            result = response.strip() if hasattr(response, 'strip') else str(response)
            self.logger.debug("Fast processing completed successfully", result_length=len(result))
            return result
        except Exception as e:
            self.logger.warning("Fast processing fallback due to LLM error", error=str(e))
            return f"Fast processing completed (LLM unavailable): {input[:50]}..."

    async def _accurate_processing(self, input: str) -> str:
        """Accurate processing mode with detailed LLM analysis."""
        prompt = f"""
        Perform a detailed analysis of the following input:

        Input: {input}

        Please provide a comprehensive response with:
        1. Analysis of the content
        2. Key insights
        3. Recommendations
        """

        try:
            if hasattr(self.llm_service, 'generate'):
                response = await self.llm_service.generate(prompt, max_tokens=500)
            elif hasattr(self.llm_service, 'chat'):
                response = await self.llm_service.chat([{"role": "user", "content": prompt}], max_tokens=500)
            else:
                return f"Detailed analysis of: {input}"

            return response.strip() if hasattr(response, 'strip') else str(response)
        except Exception as e:
            return f"Accurate processing completed (LLM unavailable): Analysis of {len(input)} characters"

    async def _balanced_processing(self, input: str) -> str:
        """Balanced processing mode."""
        prompt = f"""
        Process this input with balanced detail and efficiency:

        {input}

        Provide a helpful response that balances thoroughness with conciseness.
        """

        try:
            if hasattr(self.llm_service, 'generate'):
                response = await self.llm_service.generate(prompt, max_tokens=200)
            elif hasattr(self.llm_service, 'chat'):
                response = await self.llm_service.chat([{"role": "user", "content": prompt}], max_tokens=200)
            else:
                return f"Balanced processing of: {input}"

            return response.strip() if hasattr(response, 'strip') else str(response)
        except Exception as e:
            return f"Balanced processing completed (LLM unavailable): {input[:100]}..."

    def _format_output(self, result: str, format: str) -> str:
        """Format the result according to the specified format."""
        if format == "json":
            import json
            return json.dumps({
                "result": result,
                "plugin": "{{ plugin_name }}",
                "timestamp": datetime.datetime.now().isoformat()
            }, indent=2)
        elif format == "markdown":
            return f"## {{ display_name }} Result\n\n{result}\n\n*Processed by {{ plugin_name }}*"
        else:  # text
            return result

    {% endif %}
    {% if template == "ai" %}
    def get_config_schema(self) -> Dict[str, Any]:
        """Define configuration schema for AI plugin."""
        return {
            "type": "object",
            "properties": {
                "llm_model": {
                    "type": "string",
                    "description": "LLM model to use",
                    "default": "gpt-4o-mini"
                },
                "max_tokens": {
                    "type": "integer",
                    "minimum": 10,
                    "maximum": 4000,
                    "default": 200,
                    "description": "Maximum tokens for LLM responses"
                },
                "temperature": {
                    "type": "number",
                    "minimum": 0.0,
                    "maximum": 2.0,
                    "default": 0.7,
                    "description": "LLM temperature setting"
                }
            },
            "additionalProperties": False
        }

    def validate_config(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """Validate AI plugin configuration."""
        return {
            "valid": True,
            "errors": [],
            "warnings": []
        }

    {% else %}
    def get_config_schema(self) -> Dict[str, Any]:
        """Define configuration schema for basic plugin."""
        return {
            "type": "object",
            "properties": {
                "enabled": {
                    "type": "boolean",
                    "default": True,
                    "description": "Enable/disable the plugin"
                },
                "debug": {
                    "type": "boolean",
                    "default": False,
                    "description": "Enable debug logging"
                }
            },
            "additionalProperties": False
        }

    def validate_config(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """Validate basic plugin configuration."""
        return {
            "valid": True,
            "errors": [],
            "warnings": []
        }

    {% endif %}
    async def cleanup(self):
        """Cleanup resources when plugin is destroyed."""
        {% if template == "ai" %}
        # Clear LLM service reference
        self.llm_service = None

        # Close HTTP client if available
        if hasattr(self, 'http_client') and hasattr(self.http_client, 'close'):
            await self.http_client.close()
        {% else %}
        # Basic cleanup
        pass
        {% endif %}
