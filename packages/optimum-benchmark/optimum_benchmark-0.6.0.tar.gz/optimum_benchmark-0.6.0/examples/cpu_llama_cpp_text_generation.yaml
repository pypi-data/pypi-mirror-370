defaults:
  - benchmark
  - scenario: inference
  - backend: llama_cpp
  - launcher: process
  - _base_
  - _self_

name: cpu_llama_cpp_text_generation

backend:
  device: cpu
  task: text-generation
  model: TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF
  filename: tinyllama-1.1b-chat-v1.0.Q4_0.gguf

scenario:
  memory: true
  latency: true

  input_shapes:
    batch_size: 1
    sequence_length: 128
