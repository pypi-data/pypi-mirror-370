backend:
  model: TinyLlama/TinyLlama-1.1B-Chat-v1.0
  quantization_scheme: bnb
  quantization_config:
    load_in_4bit: true
    bnb_4bit_compute_dtype: float16
