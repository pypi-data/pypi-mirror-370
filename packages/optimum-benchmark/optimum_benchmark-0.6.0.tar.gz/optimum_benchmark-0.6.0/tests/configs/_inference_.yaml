defaults:
  - override scenario: inference

scenario:
  memory: true
  latency: true

  duration: 1
  iterations: 1
  warmup_runs: 1

  input_shapes:
    batch_size: 1
    sequence_length: 16

  generate_kwargs:
    max_new_tokens: 16
    min_new_tokens: 16

  call_kwargs:
    num_inference_steps: 4
