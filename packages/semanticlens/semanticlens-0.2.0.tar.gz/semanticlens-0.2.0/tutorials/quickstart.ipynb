{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "html"
        }
      },
      "source": [
        "<div align=\"center\">\n",
        "<img src=\"../static/images/logo-with-name_big.svg\" width=\"400px\" alt=\"SemanticLens logo\" align=\"center\" />\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üîç Tutorial: Peeking Inside Vision Models with SemanticLens\n",
        "This tutorial provides a step-by-step guide on how to use the `semanticlens` library to understand the inner workings of a pre-trained vision model. We will discover what visual 'concepts' individual neurons in the model have learned, search for these concepts using natural language, and quantify the model's interpretability.\n",
        "\n",
        "## Goals\n",
        "By the end of this tutorial, you will be able to:\n",
        "1.  Understand the three core components of `semanticlens`: the Component Visualizer, the Foundation Model, and the Lens.\n",
        "2.  Build a \"concept database\" for a model's neurons.\n",
        "3.  Search for neurons that have learned a specific concept (e.g., \"a car wheel\") using a text prompt.\n",
        "4.  Visualize what these neurons have actually learned.\n",
        "5.  Calculate scores like \"clarity\" to evaluate the model's interpretability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Core Idea\n",
        "\n",
        "<div align=\"center\">\n",
        "  <img src=\"../static/images/overview-figure.svg\" width=\"90%\" alt=\"Overview figure\" align=\"center\" />\n",
        "  <p>\n",
        "  Overview of the SemanticLens framework as introduced in our <a href=\"https://www.nature.com/articles/s42256-025-01084-w\"> research paper.</a>\n",
        "\n",
        "  </p>\n",
        "</div>\n",
        "\n",
        "The core idea of `semanticlens` is to map the incomprehensible internal components of a model you want to analyze (Model M) into the meaningful, multimodal space of a powerful foundation model (Model F, e.g., CLIP).\n",
        "This is done in three steps:\n",
        "1.  **Collect**: For each neuron in Model M, we find the data samples (\"concept examples\") that activate it the most.\n",
        "2.  **Embed**: We feed these concept examples into the foundation model (F) to translate them into meaningful vector representations (embeddings).\n",
        "3.  **Analyze**: Now that each neuron is represented by a meaningful vector, we can treat it like a searchable database of the model's internal knowledge."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup: Importing Libraries and Configuring the Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import timm\n",
        "import semanticlens as sl\n",
        "from semanticlens.component_visualization import aggregators\n",
        "from semanticlens.foundation_models import ClipMobile\n",
        "from semanticlens.component_visualization import ActivationComponentVisualizer\n",
        "from semanticlens.utils.log_setup import setup_colored_logging\n",
        "\n",
        "from torchvision.transforms import v2 as transforms\n",
        "\n",
        "# Set up logging for better feedback\n",
        "setup_colored_logging(\"INFO\")\n",
        "\n",
        "# Configure device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Preparing the Model and Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> ‚ö†Ô∏è **Note:**  \n",
        "> At its core, `semanticlens` builds a representation of a model‚Äôs components based on a dataset.\n",
        "> For best results, **it should be applied to the same dataset the model was trained on**.\n",
        "> Otherwise, some components may be misrepresented or remain unexplained, since important input features might be missing.\n",
        "> \n",
        "> In this tutorial, we analyze the model `\"resnet50d.a1_in1k\"`, which was trained on the ImageNet dataset.\n",
        "> To run a meaningful analysis with `semanticlens`, you will need access to ImageNet, which can be downloaded here: [image-net.org](https://www.image-net.org).\n",
        "> \n",
        "> Please set the variable `IMAGENET_PATH` to the path of either the train or val folder.\n",
        "> \n",
        "> If downloading ImageNet is not feasible, you can instead use a proxy dataset by setting `IMAGENET_PATH=None`.\n",
        "> Keep in mind, however, that the resulting insights will likely be limited, and any conclusions should be considered preliminary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "IMAGENET_PATH = None  # replace with path to your local ImageNet1k train or test directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, we need to load the model we want to analyze and our data. One of the key design patterns in `semanticlens` is the use of **two versions of a dataset**:\n",
        " 1.  `dataset_model`: This dataset has the specific transformations (e.g.,resizing, cropping, normalization) of the model being analyzed. It's used to get the model's *activations*.\n",
        " 2.  `dataset_fm`: This dataset applies only minimal preprocessing (e.g., the same resizing and cropping as the model). It is used by the foundation model to embed the images. Since foundation-model‚Äìspecific transformations are applied lazily, we only perform resizing and cropping here‚Äîensuring the foundation model sees the same input as the analyzed model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Load the Model to Analyze ---\n",
        "# We'll use a pretrained ResNet50d model from the timm library.\n",
        "model_name = \"resnet50d.a1_in1k\"\n",
        "model = timm.create_model(model_name, pretrained=True).to(device).eval()\n",
        "\n",
        "# It's important to give the model a name for caching purposes.\n",
        "model.name = model_name\n",
        "\n",
        "# Get the model's data configuration and transforms\n",
        "data_config = timm.data.resolve_data_config({}, model=model)\n",
        "model_transform = timm.data.create_transform(**data_config)\n",
        "\n",
        "print(\"Model Loaded:\", model_name)\n",
        "print(\"Model Transform:\", model_transform)\n",
        "\n",
        "\n",
        "# --- Prepare the Datasets ---\n",
        "# For this example, we'll use FakeData so it can be run without any downloads.\n",
        "# To use a real dataset (like ImageNet), use the commented-out code below.\n",
        "\n",
        "# a) Dataset for the model being analyzed\n",
        "\n",
        "# b) Dataset for the foundation model\n",
        "# We don't apply normalization, as the foundation model has its own preprocessing.\n",
        "fm_transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize(data_config[\"input_size\"][1], interpolation=transforms.InterpolationMode.BICUBIC),\n",
        "        transforms.CenterCrop(data_config[\"input_size\"][1]),\n",
        "    ]\n",
        ")\n",
        "\n",
        "if IMAGENET_PATH is None:\n",
        "    from torchvision.datasets import CIFAR10\n",
        "\n",
        "    cache_dir = \"./semanticlens_cache/data\"\n",
        "    dataset_model = CIFAR10(root=cache_dir, download=True, transform=model_transform)\n",
        "    dataset_fm = CIFAR10(root=cache_dir, download=True, transform=fm_transform)\n",
        "\n",
        "    print(\n",
        "        \"WARNING: Using CIFAR10 as a placeholder dataset. Expect limited insights into your model if you are not analyzing it on the dataset it was trained on.\"\n",
        "    )\n",
        "\n",
        "else:\n",
        "    from torchvision.datasets import ImageFolder\n",
        "\n",
        "    dataset_model = ImageFolder(\n",
        "        root=IMAGENET_PATH,\n",
        "        transform=model_transform,\n",
        "    )\n",
        "    dataset_fm = ImageFolder(\n",
        "        root=IMAGENET_PATH,\n",
        "        transform=fm_transform,\n",
        "    )\n",
        "\n",
        "\n",
        "print(f\"\\nLoaded {len(dataset_model)} samples for analysis.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Initializing the Core Components of SemanticLens\n",
        "Now we'll set up the three main components that will perform the analysis.\n",
        "1.  **Component Visualizer**: This is responsible for finding the \"concept examples\" (top activating images) for each neuron in a specified layer.\n",
        "2.  **Foundation Model**: This acts as the \"semantic expert\" that understands the meaning of images and text. We'll use `ClipMobile`.\n",
        "3.  **Lens**: This is the orchestrator that uses the Component Visualizer and Foundation Model to run the full analysis workflow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> üí° **Tip on Compute Demands and Caching:**  \n",
        "> This preprocessing can be quite compute-intensive. To address this, `semanticlens` provides smart caching: simply set the `cache_dir` argument when initializing your component-visualizer. Once preprocessing is complete, results are cached and reused‚Äîeven across notebook executions‚Äîso you won't need to repeat expensive computations.\n",
        "> \n",
        "> If you have limited computational resources, you can:\n",
        "> - Use a smaller subject model (e.g., a lightweight ResNet or MobileNet).\n",
        "> - Choose a more efficient foundation model, such as:\n",
        ">   ```python\n",
        ">\n",
        ">   fm = sl.foundation_models.OpenClip(url=\"RN50\", pretrained=\"openai\", device=device)\n",
        ">   ```\n",
        "> - Analyze a subset of your dataset instead of the full set.\n",
        ">\n",
        "> **Note:** Best results are obtained using the full dataset and high-performance foundation models. Reducing compute may limit the interpretability and accuracy of your analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Specify the layer of the model we want to analyze.\n",
        "layer_to_analyze = \"layer4\"\n",
        "# Directory to store cached results\n",
        "cache_dir = \"./semanticlens_cache\"\n",
        "\n",
        "# 1. Initialize the Component Visualizer\n",
        "# This class finds the images that most activate each neuron.\n",
        "cv = ActivationComponentVisualizer(\n",
        "    model=model,\n",
        "    dataset_model=dataset_model,\n",
        "    dataset_fm=dataset_fm,\n",
        "    layer_names=[layer_to_analyze],\n",
        "    num_samples=20,  # Number of top-activating images to collect per neuron\n",
        "    aggregate_fn=aggregators.aggregate_conv_mean,  # How to aggregate activations\n",
        "    cache_dir=cache_dir,\n",
        ")\n",
        "\n",
        "# 2. Initialize the Foundation Model\n",
        "# This model turns images and text into meaningful vectors.\n",
        "fm = ClipMobile(device=device)\n",
        "\n",
        "# 3. Initialize the Lens\n",
        "# This class manages the whole process.\n",
        "lens = sl.Lens(fm, device=device)\n",
        "\n",
        "print(\"SemanticLens components initialized successfully.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## 4. Running the Analysis: Building the Semantic Database\n",
        "Now we'll run the analysis in two steps.\n",
        "1.  **Find Concept Examples**: We call `cv.run()` to collect the top-activating images for each neuron. This is computationally intensive but the results are cached, so it only runs once.\n",
        "2.  **Build Semantic Database**: We call `lens.compute_concept_db()` to use the foundation model to embed the concept examples found in step 1.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Step 1: Find top-activating images for each neuron ---\n",
        "print(\"Finding top-activating images for each neuron...\")\n",
        "# This function saves its results to './semanticlens_cache', so if you run it again, it will load from the cache.\n",
        "cv.run(batch_size=64)\n",
        "\n",
        "# --- Step 2: Build the semantic database ---\n",
        "print(\"\\nBuilding the semantic database...\")\n",
        "# The Lens uses the foundation model to embed the concept examples. This result is also cached.\n",
        "concept_db = lens.compute_concept_db(cv, batch_size=64)\n",
        "\n",
        "print(\"\\nAnalysis complete!\")\n",
        "db_shape = concept_db[layer_to_analyze].shape\n",
        "print(f\"Concept database for '{layer_to_analyze}' has shape: (num_neurons, num_samples, embedding_dim)\")\n",
        "print(f\"Shape: {db_shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(dataset_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5. Exploring the Model's Knowledge\n",
        "We now have our `concept_db`, which holds the internal knowledge of our model. Let's use it to explore.\n",
        "## 5.1. Text Probing\n",
        "We can use natural language queries to find neurons that have learned specific concepts. First, we need to aggregate our concept database to get a single representative vector per neuron (we'll take the mean)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Average across the samples dimension to get one vector per neuron.\n",
        "aggregated_db = {k: v.mean(dim=1) for k, v in concept_db.items()}\n",
        "print(f\"Aggregated DB for '{layer_to_analyze}' has shape: {aggregated_db[layer_to_analyze].shape}\")\n",
        "\n",
        "# Probe the database with a text query for the concept of \"wheel\".\n",
        "query = \"car wheel\"\n",
        "print(f\"\\nSearching for concept: '{query}'\")\n",
        "text_scores = lens.text_probing(query, aggregated_db, templates=[\"a photo of a {}\"])\n",
        "\n",
        "# Find the top 5 neurons that are most similar to our query.\n",
        "top_neuron_ids = text_scores[layer_to_analyze].topk(5).indices.flatten()\n",
        "\n",
        "print(f\"Top 5 neurons for '{query}': {top_neuron_ids.tolist()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2. Visualizing the Results\n",
        "Now for the moment of truth! Let's visualize what the neurons we found via text search have actually learned. `cv.visualize_components` will show us a grid of the top-activating images for each neuron.\n",
        "\n",
        "In order to obtain a proper visualization we need to provide a denormalization transform either as attribute of the `cv.dataset` or as a key-word argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create a denormalization transform\n",
        "denormalization_fn = sl.utils.get_unnormalization_transform(\n",
        "    mean=list(data_config[\"mean\"]), std=list(data_config[\"std\"])\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Visualizing what the top 5 neurons for '{query}' have learned...\")\n",
        "cv.visualize_components(\n",
        "    component_ids=top_neuron_ids,\n",
        "    layer_name=layer_to_analyze,\n",
        "    n_samples=9,  # Number of images to display for each neuron\n",
        "    denormalization_fn=denormalization_fn,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# lets assign the denormalization function to the cv.dataset so we dont have to pass it every time.\n",
        "cv.dataset.denormalization_fn = denormalization_fn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Evaluating Interpretability\n",
        "`semanticlens` also provides scores to quantify the interpretability of model components. The **Clarity** score measures how semantically consistent the concept examples for a single neuron are. A higher score means the neuron has learned a clearer, more well-defined, single concept.\n",
        "Let's visualize the most and least clear neurons to get an intuition for what this score means."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate clarity scores for 'layer4'\n",
        "print(f\"Calculating clarity scores for layer '{layer_to_analyze}'...\")\n",
        "clarity_scores = lens.eval_clarity(concept_db)[layer_to_analyze]\n",
        "\n",
        "# Find the top 5 most clear neurons\n",
        "most_clear_ids = clarity_scores.topk(5).indices\n",
        "print(f\"\\nTop 5 most clear neurons: {most_clear_ids.tolist()}\")\n",
        "print(\"Visualizing most clear neurons (they should represent a single, consistent concept):\")\n",
        "cv.visualize_components(component_ids=most_clear_ids, layer_name=layer_to_analyze)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find the top 5 least clear neurons\n",
        "least_clear_ids = clarity_scores.topk(5, largest=False).indices\n",
        "print(f\"\\nTop 5 least clear neurons: {least_clear_ids.tolist()}\")\n",
        "print(\"Visualizing least clear neurons (they may represent multiple concepts or noisy features):\")\n",
        "cv.visualize_components(component_ids=least_clear_ids, layer_name=layer_to_analyze)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQccp1X5us9J"
      },
      "source": [
        "\n",
        "## Conclusion and Next Steps\n",
        "In this tutorial, we used `semanticlens` to:\n",
        "- Build a **semantic database** representing the visual concepts learned by a vision model's internal neurons.\n",
        "- Successfully find the neurons responsible for a specific concept using a **text query**.\n",
        "- **Visualize** the learnings of these neurons to confirm our findings.\n",
        "- Quantitatively evaluate the interpretability of neurons using the **clarity score**.\n",
        "### Where to go from here:\n",
        "* **Different Models and Layers**: Try analyzing a different `timm` model or a different layer of the model (e.g., `layer1`, `layer2`).\n",
        "* **Different Foundation Models**: Use a different foundation model, like `semanticlens.foundation_models.SigLipV2`.\n",
        "* **Other Scores**: Explore `lens.eval_polysemanticity()` and `lens.eval_redundancy()` to understand other aspects of your model's interpretability.\n",
        "* **Image Probing**: Use `lens.image_probing()` to search for concepts with an example image instead of text.\n",
        "\n",
        "### Comming Soon:\n",
        "* **Relevance-based Component Visualization**: Using Layer-wise Relevance Propagation not only the samples that highly activate a model component of interest can be identified but also responsible features within the images can be pin-pointed and used during the semantic embedding.\n",
        "* **Activation Heatmaps**: Faithfully attributing transformer architectures is still an open research question. Instead we can use the latent activations as a proxy to identify important input featuers.\n",
        "* ... (If you have additional ideas, feel free to open a pull-request or an issue.)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "semanticlens",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.23"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
